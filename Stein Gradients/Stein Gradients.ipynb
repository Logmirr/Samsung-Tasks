{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orth\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "device = None\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    use_cuda = True\n",
    "\n",
    "# Ignore cuda\n",
    "# use_cuda = False\n",
    "# device = None\n",
    "\n",
    "# Set DoubleTensor as a base type for computations\n",
    "t_type = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import functools\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_plots(data, axis, labels, file_name=None):\n",
    "    N_plots = len(data)\n",
    "    plt.figure(figsize=(30, (N_plots // 3 + 1) * 10))\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        for jdx in range(len(data[idx])):\n",
    "            plt.plot(data[idx][jdx], label=labels[idx][jdx])\n",
    "        plt.xlabel(axis[idx][0], fontsize=16)\n",
    "        plt.ylabel(axis[idx][1], fontsize=16)\n",
    "        plt.legend(loc=0, fontsize=16)\n",
    "    if file_name is not None:\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_projections(dm=None, use_real=True, kernel='tri', pdf=None, N_plots_max=10):\n",
    "    \"\"\"\n",
    "        Plot marginal kernel density estimation\n",
    "    Args:\n",
    "        dm (DistributionMover): class containing particles which define distribution\n",
    "        use_real (bool): If set to True then apply transformation dm.lt.transform before creating plot\n",
    "        kernel (str): Kernel type for kernel density estimation\n",
    "        pdf (array_like, None): Samples from target distribution\n",
    "        N_plots_max (int): Maximum number of plots\n",
    "    \"\"\"\n",
    "    N_plots = None\n",
    "    scale_factor = None\n",
    "    \n",
    "    if use_real:\n",
    "        if not dm.use_latent:\n",
    "            return\n",
    "        N_plots = dm.lt.A.shape[0]\n",
    "    else:\n",
    "        N_plots = dm.particles.shape[0]\n",
    "    if N_plots > 6:\n",
    "        scale_factor = 15\n",
    "    else:\n",
    "        scale_factor = 5\n",
    "        \n",
    "    N_plots = min(N_plots, N_plots_max)\n",
    "        \n",
    "    plt.figure(figsize=(3 * scale_factor, (N_plots // 3 + 1) * scale_factor))\n",
    "    \n",
    "    for idx in range(N_plots):\n",
    "        slice_dim = idx\n",
    "        \n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        \n",
    "        particles = None\n",
    "        if use_real:\n",
    "            particles = dm.lt.transform(dm.particles, n_particles_second=True).t()[:, slice_dim]\n",
    "        else:\n",
    "            particles = dm.particles.t()[:, slice_dim]\n",
    "        \n",
    "        if pdf is not None:\n",
    "            plt.plot(np.linspace(-10, 10, len(pdf), dtype=np.float64), pdf)\n",
    "        plt.plot(particles.data.cpu().numpy(), torch.zeros_like(particles).data.cpu().numpy(), 'ro')\n",
    "        sns.kdeplot(particles.data.cpu().numpy(), \n",
    "                    kernel=kernel, color='darkblue', linewidth=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_condition_distribution(dm, n_samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dm (DistributionMover): object contains unconditioned density, linear manifold and particles\n",
    "        n_samples (int): number of samples\n",
    "    Return:\n",
    "        (points, weight)\n",
    "    \"\"\"\n",
    "    if not dm.use_latent:\n",
    "        return\n",
    "    \n",
    "    points = torch.zeros([dm.n_hidden_dims, n_samples], dtype=t_type, device=device).uniform_(-10, 10)\n",
    "    weight = dm.real_target_density(dm.lt.transform(points, n_particles_second=True))\n",
    "    points = points.view(-1)\n",
    "    \n",
    "    plt.hist(points.data.cpu().numpy(), weights=weight.data.cpu().numpy(), density=True, bins=100, alpha=0.5, label='True conditional density')\n",
    "    plt.plot(dm.particles.data.cpu().numpy(), torch.zeros_like(dm.particles).data.cpu().numpy(), 'ro')\n",
    "    sns.kdeplot(dm.particles[0, :].data.cpu().numpy(), \n",
    "                kernel='tri', color='darkblue', linewidth=4, label='Approximated conditional density')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_diffs(x, y, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: x is a dxN matrix\n",
    "                   y is an optional dxM matrix\n",
    "            Output: diffs is a dxNxM matrix where diffs[i,j] is the subtraction between x[:,i] and y[:,j]\n",
    "            i.e. diffs[i,j] = x[:,i] - y[:,j]\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: x is a Nxd matrix\n",
    "                   y is an optional Mxd matrix\n",
    "            Output: diffs is a NxMxd matrix where diffs[i,j] is the subtraction between x[i,:] and y[j,:]\n",
    "            i.e. diffs[i,j] = x[i,:]-y[j,:]\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return x[:,:,np.newaxis] - y[:,np.newaxis,:]        \n",
    "    return x[:,np.newaxis,:] - y[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_dists(diffs=None, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: diffs is a dxNxM matrix where diffs[i,j] = x[:,i] - y[:,j]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[:,i] - y[:,j]||\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: diffs is a NxMxd matrix where diffs[i,j] = x[i,:] - y[j, :]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[i,:]-y[j,:]||\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return torch.norm(diffs, dim=0)\n",
    "    return torch.norm(diffs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class normal_density():\n",
    "    \"\"\"\n",
    "        Multinomial normal density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, mu=0., std=1., n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            mu (float, array_like): mean of distribution\n",
    "                Default: 0.\n",
    "                if mu is float - use same mean across all dimensions\n",
    "                if mu is 1D array_like - use different mean for each dimension but same for each particles dimension\n",
    "                if mu is 2D array_like - use different mean for each dimension\n",
    "            std (float, array_like): std of distribution\n",
    "                Default: 1.\n",
    "                if std is float - use same std across all dimensions\n",
    "                if std is 1D array_like - use different std for each dimension but same for each particles dimension\n",
    "                if std is 2D array_like - use different std for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis\n",
    "                Input will be reduced along all axis exclude particle axis\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        \n",
    "        if isinstance(self.mu, float):\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.mu.shape) == 1:\n",
    "            if self.mu.shape[0] == 1:\n",
    "                self.mu = self.mu.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.mu.shape) == 2:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if isinstance(self.std, float):\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.std.shape) == 1:\n",
    "            if len(self.std) == 1:\n",
    "                self.std = self.std.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.zero = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.pi = torch.tensor(math.pi, dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "    def __call__(self, x, n_axis=None):        \n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.pow(self.two * self.pi, -self.n / self.two) / \n",
    "                torch.prod(self.std, dim=n_axis) * \n",
    "                torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):      \n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std , self.two), dim=n_axis))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):  \n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (-self.n / self.two * torch.log(self.two * self.pi) + \n",
    "                torch.sum(torch.log(self.std), dim=n_axis) -\n",
    "                self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return -self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)\n",
    "    \n",
    "    def get_sample(self):\n",
    "        \"\"\"\n",
    "            Sample from normal distribution\n",
    "        \"\"\"\n",
    "        sample = torch.normal(self.mu, self.std)\n",
    "        if self.n_particles_second:\n",
    "            return sample.view(-1, 1)\n",
    "        else:\n",
    "            return sample.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class gamma_density():\n",
    "    \"\"\"\n",
    "        Multinomial gamma density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, alpha=1, betta=1, n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            alpha (float, array_like): shape of distribution\n",
    "                Default: 1.\n",
    "                if alpha is float - use same shape across all dimensions\n",
    "                if alpha is 1D array_like - use different shape for each dimension but same for each particles dimension\n",
    "                if alpha is 2D array_like - use different shape for each dimension\n",
    "            betta (float, array_like): rate of distribution\n",
    "                Default: 1.\n",
    "                if betta is float - use same rate across all dimensions\n",
    "                if betta is 1D array_like - use different rate for each dimension but same for each particles dimension\n",
    "                if betta is 2D array_like - use different rate for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis  \n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.betta = betta\n",
    "        \n",
    "        if isinstance(self.alpha, float):\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.alpha.shape) == 1:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.alpha.shape) == 2:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if isinstance(self.betta, float):\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.betta.shape) == 1:\n",
    "            if len(self.betta) == 1:\n",
    "                self.betta = self.betta.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "        ### log Г(alpha)\n",
    "        self.lgamma = torch.lgamma(self.alpha)\n",
    "        ### Г(alpha)\n",
    "        self.gamma = torch.exp(self.lgamma)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(self.betta, self.alpha) / self.gamma * torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     11,
     33,
     36,
     47,
     54,
     73,
     107
    ]
   },
   "outputs": [],
   "source": [
    "class SteinLinear(nn.Module):\n",
    "    \"\"\"\n",
    "        Custom full connected layer for Stein Gradient Neural Networks\n",
    "        Transformation: y = xA + b\n",
    "        Parameters prior: \n",
    "            1 - p(w, a) = p(w|a)p(a) = П p(w_i|a_i)p(a_i)\n",
    "                p(w_i|a_i) = N(w_i|0, a_i^(-1)); p(a_i) = G(1e-4, 1e-4)\n",
    "                          \n",
    "            2 - p(w) = П p(w_i)\n",
    "                p(w_i) = N(w_i|0, alpha^(-1))\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, n_particles=1, use_bias=True, use_var_prior=True, alpha=1e-2):\n",
    "        super(SteinLinear, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features (int): size of each input sample\n",
    "            out_features (int): size of each output sample\n",
    "            n_particles (int): number of particles\n",
    "            use_bias (bool): If set to False, the layer will not learn an additive bias.\n",
    "                Default: True\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of weight variance\n",
    "                Default: True\n",
    "            alpha (float): If use_var_prior == False - defines weight variance\n",
    "                Default: 1e-2\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_particles = n_particles\n",
    "        self.use_bias = use_bias\n",
    "        self.use_var_prior = use_var_prior\n",
    "\n",
    "        ### if alpha is None use GLOROT prior\n",
    "        if alpha is None:\n",
    "            self.alpha_weight = (self.in_features + self.out_features) / 2.\n",
    "            self.alpha_bias = (self.out_features) / 2.\n",
    "        else:\n",
    "            self.alpha_weight = alpha\n",
    "            self.alpha_bias = alpha\n",
    "    \n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.zeros([in_features, out_features, n_particles], dtype=t_type, device=device))\n",
    "        if self.use_var_prior:\n",
    "            self.log_weight_alpha = torch.nn.Parameter(torch.zeros([in_features * out_features, n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_weight_alpha = torch.tensor([math.log(self.alpha_weight)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros([1, out_features, n_particles], dtype=t_type, device=device))\n",
    "            if self.use_var_prior:\n",
    "                self.log_bias_alpha = torch.nn.Parameter(torch.zeros([out_features, n_particles], dtype=t_type, device=device))\n",
    "            else:\n",
    "                self.log_bias_alpha = torch.tensor([math.log(self.alpha_bias)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        if self.use_var_prior:\n",
    "            ### define prior on alpha p(a) = G(1e-4, 1e-4)\n",
    "            self.weight_alpha_log_prior = lambda x: (gamma_density(n=self.log_weight_alpha.shape[0],\n",
    "                                                                   alpha=1e-4,\n",
    "                                                                   betta=1e-4,\n",
    "                                                                   n_particles_second=True\n",
    "                                                                  ).log_unnormed_density_log_x(x))\n",
    "            if self.use_bias:\n",
    "                self.bias_alpha_log_prior = lambda x: (gamma_density(n=self.log_bias_alpha.shape[0],\n",
    "                                                                     alpha=1e-4,\n",
    "                                                                     betta=1e-4,\n",
    "                                                                     n_particles_second=True\n",
    "                                                                    ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    ### useless function - all initialization defined in DistributionMover class\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.log_weight_alpha.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.log_bias_alpha.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Apply transformation: X_out[i, :, :] = X_in[i, :, :] * W + b[i]\n",
    "        Args:\n",
    "            X (torch.tensor): tensor \n",
    "        Shape:\n",
    "            Input: [n_particles, batch_size, in_features]\n",
    "            Output: [n_particles, batch_size, out_features]\n",
    "        \"\"\"\n",
    "        ### NEED SOME OPTIMIZATION TO OMMIT .permute\n",
    "        if self.use_bias:\n",
    "            return torch.bmm(X, self.weight.permute(2, 0, 1)) + self.bias.permute(2, 0, 1)\n",
    "        return torch.bmm(X, self.weight.permute(2, 0, 1))\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())\n",
    "        \n",
    "    def calc_log_prior(self):\n",
    "        \"\"\"\n",
    "            Evaluate log prior of trainable parameters\n",
    "        log p(w,a) = log p(w|a) + log p(a)\n",
    "        \"\"\"\n",
    "        ### define prior on weight p(w|a) = N(0, 1 / alpha)\n",
    "        weight_log_prior = lambda x: (normal_density(n=self.weight.numel() // self.n_particles,\n",
    "                                                     mu=0.,\n",
    "                                                     std=self.one / torch.exp(self.log_weight_alpha),\n",
    "                                                     n_particles_second=True\n",
    "                                                    ).log_unnormed_density(x))\n",
    "\n",
    "        bias_log_prior = lambda x: (normal_density(self.bias.numel() // self.n_particles,\n",
    "                                                   mu=0.,\n",
    "                                                   std=self.one / torch.exp(self.log_bias_alpha),\n",
    "                                                   n_particles_second=True\n",
    "                                                  ).log_unnormed_density(x))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            if self.use_var_prior:\n",
    "                return (weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha) +\n",
    "                        bias_log_prior(self.bias.view(-1, self.n_particles)) + self.bias_alpha_log_prior(self.log_bias_alpha))   \n",
    "            return (weight_log_prior(self.weight.view(-1, self.n_particles)) +\n",
    "                    bias_log_prior(self.bias.view(-1, self.n_particles)))     \n",
    "        \n",
    "        if self.use_var_prior:\n",
    "            return weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha)\n",
    "        return weight_log_prior(self.weight.view(-1, self.n_particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     4,
     47,
     58,
     69,
     84,
     109
    ]
   },
   "outputs": [],
   "source": [
    "class LinearTransform():\n",
    "    \"\"\"\n",
    "        Class for various linear transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, n_dims, n_hidden_dims, use_identity=False, normalize=False, A=None, theta_0=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_dims (int): dimension of the space\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_identity (bool): If set to True, use 'eye' matrix for transformations\n",
    "            normalize (bool): If set to True, columns of the transformation matrix is an orthonormal basis \n",
    "            A (2D array_like, None): Initial value for transformation matrix\n",
    "                If None then matrix will be sampled from uniform distribution and then orthonormate\n",
    "                Default: None\n",
    "            theta_0 (1D array_like, None): Initial value for bias\n",
    "                If None then matrix will be sampled from uniform distribution\n",
    "                Default: None\n",
    "        \"\"\"\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_identity = use_identity\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if self.use_identity:\n",
    "            return\n",
    "        \n",
    "        self.A = A\n",
    "        self.theta_0 = theta_0\n",
    "        \n",
    "        if self.A is None:\n",
    "            self.A = torch.zeros([self.n_dims, self.n_hidden_dims], dtype=t_type, device=device)\n",
    "            self.A.uniform_(-1., 1.)\n",
    "            if self.normalize:\n",
    "                ### normalize columns of matrix A\n",
    "                self.A = torch.tensor(orth(self.A.data.cpu().numpy()), dtype=t_type, device=device)\n",
    "                    \n",
    "        if self.theta_0 is None:\n",
    "            self.theta_0 = torch.zeros([self.n_dims, 1], dtype=t_type, device=device)\n",
    "            self.theta_0.uniform_(-1.,1.)\n",
    "        \n",
    "        ### A^(t)A\n",
    "        self.AtA = torch.matmul(self.A.t(), self.A)\n",
    "        ### (A^(t)A)^(-1)\n",
    "        self.AtA_1 = torch.inverse(self.AtA)\n",
    "        ### (A^(t)A)^(-1)A^(t)\n",
    "        self.inverse_base = torch.matmul(self.AtA_1, self.A.t())\n",
    "        \n",
    "    def transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Transform thetas as follows: \n",
    "                theta = Atheta` + theta_0\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.A, theta) + self.theta_0\n",
    "        return (torch.matmul(self.A, theta.t()) + self.theta_0).t()\n",
    "    \n",
    "    def inverse_transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Apply inverse transformation: \n",
    "                theta` = (A^(t)A)^(-1)A^(t)(theta - theta_0)\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.inverse_base, theta - self.theta_0)\n",
    "        return torch.matmul(self.inverse_base, theta.t() - self.theta_0).t()\n",
    "    \n",
    "    def project_inverse(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Project and then apply inverse transform to theta - theta_0:\n",
    "                theta_s_p_i = T^(-1)P(theta - theta_0)= (A^(t)A)^(-1)A^(t)theta\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        ### This optimization severely reduces performance!!!!\n",
    "        ### use solver trick: theta_s_p_i : A^(t)Atheta_s_p_i = A^(t)theta\n",
    "        if n_particles_second:\n",
    "            # return torch.gesv(torch.matmul(self.A.t(), theta), self.AtA)[0]\n",
    "            return torch.matmul(self.inverse_base, theta)\n",
    "        # return torch.gesv(torch.matmul(self.A.t(), theta.t()), self.AtA)[0].t()\n",
    "        return torch.matmul(theta, self.inverse_base.t())\n",
    "        \n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        Example::\n",
    "            >>> module.state_dict().keys()\n",
    "            ['bias', 'weight']\n",
    "        \"\"\"\n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=1)\n",
    "        \n",
    "        destination[prefix + 'A'] = self.A if keep_vars else self.A.data\n",
    "        destination[prefix + 'theta_0'] = self.theta_0 if keep_vars else self.theta_0.data\n",
    "        destination[prefix + 'n_dims'] = self.n_dims\n",
    "        destination[prefix + 'n_hidden_dims'] = self.n_hidden_dims\n",
    "        destination[prefix + 'use_identity'] = self.use_identity\n",
    "        destination[prefix + 'normalize'] = self.normalize\n",
    "        \n",
    "        return destination\n",
    "    \n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        self.A.copy_(state_dict[prefix + 'A'])\n",
    "        self.theta_0.copy_(state_dict[prefix + 'theta_0'])\n",
    "        self.__init__(state_dict[prefix + 'n_dims'],\n",
    "                      state_dict[prefix + 'n_hidden_dims'],\n",
    "                      state_dict[prefix + 'use_identity'],\n",
    "                      state_dict[prefix + 'normalize'],\n",
    "                      self.A, self.theta_0\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0,
     4,
     20,
     22,
     25,
     36,
     74,
     77
    ]
   },
   "outputs": [],
   "source": [
    "class RegressionDistribution(nn.Module):\n",
    "    \"\"\"\n",
    "        Distribution over data for regression task: p(D|w) = N(y_predicted|y, )\n",
    "    \"\"\"\n",
    "    def __init__(self, n_particles, use_var_prior=True, betta=1e-1):\n",
    "        super(RegressionDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of prediction variance\n",
    "                Default: True\n",
    "            betta (float): If use_var_prior == False - defines variance of prediction\n",
    "                Default: 1e-1\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.use_var_prior = use_var_prior\n",
    "        self.betta = betta\n",
    "        \n",
    "        ### define betta - variance of data distribution for regression tasks (betta = 1 / std ** 2)\n",
    "        if self.use_var_prior:\n",
    "            self.log_betta = torch.nn.Parameter(torch.zeros([1, self.n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_betta = torch.tensor([math.log(self.betta)], dtype=t_type, device=device, requires_grad=False)\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            ### define prior on betta p(betta)\n",
    "            self.betta_log_prior = lambda x: (gamma_density(n=1,\n",
    "                                                            alpha=1e-4,\n",
    "                                                            betta=1e-4,\n",
    "                                                            n_particles_second=True\n",
    "                                                           ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        ### Support tensors for computations\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "    \n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(theta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predicted values\n",
    "            train_size (int) - size of training dataset\n",
    "        Shapes:\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, 1]\n",
    "        \"\"\"\n",
    "        ### squeeze last axis because regression task is being solved\n",
    "        y_predict.squeeze_(2)\n",
    "        \n",
    "        batch_size = torch.tensor(y.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        log_data_distr = None\n",
    "        if self.use_var_prior:\n",
    "            log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                       mu=y,\n",
    "                                                       std=self.one / torch.sqrt(torch.exp(self.log_betta.expand(X.shape[0], self.n_particles).t())),\n",
    "                                                       n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "        else:\n",
    "            log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                       mu=y,\n",
    "                                                       std=self.one / torch.sqrt(torch.exp(self.log_betta)),\n",
    "                                                       n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "            \n",
    "        if self.use_var_prior:\n",
    "            return train_size / batch_size * log_data_distr(y_predict) + self.betta_log_prior(self.log_betta)\n",
    "        return train_size / batch_size * log_data_distr(y_predict)\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ClassificationDistribution(nn.Module):\n",
    "    def __init__(self, n_particles):\n",
    "        super(ClassificationDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        \"\"\"\n",
    "        self.n_particles = n_particles \n",
    "\n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(theta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shapes: \n",
    "            X.shape = [batch_size, in_features]\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        batch_size = torch.tensor(X.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        \n",
    "        probas = nn.LogSoftmax(dim=2)(y_predict)\n",
    "        probas_selected = torch.gather(input=probas, dim=2, index=y.view(1, -1, 1).expand(probas.shape[0], probas.shape[1], 1)).squeeze(2)\n",
    "        log_data = torch.sum(probas_selected, dim=1)\n",
    "        \n",
    "        return train_size / batch_size  * log_data\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0,
     1,
     45,
     47,
     51,
     58,
     65,
     73,
     77,
     79,
     82,
     98,
     103,
     116,
     129,
     226,
     257,
     267,
     287,
     328,
     334,
     352,
     466,
     477,
     484,
     490,
     516
    ]
   },
   "outputs": [],
   "source": [
    "class DistributionMover(nn.Module):\n",
    "    def __init__(self,\n",
    "                 task='app', \n",
    "                 n_particles=None,\n",
    "                 particles=None,\n",
    "                 target_density=None,\n",
    "                 n_dims=None,\n",
    "                 n_hidden_dims=None,\n",
    "                 use_latent=False,\n",
    "                 net=None,\n",
    "                 precomputed_params=None,\n",
    "                 data_distribution=None\n",
    "                ):\n",
    "        super(DistributionMover, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task (str):\n",
    "                'app' | 'net_reg' | 'net_class'\n",
    "                - approximate target distribution\n",
    "                - solve regression task using net\n",
    "                - solve classification task using net\n",
    "            n_particles (int): number of particles\n",
    "            particles (2D array_like): array which contains initialized particles\n",
    "            target_density (callable): computes probability density function of target distribution (only for 'app' task)\n",
    "            n_dims (int): dimension of the space where optimization is performed\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_latent (bool): If set to True, Subspace Stein is used\n",
    "            acr (list): List contains arcitecture of object which is used to make predictions (for 'net_reg' and' net_class' tasks)\n",
    "            precomputed_params (1D array_like): Precomputed parameters, which will be used for particles initialization\n",
    "            data_distribution (callable): computes probability over data p(D|w) (for 'net_reg' and' net_class' tasks)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.task = task\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.particles = particles\n",
    "        self.target_density = target_density\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_latent = use_latent\n",
    "        self.net = net\n",
    "        self.precomputed_params = precomputed_params\n",
    "        self.data_distribution = data_distribution\n",
    "\n",
    "        \n",
    "        if self.task == 'net_reg' or self.task == 'net_class':\n",
    "            self.n_dims = self.numel() // self.n_particles\n",
    "        if not self.use_latent:\n",
    "            self.n_hidden_dims = self.n_dims\n",
    "\n",
    "        ### Learnable samples from the target distribution\n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_hidden_dims, self.n_particles],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., 2.)\n",
    "\n",
    "        ### Class for performing linear transformations\n",
    "        if self.use_latent:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=False, \n",
    "                normalize=True\n",
    "            )\n",
    "        else:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=True, \n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "        if self.precomputed_params is not None:\n",
    "            self.particles = self.lt.inverse_transform(self.precomputed_params.unsqueeze(1).expand(self.n_dims, self.n_particles))\n",
    "\n",
    "        ### Functions of probability density of target distribution\n",
    "        if self.net is None:\n",
    "            # use unnormed probability density to speedup computations\n",
    "            if target_density is not None:\n",
    "                self.target_density = target_density\n",
    "                self.real_target_density = target_density\n",
    "            else:\n",
    "                self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                                   0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "                self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                        0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "\n",
    "        ### Number of iterations since beginning\n",
    "        self.iter = 0\n",
    "\n",
    "        ### Adagrad parameters\n",
    "        self.fudge_factor = torch.tensor(1e-6, dtype=t_type, device=device)\n",
    "        self.step_size = torch.tensor(1e-2, dtype=t_type, device=device)\n",
    "        self.auto_corr = torch.tensor(0.9, dtype=t_type, device=device)\n",
    "\n",
    "        ### Gradient history term for adagrad optimization\n",
    "        if self.use_latent:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_hidden_dims, n_particles], dtype=t_type, device=device)\n",
    "            self.historical_grad_theta_0 = torch.zeros(\n",
    "                [self.n_dims, 1], dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_dims, n_particles], dtype=t_type, device=device)\n",
    "\n",
    "        ### Factor from kernel\n",
    "        self.h = torch.tensor(0., dtype=t_type, device=device)\n",
    "\n",
    "        ### Support tensors for computations\n",
    "        self.N = torch.tensor(self.n_particles, dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.three = torch.tensor(3., dtype=t_type, device=device)\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"   \n",
    "        cnt = 0\n",
    "        for module in self.children():\n",
    "            if 'numel' in dir(module):\n",
    "                cnt += module.numel(trainable)\n",
    "        return cnt\n",
    "    \n",
    "    def calc_kernel_term_latent(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        ### power for rational kernel\n",
    "        p = torch.tensor(p, dtype=t_type, device=device) if p is not None else None\n",
    "        \n",
    "        ### theta = Atheta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### diffs[i, j] = A(theta`_i - theta`_j)\n",
    "        diffs = pairwise_diffs(real_particles, real_particles, n_particles_second=True)\n",
    "        ### dists[i, j] = ||A(theta`_i - theta`_j)||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        ### sq_dists[i, j] = ||A(theta`_i - theta`_j)||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "        \n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 2:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 5:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  med / (torch.pow(self.N, self.two) - self.one)\n",
    "        elif h_type == 6:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  med / (torch.pow(self.N, -self.one / p) - self.one)\n",
    "        elif h_type == 7:\n",
    "            med = torch.median(torch.matmul(real_particles.t(), real_particles)) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "        \n",
    "        kernel = None\n",
    "        grad_kernel = None\n",
    "        if kernel_type == 'rbf':\n",
    "            ### RBF Kernel:\n",
    "            ### kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "            kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "            ### grad_kernel[i, j] = -2/h * A(theta`_i - theta`_j) * kernel[i, j]\n",
    "            grad_kernel = -self.two / self.h * kernel.unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'imq':\n",
    "            ### IMQ Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, -self.one / self.two)\n",
    "            ### grad_kernel[i, j] = -1/h * A(theta`_i - theta`_j) * kernel^(3)[i, j]\n",
    "            grad_kernel = -self.one / self.h * torch.pow(kernel, self.three).unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'exp':\n",
    "            ### Exponential Kernel:\n",
    "            ### kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "            kernel = torch.exp(self.one / self.h * torch.matmul(real_particles.t(), real_particles))\n",
    "            ### grad_kernel[i, j] = 1/h * (Atheta`_j + theta_0) * kernel[i, j]\n",
    "            grad_kernel = 1. / self.h * kernel.unsqueeze(0) * real_particles.unsqueeze(1)\n",
    "        elif kernel_type == 'rat':\n",
    "            ### RAT Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, p)\n",
    "            ### grad_kernel[i, j] = p/h * A(theta`_i - theta`_j) * kernel^((p - 1)/p)[i, j]\n",
    "            grad_kernel = p / self.h * torch.pow(kernel, (self.p - self.one) / p).unsqueeze(0) * diffs\n",
    "            \n",
    "        return kernel, grad_kernel\n",
    "    \n",
    "    def calc_kernel_term_latent_net(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        return self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "    \n",
    "    def calc_log_prior_net(self):\n",
    "        \"\"\"\n",
    "            Traverse all modules and evaluate weights log prior\n",
    "        \"\"\"\n",
    "        log_prior = 0\n",
    "        for module in self.children():\n",
    "            if 'calc_log_prior' in dir(module):\n",
    "                log_prior += module.calc_log_prior()\n",
    "        return log_prior\n",
    "\n",
    "    def calc_log_term_latent(self):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        \n",
    "        ### theta = A theta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_term = torch.log(self.target_density(real_particles))\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "    \n",
    "        grad_log_term = real_particles.grad\n",
    "        \n",
    "        return grad_log_term\n",
    "    \n",
    "    def calc_log_term_latent_net(self, X, y, train_size):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta)) \n",
    "        Args:\n",
    "            X (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input: \n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output: \n",
    "                [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        log_data = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        log_prior = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        \n",
    "        ### get real net parameters: theta_i = A theta`_i + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### init net with real parameters\n",
    "        self.vector_to_parameters(real_particles.view(-1), self.parameters_net())\n",
    "        ### compute log prior of all weight in the net\n",
    "        log_prior = self.calc_log_prior_net()\n",
    "        \n",
    "        ### get prediction for the batch of data\n",
    "        y_predict = self.predict_net(X)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_data = self.data_distribution.calc_log_data(X, y, y_predict, train_size)\n",
    "        \n",
    "        ### log_term = log p(theta) = log p_prior(theta) + log p_data(D|theta)\n",
    "        log_term = log_prior + log_data\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "        \n",
    "        ### collect all gradients into one vector\n",
    "        grad_log_term = self.parameters_grad_to_vector(self.parameters_net()).view(-1, self.n_particles)\n",
    "            \n",
    "        return grad_log_term\n",
    "    \n",
    "    def parameters_net(self):\n",
    "        \"\"\"\n",
    "            Return all trainable parameters\n",
    "        \"\"\"\n",
    "        return chain(self.net.parameters(), self.data_distribution.parameters())\n",
    "    \n",
    "    def predict_net(self, X, inference=False):\n",
    "        \"\"\"\n",
    "            Use net to make predictions        \n",
    "            Args:\n",
    "                X (array_like): batch of data\n",
    "        \"\"\"\n",
    "        predictions = self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape))\n",
    "        if self.task == 'net_reg':\n",
    "            if inference:\n",
    "                return torch.mean(predictions, dim=0)\n",
    "            else:\n",
    "                return predictions\n",
    "        elif self.task == 'net_class':\n",
    "            if inference:\n",
    "                return torch.log(torch.mean(torch.nn.Softmax(dim=2)(predictions), dim=0))\n",
    "            else:\n",
    "                return predictions\n",
    "\n",
    "    def update_latent(self, \n",
    "                      h_type, kernel_type='rbf', p=None, \n",
    "                      step_size=None, \n",
    "                      move_theta_0=False, \n",
    "                      burn_in=False, burn_in_coeff=None,\n",
    "                      epoch=None\n",
    "                     ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "            \n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent()\n",
    "        \n",
    "        ### Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "        \n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "        \n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "        \n",
    "        ### Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            ### Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "            \n",
    "            ### Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            ### Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "               \n",
    "    def update_latent_net(self,\n",
    "                          h_type, kernel_type='rbf', p=None,\n",
    "                          X_batch=None, y_batch=None, train_size=None,\n",
    "                          step_size=None,\n",
    "                          move_theta_0=False, \n",
    "                          burn_in=False, burn_in_coeff=None, \n",
    "                          epoch=None\n",
    "                         ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "            \n",
    "        self.iter += 1\n",
    "        self.net.zero_grad()\n",
    "        self.data_distribution.zero_grad()\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent_net(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_net(X_batch, y_batch, train_size)\n",
    "        \n",
    "        ### Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "        \n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "                \n",
    "        ### Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            ### Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "            \n",
    "            ### Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            ### Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_to_parameters(vec, parameters):\n",
    "        pointer = 0\n",
    "        for param in parameters:\n",
    "            # The length of the parameter\n",
    "            num_param = param.numel()\n",
    "            # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "            param.data = vec[pointer:pointer + num_param].view_as(param).data\n",
    "            # Increment the pointer\n",
    "            pointer += num_param\n",
    "            \n",
    "    @staticmethod \n",
    "    def parameters_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    @staticmethod \n",
    "    def parameters_grad_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.grad.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        \"\"\"\n",
    "        destination = super(DistributionMover, self).state_dict(destination, prefix, keep_vars)\n",
    "        \n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=self._version)\n",
    "        \n",
    "        destination[prefix + 'particles'] = self.particles if keep_vars else self.particles.data\n",
    "        destination[prefix + 'historical_grad'] = self.historical_grad if keep_vars else self.historical_grad.data\n",
    "        if self.use_latent:\n",
    "            destination[prefix + 'historical_grad_theta_0'] = self.historical_grad_theta_0 if keep_vars else self.historical_grad_theta_0.data\n",
    "            self.lt.state_dict(destination, prefix + 'lt' + '.', keep_vars=keep_vars)\n",
    "        destination[prefix + 'step_size'] = self.step_size\n",
    "        destination[prefix + 'iter'] = self.iter\n",
    "        destination[prefix + 'epoch'] = self.epoch\n",
    "                \n",
    "        return destination\n",
    "    \n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        super(DistributionMover, self).load_state_dict(state_dict, prefix)\n",
    "        \n",
    "        self.particles.copy_(state_dict[prefix + 'particles'])\n",
    "        self.historical_grad.copy_(state_dict[prefix + 'historical_grad'])\n",
    "        self.step_size = state_dict[prefix + 'step_size']\n",
    "        self.iter = state_dict[prefix + 'iter']\n",
    "        self.epoch = state_dict[prefix + 'epoch']\n",
    "        \n",
    "        if self.use_latent:\n",
    "            self.historical_grad_theta_0.copy_(state_dict[prefix + 'historical_grad_theta_0'])\n",
    "            self.lt.load_state_dict(state_dict, prefix + 'lt' + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LRStrategy:\n",
    "    def __init__(self, step_size, factor=0.1, n_epochs=1, patience=10):\n",
    "        \"\"\"\n",
    "            Multiply @step_size by factor each @n_epochs epochs\n",
    "            Freeze @step_size after @patience epochs\n",
    "        \"\"\"\n",
    "        self.step_size = step_size\n",
    "        self.factor = factor\n",
    "        self.n_epochs = n_epochs\n",
    "        self.patience = patience\n",
    "        self.iter = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.iter += 1\n",
    "        if self.iter < self.patience and self.iter % self.n_epochs == 0:\n",
    "            self.step_size *= self.factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add some methods to nn.Sequential to make code clear \n",
    "\n",
    "setattr(nn.Sequential, \"numel\", DistributionMover.numel)\n",
    "setattr(nn.Sequential, \"calc_log_prior\", DistributionMover.calc_log_prior_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Bostor housing dataset for regression task\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = (torch.tensor(X_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(X_test, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_test, dtype=t_type, device=device))\n",
    "\n",
    "### Linear Regression baseline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.tensor(model.predict(X_test), dtype=t_type, device=device), y_test), \n",
    "      torch.nn.MSELoss()(torch.tensor(model.predict(X_train), dtype=t_type, device=device), y_train))\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.mean(y_train).expand(y_test.shape[0]), y_test), \n",
    "      torch.nn.MSELoss()(torch.mean(y_train).expand(y_train.shape[0]), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Check all functions\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=5, n_particles=10, use_latent=True)\n",
    "dm.calc_log_term_latent()\n",
    "dm.calc_kernel_term_latent(0)\n",
    "dm.update_latent(0)\n",
    "\n",
    "net = nn.Sequential(SteinLinear(13, 1, 10, use_var_prior=True))\n",
    "dd = RegressionDistribution(n_particles=10, use_var_prior=False)\n",
    "dm = DistributionMover(task='net_reg', n_hidden_dims=5, n_particles=10, use_latent=True, net=net, data_distribution=dd)\n",
    "dm.calc_log_term_latent_net(X_train, y_train, X_train.shape[0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(13, 1, 100, use_var_prior=False, alpha=1e-2, use_bias=True))\n",
    "data_distr = RegressionDistribution(100, use_var_prior=False, betta=1e-1)\n",
    "dm = DistributionMover(task='net_reg', n_particles=100, use_latent=False, net=net, data_distribution=data_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# alpha = dm.net[0].alpha\n",
    "# betta = dm.data_distribution.betta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Append column of ones to data \n",
    "# XX = X_train\n",
    "# XX_test = X_test\n",
    "XX = torch.cat([X_train, torch.ones([X_train.shape[0], 1], dtype=t_type, device=device)], dim=1)\n",
    "XX_test = torch.cat([X_test, torch.ones([X_test.shape[0], 1], dtype=t_type, device=device)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sigma = torch.inverse(betta * XX.t() @ XX + alpha * torch.eye(XX.shape[1], dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mu = betta * sigma @ XX.t() @ y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu @ XX.t(), y_train), torch.nn.MSELoss()(mu @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_test = torch.load('particles_12400.txt').cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_mean = torch.mean(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(particles_mean @ XX.t(), y_train), torch.nn.MSELoss()(particles_mean @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.var(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.diag(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import *\n",
    "\n",
    "mod = WLS(y_train.data.cpu().numpy(), XX.data.cpu().numpy())\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stein_mu = (torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, torch.tensor(results.params, dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, particles_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(sum(np.logical_and(results.conf_int()[:,0] < stein_mu.data.cpu().numpy(), stein_mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < mu.data.cpu().numpy(), mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < particles_mean.data.cpu().numpy(), particles_mean.data.cpu().numpy() < results.conf_int()[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    step_size = 0.00075\n",
    "    dm.historical_grad.zero_()\n",
    "    for _ in range(100000):\n",
    "        dm.update_latent_net(h_type=1, kernel_type='rbf', p=-1, X_batch=X_train, y_batch=y_train, train_size=X_train.shape[0], step_size=step_size)\n",
    "        train_loss = torch.nn.MSELoss()(dm.predict_net(X_train, inference=True).view(-1), y_train)\n",
    "        test_loss = torch.nn.MSELoss()(dm.predict_net(X_test, inference=True).view(-1), y_test)\n",
    "        \n",
    "        if _ % 10 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train): {1:.3f}\\t Empirical Loss(Test): {2:.3f}\\t Kernel factor: {3:.3f}'.format(\n",
    "                            _, train_loss, test_loss, dm.h))\n",
    "            \n",
    "            plot_projections(dm, use_real=True)\n",
    "            plot_projections(dm, use_real=False)\n",
    "            plt.pause(1e-300)\n",
    "            \n",
    "        if _ % 3000 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset with class selection\n",
    "\n",
    "sys.path.insert(0, '/home/m.nakhodnov/Samsung-Tasks/Datasets/MyMNIST')\n",
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform)\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform)\n",
    "\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=100, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(dm,\n",
    "          dataloader_train, dataloader_test,\n",
    "          lr_str, start_epoch, end_epoch, n_epochs_save=20, n_epochs_log=1,\n",
    "          move_theta_0=False, plot_graphs=True, verbose=False,\n",
    "          checkpoint_file_name=None, plots_file_name=None, log_file_name=None,\n",
    "          n_warmup_epochs=16, n_previous=10\n",
    "          ):\n",
    "    ### Get all y_test in one tensor\n",
    "    y_test_all = torch.tensor([], dtype=torch.int64, device=device)\n",
    "    for _, y_test in dataloader_test:\n",
    "        y_test = y_test.to(device=device)\n",
    "        y_test_all = torch.cat([y_test_all, y_test.data.detach().clone()], dim=0)\n",
    "    ### WARNING: May be incorrect if output features of dm.net != n_classes\n",
    "    n_classes = len(dataloader_train.dataset.class_nums)\n",
    "\n",
    "    ### Train loss/accuracy\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    ### Test loss/accuracy\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    ### Mean loss/accuracy from @n_warmup_epochs to current epoch\n",
    "    test_losses_mean = []\n",
    "    test_accs_mean = []\n",
    "    predictions_test_cummulative = torch.zeros([1, 1], dtype=t_type, device=device)\n",
    "    ### Mean loss/accuracy from (current epoch - n_previous)  to current epoch\n",
    "    test_losses_mean_previous = []\n",
    "    test_accs_mean_previous = []\n",
    "    predictions_test_previous = torch.zeros([n_previous, y_test_all.shape[0], n_classes], dtype=t_type, device=device)\n",
    "    ### Index of 'oldest' element in predictions_test_previous\n",
    "    pointer_to_the_back = 0\n",
    "\n",
    "    if log_file_name is not None:\n",
    "        log_file = open(log_file_name, 'a')\n",
    "        log_file.write('\\rNew run of training.\\r')\n",
    "        log_file.close()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, end_epoch):\n",
    "            epoch_since_start = epoch - start_epoch\n",
    "\n",
    "            ### One update of particles via all dataloader_train\n",
    "            for X, y in dataloader_train:\n",
    "                X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "                y = y.to(device=device)\n",
    "                burn_in_coeff = max(1. - (1. - 1.) / 20. * epoch, 1.)\n",
    "                dm.update_latent_net(h_type=0, kernel_type='rbf', p=None,\n",
    "                                     X_batch=X, y_batch=y,\n",
    "                                     train_size=len(dataloader_train.dataset),\n",
    "                                     step_size=lr_str.step_size,\n",
    "                                     move_theta_0=move_theta_0,\n",
    "                                     burn_in=True, burn_in_coeff=burn_in_coeff,\n",
    "                                     epoch=epoch\n",
    "                                     )\n",
    "\n",
    "            ### Evaluate cross entropy and accuracy over dataloader_train\n",
    "            train_loss = 0.\n",
    "            train_acc = 0.\n",
    "            for X_train, y_train in dataloader_train:\n",
    "                X_train = X_train.double().to(device=device).view(X.shape[0], -1)\n",
    "                y_train = y_train.to(device=device)\n",
    "\n",
    "                net_pred = dm.predict_net(X_train, inference=True)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                train_loss -= torch.sum(torch.gather(net_pred, 1, y_train.view(-1, 1)))\n",
    "                train_acc += torch.sum(y_pred == y_train).float()\n",
    "            train_loss /= (len(dataloader_train.dataset) + 0.)\n",
    "            train_acc /= (len(dataloader_train.dataset) + 0.)\n",
    "\n",
    "            ### Evaluate cross entropy and accuracy over dataloader_test\n",
    "            test_loss = 0.\n",
    "            test_acc = 0.\n",
    "            predictions_test_current = torch.tensor([], dtype=t_type, device=device)\n",
    "            for X_test, y_test in dataloader_test:\n",
    "                X_test = X_test.double().to(device=device).view(X.shape[0], -1)\n",
    "                y_test = y_test.to(device=device)\n",
    "\n",
    "                ### Get output of net before Softmax, mean and log, Shape = [n_particles, batch_size, output_features]\n",
    "                net_pred_pure = dm.predict_net(X_test, inference=False)\n",
    "                net_pred_pure = torch.mean(torch.nn.Softmax(dim=2)(net_pred_pure), dim=0)\n",
    "                predictions_test_current = torch.cat([predictions_test_current, net_pred_pure.data.detach().clone()],\n",
    "                                                     dim=0)\n",
    "\n",
    "                net_pred = torch.log(net_pred_pure)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                test_loss -= torch.sum(torch.gather(net_pred, 1, y_test.view(-1, 1)))\n",
    "                test_acc += torch.sum(y_pred == y_test).float()\n",
    "            test_loss /= (len(dataloader_test.dataset) + 0.)\n",
    "            test_acc /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            ### Evaluate cross entropy and accuracy over dataloader_test using\n",
    "            ### all predictions from previous (@epoch_since_start - @n_warmup_epochs) epochs\n",
    "            test_loss_mean = 0.\n",
    "            test_acc_mean = 0.\n",
    "            if epoch_since_start >= n_warmup_epochs:\n",
    "                predictions_test_cummulative = (\n",
    "                        predictions_test_cummulative * (epoch_since_start - n_warmup_epochs) / (\n",
    "                            epoch_since_start - n_warmup_epochs + 1.) +\n",
    "                        predictions_test_current / (epoch_since_start - n_warmup_epochs + 1.))\n",
    "                log_predictions_test = torch.log(predictions_test_cummulative)\n",
    "                y_pred_all = torch.argmax(log_predictions_test, dim=1)\n",
    "\n",
    "                test_loss_mean = -torch.sum(torch.gather(log_predictions_test, 1, y_test_all.view(-1, 1)))\n",
    "                test_acc_mean = torch.sum(y_pred_all == y_test_all).float()\n",
    "                test_loss_mean /= (len(dataloader_test.dataset) + 0.)\n",
    "                test_acc_mean /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            ### Evaluate cross entropy and accuracy over dataloader_test using\n",
    "            ### all predictions from previous @n_previous epochs\n",
    "            test_loss_mean_previous = 0.\n",
    "            test_acc_mean_previous = 0.\n",
    "            predictions_test_previous[pointer_to_the_back] = predictions_test_current\n",
    "            if pointer_to_the_back + 1 == n_previous:\n",
    "                pointer_to_the_back = 0\n",
    "            else:\n",
    "                pointer_to_the_back += 1\n",
    "            if epoch_since_start + 1 >= n_previous:\n",
    "                log_predictions_test = torch.log(torch.mean(predictions_test_previous, dim=0))\n",
    "                y_pred_all = torch.argmax(log_predictions_test, dim=1)\n",
    "                test_loss_mean_previous = -torch.sum(torch.gather(log_predictions_test, 1, y_test_all.view(-1, 1)))\n",
    "                test_acc_mean_previous = torch.sum(y_pred_all == y_test_all).float()\n",
    "                test_loss_mean_previous /= (len(dataloader_test.dataset) + 0.)\n",
    "                test_acc_mean_previous /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            ### Append evaluated losses and accuracies\n",
    "            train_losses.append(train_loss.data[0].cpu().numpy())\n",
    "            train_accs.append(train_acc.data[0].cpu().numpy())\n",
    "            test_losses.append(test_loss.data[0].cpu().numpy())\n",
    "            test_accs.append(test_acc.data[0].cpu().numpy())\n",
    "            if epoch_since_start >= n_warmup_epochs:\n",
    "                test_losses_mean.append(test_loss_mean.data[0].cpu().numpy())\n",
    "                test_accs_mean.append(test_acc_mean.data[0].cpu().numpy())\n",
    "            else:\n",
    "                test_losses_mean.append(None)\n",
    "                test_accs_mean.append(None)\n",
    "            if epoch_since_start + 1 >= n_previous:\n",
    "                test_losses_mean_previous.append(test_loss_mean_previous.data[0].cpu().numpy())\n",
    "                test_accs_mean_previous.append(test_acc_mean_previous.data[0].cpu().numpy())\n",
    "            else:\n",
    "                test_losses_mean_previous.append(None)\n",
    "                test_accs_mean_previous.append(None)\n",
    "\n",
    "            ### Print log into console and file\n",
    "            if epoch % n_epochs_log == 0:\n",
    "                sys.stdout.write(\n",
    "                    ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                     '\\nEmpirical Loss (Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {4:.3f}/{5:.3f}/{6:.3f}/{7:.3f}' +\n",
    "                     '\\nAccuracy (Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {8:.3f}/{9:.3f}/{10:.3f}/{11:.3f}\\t'\n",
    "                     ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                              train_loss, test_loss, test_loss_mean, test_loss_mean_previous,\n",
    "                              train_acc, test_acc, test_acc_mean, test_acc_mean_previous\n",
    "                              )\n",
    "                )\n",
    "                if log_file_name is not None:\n",
    "                    log_file = open(log_file_name, 'a')\n",
    "                    log_file.write(\n",
    "                        ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                         '\\nEmpirical Loss(Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {4:.3f}/{5:.3f}/{6:.3f}/{7:.3f}' +\n",
    "                         '\\nAccuracy(Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {8:.3f}/{9:.3f}/{10:.3f}/{11:.3f}\\t'\n",
    "                         ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                                  train_loss, test_loss, test_loss_mean, test_loss_mean_previous,\n",
    "                                  train_acc, test_acc, test_acc_mean, test_acc_mean_previous\n",
    "                                  )\n",
    "                    )\n",
    "                    log_file.close()\n",
    "\n",
    "            if epoch % n_epochs_save == 0 and epoch > start_epoch and checkpoint_file_name is not None:\n",
    "                torch.save(dm.state_dict(), checkpoint_file_name.format(epoch))\n",
    "\n",
    "            ### Update step_size\n",
    "            lr_str.step()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    if plot_graphs:\n",
    "        print_plots([[train_losses, test_losses, test_losses_mean, test_losses_mean_previous],\n",
    "                     [train_accs, test_accs, test_accs_mean, test_accs_mean_previous]],\n",
    "                    [['Epochs', ''],\n",
    "                     ['Epochs', '% * 1e-2']],\n",
    "                    [['Cross Entropy Loss (Train)', 'Cross Entropy Loss (Test)', 'Cross Entropy Loss (Test (Mean))',\n",
    "                      'Cross Entropy Loss (Test (Mean (n_prev)))'],\n",
    "                     ['Accuracy (Train)', 'Accuracy (Test)', 'Accuracy (Mean)', 'Accuracy (Mean (n_prev))']\n",
    "                     ],\n",
    "                    plots_file_name\n",
    "                    )\n",
    "    if checkpoint_file_name is not None:\n",
    "        torch.save(dm.state_dict(), checkpoint_file_name.format(epoch))\n",
    "\n",
    "    if verbose:\n",
    "        return (train_losses, test_losses, test_losses_mean, test_losses_mean_previous,\n",
    "                train_accs, test_accs, test_accs_mean, test_accs_mean_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(28 * 28, 300, 5, use_var_prior=False, alpha=None),\n",
    "                        nn.Tanh(),\n",
    "                        SteinLinear(300, 100, 5, use_var_prior=False, alpha=None),\n",
    "                        nn.Tanh(),\n",
    "                        SteinLinear(100, 10, 5, use_var_prior=False, alpha=None)\n",
    "                        ).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distr = ClassificationDistribution(5)\n",
    "dm = DistributionMover(task='net_class',\n",
    "                       n_particles=5,\n",
    "                       n_hidden_dims=5,\n",
    "                       use_latent=True,\n",
    "                       net=net,\n",
    "                       data_distribution=data_distr)\n",
    "lr_str = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.load_state_dict(torch.load('./Experiments/Checkpoints/e0-199_model_13.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_projections(dm, use_real=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14546, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('Experiments/Checkpoints/e0-199_model_4.pth')['lt.theta_0'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 1. particles - 1 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### MAP estimate for MNIST classification task\n",
    "net_1 = nn.Sequential(SteinLinear(28 * 28, 18, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 1, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_1 = ClassificationDistribution(1)\n",
    "dm_1 = DistributionMover(task='net_class', n_particles=1, use_latent=False, net=net_1, data_distribution=data_distr_1)\n",
    "lr_str_1 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_1 = 'model_1'\n",
    "version_1 = 19\n",
    "checkpoint_file_name_1 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_1 + '.pth'\n",
    "plots_file_name_1 = './Experiments/Plots/' + own_name_1 + '.png'\n",
    "log_file_name_1 = './Experiments/Logs/' + own_name_1 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_1.format(version_1)):\n",
    "    dm_1.load_state_dict(torch.load(checkpoint_file_name_1.format(version_1)))\n",
    "    lr_str_1.step_size = dm_1.step_size\n",
    "    lr_str_1.iter = dm_1.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_1,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_1, start_epoch=lr_str_1.iter, end_epoch=26,\n",
    "      checkpoint_file_name=checkpoint_file_name_1, plots_file_name=plots_file_name_1, log_file_name=log_file_name_1,\n",
    "      n_warmup_epochs=5\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X, y = next(iter(dataloader_m_train))\n",
    "# X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_1.update_latent_net(h_type=0, kernel_type='rbf', p=None, X_batch=X, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_1.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2. particles - 5 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_2 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_2 = ClassificationDistribution(5)\n",
    "dm_2 = DistributionMover(task='net_class',\n",
    "                         n_particles=5,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_2,\n",
    "                         data_distribution=data_distr_2\n",
    "                        )\n",
    "lr_str_2 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_2 = 'model_2'\n",
    "version_2 = 0\n",
    "checkpoint_file_name_2 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_2 + '.pth'\n",
    "plots_file_name_2 = './Experiments/Plots/' + own_name_2 + '.png'\n",
    "log_file_name_2 = './Experiments/Logs/' + own_name_2 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_2.format(version_2)):\n",
    "    dm_2.load_state_dict(torch.load(checkpoint_file_name_2.format(version_2)))\n",
    "    lr_str_2.step_size = dm_2.step_size\n",
    "    lr_str_2.iter = dm_2.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_2,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_2, start_epoch=lr_str_2.iter, end_epoch=lr_str_2.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_2, plots_file_name=plots_file_name_2, log_file_name=log_file_name_2,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X, y = next(iter(dataloader_m_train))\n",
    "# X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_2.update_latent_net(h_type=0, kernel_type='rbf', p=None, X_batch=X, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_2.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3. particles - 5 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_3 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_3 = ClassificationDistribution(5)\n",
    "dm_3 = DistributionMover(task='net_class',\n",
    "                       n_particles=5,\n",
    "                       use_latent=False,\n",
    "                       net=net_3,\n",
    "                       data_distribution=data_distr_3\n",
    "                      )\n",
    "lr_str_3 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_3 = 'model_3'\n",
    "version_3 = 0\n",
    "checkpoint_file_name_3 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_3 + '.pth'\n",
    "plots_file_name_3 = './Experiments/Plots/' + own_name_3 + '.png'\n",
    "log_file_name_3 = './Experiments/Logs/' + own_name_3 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_3.format(version_3)):\n",
    "    dm_3.load_state_dict(torch.load(checkpoint_file_name_3.format(version_3)))\n",
    "    lr_str_3.step_size = dm_3.step_size\n",
    "    lr_str_3.iter = dm_3.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_3,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_3, start_epoch=lr_str_3.iter, end_epoch=lr_str_3.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_3, plots_file_name=plots_file_name_3, log_file_name=log_file_name_3,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X, y = next(iter(dataloader_m_train))\n",
    "# X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_3.update_latent_net(h_type=0, kernel_type='rbf', p=None, X_batch=X, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_3.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 4. particles - 20 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_4 = nn.Sequential(SteinLinear(28 * 28, 18, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 20, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_4 = ClassificationDistribution(20)\n",
    "dm_4 = DistributionMover(task='net_class',\n",
    "                         n_particles=20,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_4,\n",
    "                         data_distribution=data_distr_4\n",
    "                        )\n",
    "lr_str_4 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_4 = 'model_4'\n",
    "version_4 = 0\n",
    "checkpoint_file_name_4 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_4 + '.pth'\n",
    "plots_file_name_4 = './Experiments/Plots/' + own_name_4 + '.png'\n",
    "log_file_name_4 = './Experiments/Logs/' + own_name_4 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_4.format(version_4)):\n",
    "    dm_4.load_state_dict(torch.load(checkpoint_file_name_4.format(version_4)))\n",
    "    lr_str_4.step_size = dm_4.step_size\n",
    "    lr_str_4.iter = dm_4.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_4,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_4, start_epoch=lr_str_4.iter, end_epoch=lr_str_4.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_4, plots_file_name=plots_file_name_4, log_file_name=log_file_name_4,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MNIST with FC neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_nn = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ").double()\n",
    "optim_nn = torch.optim.Adam(net_nn.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for _, (X, y) in enumerate(dataloader_m_train):\n",
    "        X = X.double().view(X.shape[0], -1)\n",
    "        \n",
    "        optim_nn.zero_grad()\n",
    "        train_loss = nn.CrossEntropyLoss()(net_nn(X), y)\n",
    "        train_loss.backward()\n",
    "        optim_nn.step()\n",
    "   \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_train)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(net_nn(X_train), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 30.\n",
    "        train_acc /= (30. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_test)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(net_nn(X_test), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 30.\n",
    "        test_acc /= (30. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            _, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X.shape[0], -1)\n",
    "y_pred = net_nn(X_test)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marginal_density = lambda x : (0.3 * normal_density(mu=-2., std=1., n=1)(x) + 0.7 * normal_density(mu=2., std=1., n=1)(x))\n",
    "#marginal_density = lambda x : (normal_density(mu=0., std=2., n=1)(x))\n",
    "\n",
    "pdf = []\n",
    "for _ in np.linspace(-10, 10, 1000): \n",
    "    _ = torch.tensor(_, dtype=t_type, device=device)\n",
    "    pdf.append(marginal_density(_))\n",
    "    \n",
    "from pynverse import inversefunc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "cum_density = interp1d(np.linspace(-20, 20, 250), [quad(marginal_density, -20, x)[0] for x in np.linspace(-20, 20, 250)])\n",
    "inv_cum_density = inversefunc(cum_density)\n",
    "real_samples = [inv_cum_density(np.random.random()) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean = quad(lambda x : marginal_density(x) * x, -20, 20)[0]\n",
    "var = quad(lambda x : marginal_density(x) * (x - mean) ** 2, -20, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10, 10, 1000), pdf)\n",
    "plt.plot(real_samples, np.zeros_like(real_samples))\n",
    "sns.kdeplot(real_samples, kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=20, n_hidden_dims=5, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot_projections(dm, use_real=True, N_plots_max=1, pdf=pdf)\n",
    "# plot_projections(dm, use_real=False, N_plots_max=1, pdf=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_projections(dm, use_real=True, kernel='tri', pdf=pdf)\n",
    "            plot_projections(dm, use_real=False, kernel='tri', pdf=pdf)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(torch.mean(dm.lt.transform(dm.particles, n_particles_second=True)),\n",
    " torch.mean(torch.std(dm.lt.transform(dm.particles, n_particles_second=True), dim=1) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conditional Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=2, n_hidden_dims=1, use_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_condition_distribution(dm, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(200): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "            \n",
    "            #plot_projections(dm, use_real=True, pdf=pdf)\n",
    "            plot_condition_distribution(dm, 100000)\n",
    "            \n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-5, 5, 0.025)\n",
    "Y = np.arange(-5, 5, 0.025)\n",
    "XX, YY = np.meshgrid(X, Y)\n",
    "XY = torch.stack([torch.tensor(XX, dtype=t_type), torch.tensor(YY, dtype=t_type)], dim=2)\n",
    "Z = torch.zeros([XY.shape[0], XY.shape[1]])\n",
    "\n",
    "Z = dm.real_target_density(XY.permute(2, 0, 1).view(2, -1)).view(Z.shape)\n",
    "Z = Z.cpu().data.numpy()\n",
    "\n",
    "# Plot the surface.\n",
    "plt.contour(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "# Plot conditioning line\n",
    "XXX, YYY = dm.lt.transform(torch.tensor(X, dtype=t_type, device=device).view(1, -1), n_particles_second=True).data.cpu().numpy()\n",
    "plt.plot(XXX, YYY, 'r', label='Linear manifold', )\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Density contour lines\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare implementations of Stein Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class SVGD():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def svgd_kernel(self, theta, h = -1):\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = h / np.log(theta.shape[0]+1)\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = 2. * dxkxy / (h)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    " \n",
    "    def update(self, x0, lnprob, n_iter = 1000, stepsize = 1e-3, bandwidth = -1, alpha = 0.9, debug = False):\n",
    "        # Check input\n",
    "        if x0 is None or lnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        theta = np.copy(x0) \n",
    "        \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter+1) % 1000 == 0:\n",
    "                print( 'iter ' + str(iter+1) )\n",
    "            \n",
    "            lnpgrad = lnprob(theta)\n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(theta, h = bandwidth)  \n",
    "            grad_theta = (np.matmul(kxy, lnpgrad) + dxkxy) / x0.shape[0]  \n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + grad_theta ** 2\n",
    "            else:\n",
    "                historical_grad = alpha * historical_grad + (1 - alpha) * (grad_theta ** 2)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            theta = theta + stepsize * adj_grad \n",
    "            \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_ex = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lnprob(particles):\n",
    "    grad_log_term = torch.zeros([particles.shape[0], particles.shape[1]], dtype=t_type, device=device)\n",
    "        \n",
    "    for idx in range(100):\n",
    "        particle = torch.tensor(particles[idx:idx+1], dtype=t_type, device=device, requires_grad=True)\n",
    "        log_term = torch.log(dm_ex.target_density(particle.t()))\n",
    "        grad_log_term[idx] = torch.autograd.grad(log_term, particle,\n",
    "                                                     only_inputs=True, retain_graph=False, \n",
    "                                                     create_graph=False, allow_unused=False)[0]\n",
    "    return grad_log_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = dm_ex.particles.data.clone().detach().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare kernel implementation\n",
    "\n",
    "ker, grad_ker = dm_ex.calc_kernel_term_latent(10.)\n",
    "print(dm_ex.h)\n",
    "grad_ker_sum = torch.sum(grad_ker, dim=1)\n",
    "\n",
    "ker_l, grad_ker_sum_l = dm_svgd.svgd_kernel(result, h=10.)\n",
    "\n",
    "print((ker - torch.tensor(ker_l, dtype=t_type, device=device)).norm())\n",
    "print((grad_ker_sum - torch.tensor(grad_ker_sum_l, dtype=t_type, device=device).t()).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare log_term implementation\n",
    "(lnprob(result) - dm_ex.calc_log_term_latent().t()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(201): \n",
    "    try:\n",
    "        result = dm_svgd.update(result, lnprob, stepsize=1e-2, bandwidth=-1, n_iter=20)\n",
    "        result = torch.tensor(result, dtype=t_type, device=device)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(result.data.cpu().numpy(), torch.zeros_like(result).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(result.data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(2000): \n",
    "    try:\n",
    "        dm_ex.update_latent(h_type=0)\n",
    "\n",
    "        if _ % 30 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm_ex.h))\n",
    "            \n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(dm_ex.particles.t().data.cpu().numpy(), torch.zeros_like(dm_ex.particles.t()).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(dm_ex.particles.t().data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(result - dm_ex.particles.t()).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"Stein-Variational-Gradient-Descent/python/\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.matlib as nm\n",
    "from svgd import SVGD\n",
    "\n",
    "r\"\"\"\n",
    "    Example of Bayesian Logistic Regression (the same setting as Gershman et al. 2012):\n",
    "    The observed data D = {X, y} consist of N binary class labels, \n",
    "    y_t \\in {-1,+1}, and d covariates for each datapoint, X_t \\in R^d.\n",
    "    The hidden variables \\theta = {w, \\alpha} consist of d regression coefficients w_k \\in R,\n",
    "    and a precision parameter \\alpha \\in R_+. We assume the following model:\n",
    "        p(\\alpha) = Gamma(\\alpha; a, b)\n",
    "        p(w_k | a) = N(w_k; 0, \\alpha^-1)\n",
    "        p(y_t = 1| x_t, w) = 1 / (1+exp(-w^T x_t))\n",
    "\"\"\"\n",
    "class BayesianLR:\n",
    "    def __init__(self, X, Y, batchsize=100, a0=1, b0=0.01):\n",
    "        self.X, self.Y = X, Y\n",
    "        # TODO. Y in \\in{+1, -1}\n",
    "        self.batchsize = min(batchsize, X.shape[0])\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.permutation = np.random.permutation(self.N)\n",
    "        self.iter = 0\n",
    "    \n",
    "        \n",
    "    def dlnprob(self, theta):\n",
    "        \n",
    "        if self.batchsize > 0:\n",
    "            batch = [ i % self.N for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize) ]\n",
    "            ridx = self.permutation[batch]\n",
    "            self.iter += 1\n",
    "        else:\n",
    "            ridx = np.random.permutation(self.X.shape[0])\n",
    "            \n",
    "        Xs = self.X[ridx, :]\n",
    "        Ys = self.Y[ridx]\n",
    "        \n",
    "        w = theta[:, :-1]  # logistic weights\n",
    "        alpha = np.exp(theta[:, -1])  # the last column is logalpha\n",
    "        d = w.shape[1]\n",
    "        \n",
    "        wt = np.multiply((alpha / 2), np.sum(w ** 2, axis=1))\n",
    "        \n",
    "        coff = np.matmul(Xs, w.T)\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-1 * coff))\n",
    "        \n",
    "        dw_data = np.matmul(((nm.repmat(np.vstack(Ys), 1, theta.shape[0]) + 1) / 2.0 - y_hat).T, Xs)  # Y \\in {-1,1}\n",
    "        dw_prior = -np.multiply(nm.repmat(np.vstack(alpha), 1, d) , w)\n",
    "        dw = dw_data * 1.0 * self.X.shape[0] / Xs.shape[0] + dw_prior  # re-scale\n",
    "        \n",
    "        dalpha = d / 2.0 - wt + (self.a0 - 1) - self.b0 * alpha + 1  # the last term is the jacobian term\n",
    "        \n",
    "        return np.hstack([dw, np.vstack(dalpha)])  # % first order derivative \n",
    "    \n",
    "    def evaluation(self, theta, X_test, y_test):\n",
    "        theta = theta[:, :-1]\n",
    "        M, n_test = theta.shape[0], len(y_test)\n",
    "\n",
    "        prob = np.zeros([n_test, M])\n",
    "        for t in range(M):\n",
    "            coff = np.multiply(y_test, np.sum(-1 * np.multiply(nm.repmat(theta[t, :], n_test, 1), X_test), axis=1))\n",
    "            prob[:, t] = np.divide(np.ones(n_test), (1 + np.exp(coff)))\n",
    "        \n",
    "        prob = np.mean(prob, axis=1)\n",
    "        acc = np.mean(prob > 0.5)\n",
    "        llh = np.mean(np.log(prob))\n",
    "        return [acc, llh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = X_test.shape[0] + X_train.shape[0]\n",
    "X_input = np.hstack([X, np.ones([N, 1])])\n",
    "y_input = y\n",
    "d = X_input.shape[1]\n",
    "D = d + 1\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.2, random_state=42)\n",
    "    \n",
    "a0, b0 = 1, 0.01 #hyper-parameters\n",
    "model = BayesianLR(X_train, y_train, 100, a0, b0) # batchsize = 100\n",
    "    \n",
    "# initialization\n",
    "M = 100  # number of particles\n",
    "theta0 = np.zeros([M, D]);\n",
    "alpha0 = np.random.gamma(a0, b0, M); \n",
    "for i in range(M):\n",
    "    theta0[i, :] = np.hstack([np.random.normal(0, np.sqrt(1 / alpha0[i]), d), np.log(alpha0[i])])\n",
    "    \n",
    "theta = SVGD().update(x0=theta0, lnprob=model.dlnprob, bandwidth=-1, n_iter=6000, stepsize=0.05, alpha=0.9, debug=True)\n",
    "    \n",
    "print('[accuracy, log-likelihood]')\n",
    "print(model.evaluation(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Metropolis–Hastings algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MHAlgorithm():\n",
    "    def __init__(self, n_dims): \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                           0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "        self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "        \n",
    "#         self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "#         self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "    \n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_dims, 1],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., -1.)\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "    def generate_next(self):\n",
    "        previous = self.particles[:, -1].unsqueeze(1)\n",
    "#         self.additional_distribution = normal_density(self.n_dims, previous, 1, n_particles_second=True)\n",
    "#         candidate = self.additional_distribution.get_sample()\n",
    "        \n",
    "#         alpha = (self.target_density(candidate) * self.additional_distribution.unnormed_density(previous) / \n",
    "#                 (self.target_density(previous) * self.additional_distribution.unnormed_density(candidate)))\n",
    "        candidate = previous + torch.zeros([self.n_dims, 1], device=device, dtype=t_type).uniform_(-1., 1.)\n",
    "        alpha = self.target_density(candidate) / self.target_density(previous)\n",
    "        \n",
    "#         print(self.target_density(candidate)[0].data.numpy(), self.additional_distribution.unnormed_density(previous)[0].data.numpy(),\n",
    "#               self.target_density(previous)[0].data.numpy(), self.additional_distribution.unnormed_density(candidate)[0].data.numpy())\n",
    "        \n",
    "        if alpha > self.one:\n",
    "            self.particles = torch.cat([self.particles, candidate], dim=1)\n",
    "        else:\n",
    "            random = torch.bernoulli(alpha)\n",
    "            if random:\n",
    "                self.particles = torch.cat([self.particles, candidate], dim=1)                \n",
    "            else:\n",
    "                self.particles = torch.cat([self.particles, previous], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mh = MHAlgorithm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    mh.generate_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(mh.particles.t().data.cpu().numpy()[:,1], \n",
    "            kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(mh.particles.t().data.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import time\n",
    "\n",
    "mu,sig,N = 0,1,1000000\n",
    "pts = []\n",
    "\n",
    "def q(x):\n",
    "#     return (1/(math.sqrt(2*math.pi*sig**2)))*(math.e**(-((x-mu)**2)/(2*sig**2)))\n",
    "#     return normal_density(1, 0., 2., n_particles_second=True).unnormed_density(x)\n",
    "    return (0.3 * normal_density(1, -2., 1., n_particles_second=True).unnormed_density(x) +\n",
    "            0.7 * normal_density(1, 2., 1., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "def metropolis(N):\n",
    "    r = np.zeros(1)\n",
    "    p = q(r[0])\n",
    "    pts = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        rn = r + np.random.uniform(-1,1)\n",
    "        pn = q(rn[0])\n",
    "        if pn >= p:\n",
    "            p = pn\n",
    "            r = rn\n",
    "        else:\n",
    "            u = np.random.rand()\n",
    "            if u < pn/p:\n",
    "                p = pn\n",
    "                r = rn\n",
    "        pts.append(r)\n",
    "    \n",
    "    pts = np.array(pts)\n",
    "    return pts\n",
    "    \n",
    "def hist_plot(array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1,)\n",
    "    ax.hist(array, bins=1000)    \n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(metropolis(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gg =  lambda : (normal_density(1, -60., 20., n_particles_second=True).get_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot([gg()[0,0] for _ in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

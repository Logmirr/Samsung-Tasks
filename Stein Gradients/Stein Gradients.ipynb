{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orth\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "device = None\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    use_cuda = True\n",
    "\n",
    "# Ignore cuda\n",
    "# use_cuda = False\n",
    "# device = None\n",
    "\n",
    "# Set DoubleTensor as a base type for computations\n",
    "t_type = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import functools\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_plots(data, axis, labels, file_name=None):\n",
    "    n_plots = len(data)\n",
    "    plt.figure(figsize=(30, (n_plots // 3 + 1) * 10))\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        plt.subplot(n_plots // 3 + 1, 3, idx + 1)\n",
    "        for jdx in range(len(data[idx])):\n",
    "            plt.plot(data[idx][jdx], label=labels[idx][jdx])\n",
    "        plt.xlabel(axis[idx][0], fontsize=16)\n",
    "        plt.ylabel(axis[idx][1], fontsize=16)\n",
    "        plt.legend(loc=0, fontsize=16)\n",
    "    if file_name is not None:\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_projections(dm=None, use_real=True, kernel='tri', pdf=None, n_plots_max=10):\n",
    "    \"\"\"\n",
    "        Plot marginal kernel density estimation\n",
    "    Args:\n",
    "        dm (DistributionMover): class containing particles which define distribution\n",
    "        use_real (bool): If set to True then apply transformation dm.lt.transform before creating plot\n",
    "        kernel (str): Kernel type for kernel density estimation\n",
    "        pdf (array_like, None): Samples from target distribution\n",
    "        n_plots_max (int): Maximum number of plots\n",
    "    \"\"\"\n",
    "    if use_real:\n",
    "        if not dm.use_latent:\n",
    "            return\n",
    "        n_plots = dm.lt.A.shape[0]\n",
    "    else:\n",
    "        n_plots = dm.particles.shape[0]\n",
    "    if n_plots > 6:\n",
    "        scale_factor = 15\n",
    "    else:\n",
    "        scale_factor = 5\n",
    "\n",
    "    n_plots = min(n_plots, n_plots_max)\n",
    "\n",
    "    plt.figure(figsize=(3 * scale_factor, (n_plots // 3 + 1) * scale_factor))\n",
    "\n",
    "    for idx in range(n_plots):\n",
    "        slice_dim = idx\n",
    "\n",
    "        plt.subplot(n_plots // 3 + 1, 3, idx + 1)\n",
    "\n",
    "        if use_real:\n",
    "            particles = dm.lt.transform(dm.particles, n_particles_second=True).t()[:, slice_dim]\n",
    "        else:\n",
    "            particles = dm.particles.t()[:, slice_dim]\n",
    "\n",
    "        if pdf is not None:\n",
    "            plt.plot(np.linspace(-10, 10, len(pdf), dtype=np.float64), pdf)\n",
    "        plt.plot(particles.data.cpu().numpy(), torch.zeros_like(particles).data.cpu().numpy(), 'ro')\n",
    "        sns.kdeplot(particles.data.cpu().numpy(),\n",
    "                    kernel=kernel, color='darkblue', linewidth=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_condition_distribution(dm, n_samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dm (DistributionMover): object contains unconditioned density, linear manifold and particles\n",
    "        n_samples (int): number of samples\n",
    "    Return:\n",
    "        (points, weight)\n",
    "    \"\"\"\n",
    "    if not dm.use_latent:\n",
    "        return\n",
    "\n",
    "    points = torch.zeros([dm.n_hidden_dims, n_samples], dtype=t_type, device=device).uniform_(-10, 10)\n",
    "    weight = dm.real_target_density(dm.lt.transform(points, n_particles_second=True))\n",
    "    points = points.view(-1)\n",
    "\n",
    "    plt.hist(points.data.cpu().numpy(), weights=weight.data.cpu().numpy(), density=True, bins=100, alpha=0.5,\n",
    "             label='True conditional density')\n",
    "    plt.plot(dm.particles.data.cpu().numpy(), torch.zeros_like(dm.particles).data.cpu().numpy(), 'ro')\n",
    "    sns.kdeplot(dm.particles[0, :].data.cpu().numpy(),\n",
    "                kernel='tri', color='darkblue', linewidth=4, label='Approximated conditional density')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_diffs(x, y, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (2D array_like)\n",
    "        y (2D array_like)\n",
    "        n_particles_second (bool)\n",
    "        if n_particles_second == True:\n",
    "            x is a dxN matrix\n",
    "            y is an optional dxM matrix\n",
    "            Output - diffs is a dxNxM matrix where diffs[i,j] is the subtraction between x[:,i] and y[:,j]\n",
    "                    i.e. diffs[i,j] = x[:,i] - y[:,j]\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            x is a Nxd matrix\n",
    "            y is an optional Mxd matrix\n",
    "            Output - diffs is a NxMxd matrix where diffs[i,j] is the subtraction between x[i,:] and y[j,:]\n",
    "                    i.e. diffs[i,j] = x[i,:]-y[j,:]\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return x[:, :, np.newaxis] - y[:, np.newaxis, :]\n",
    "    return x[:, np.newaxis, :] - y[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_dists(diffs=None, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        diffs (2D or 3D array_like)\n",
    "        n_particles_second (bool)\n",
    "        if n_particles_second == True:\n",
    "            diffs is a dxNxM matrix where diffs[i,j] = x[:,i] - y[:,j]\n",
    "            Output - dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "                    i.e. dist[i,j] = ||x[:,i] - y[:,j]||\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            diffs is a NxMxd matrix where diffs[i,j] = x[i,:] - y[j, :]\n",
    "            Output - dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "                i.e. dist[i,j] = ||x[i,:]-y[j,:]||\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return torch.norm(diffs, dim=0)\n",
    "    return torch.norm(diffs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     72,
     87,
     100,
     128
    ]
   },
   "outputs": [],
   "source": [
    "class NormalDensity:\n",
    "    \"\"\"\n",
    "        Multinomial normal density for independent random variables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=1, mu=0., std=1., n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            mu (float, array_like): mean of distribution\n",
    "                Default: 0.\n",
    "                if mu is float - use same mean across all dimensions\n",
    "                if mu is 1D array_like - use different mean for each dimension but same for each particles dimension\n",
    "                if mu is 2D array_like - use different mean for each dimension\n",
    "            std (float, array_like): std of distribution\n",
    "                Default: 1.\n",
    "                if std is float - use same std across all dimensions\n",
    "                if std is 1D array_like - use different std for each dimension but same for each particles dimension\n",
    "                if std is 2D array_like - use different std for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis\n",
    "                Input will be reduced along all axis exclude particle axis\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "\n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "\n",
    "        if isinstance(self.mu, float):\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device).expand(n)\n",
    "        if len(self.mu.shape) == 1:\n",
    "            if self.mu.shape[0] == 1:\n",
    "                self.mu = self.mu.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.mu.shape) == 2:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "        if isinstance(self.std, float):\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device).expand(n)\n",
    "        if len(self.std.shape) == 1:\n",
    "            if len(self.std) == 1:\n",
    "                self.std = self.std.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "        self.zero = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.pi = torch.tensor(math.pi, dtype=t_type, device=device)\n",
    "\n",
    "        # specify axis to reduce\n",
    "        # if n_particles_second == True - n_axis == 0\n",
    "        # if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.pow(self.two * self.pi, -self.n / self.two) /\n",
    "                torch.prod(self.std, dim=n_axis) *\n",
    "                torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)))\n",
    "\n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "\n",
    "    def log_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (-self.n / self.two * torch.log(self.two * self.pi) +\n",
    "                torch.sum(torch.log(self.std), dim=n_axis) -\n",
    "                self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "\n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return -self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)\n",
    "\n",
    "    def get_sample(self):\n",
    "        \"\"\"\n",
    "            Sample from normal distribution\n",
    "        \"\"\"\n",
    "        sample = torch.normal(self.mu, self.std)\n",
    "        if self.n_particles_second:\n",
    "            return sample.view(-1, 1)\n",
    "        else:\n",
    "            return sample.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     5,
     74,
     89,
     103,
     118,
     132,
     147
    ]
   },
   "outputs": [],
   "source": [
    "class GammaDensity:\n",
    "    \"\"\"\n",
    "        Multinomial gamma density for independent random variables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=1, alpha=1, betta=1, n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            alpha (float, array_like): shape of distribution\n",
    "                Default: 1.\n",
    "                if alpha is float - use same shape across all dimensions\n",
    "                if alpha is 1D array_like - use different shape for each dimension but same for each particles dimension\n",
    "                if alpha is 2D array_like - use different shape for each dimension\n",
    "            betta (float, array_like): rate of distribution\n",
    "                Default: 1.\n",
    "                if betta is float - use same rate across all dimensions\n",
    "                if betta is 1D array_like - use different rate for each dimension but same for each particles dimension\n",
    "                if betta is 2D array_like - use different rate for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.betta = betta\n",
    "\n",
    "        if isinstance(self.alpha, float):\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).expand(n)\n",
    "        if len(self.alpha.shape) == 1:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.alpha.shape) == 2:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "        if isinstance(self.betta, float):\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device).expand(n)\n",
    "        if len(self.betta.shape) == 1:\n",
    "            if len(self.betta) == 1:\n",
    "                self.betta = self.betta.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "\n",
    "        # specify axis to reduce\n",
    "        # if n_particles_second == True - n_axis == 0\n",
    "        # if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "\n",
    "        # log Г(alpha)\n",
    "        self.lgamma = torch.lgamma(self.alpha)\n",
    "        # Г(alpha)\n",
    "        self.gamma = torch.exp(self.lgamma)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(self.betta, self.alpha) / self.gamma * torch.pow(x, self.alpha - self.one),\n",
    "                           dim=n_axis) *\n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "\n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(x, self.alpha - self.one), dim=n_axis) *\n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "\n",
    "    def log_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * torch.log(x),\n",
    "                          dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "\n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "\n",
    "    def log_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in point log(x)\n",
    "        Args:\n",
    "            log_x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * log_x,\n",
    "                          dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))\n",
    "\n",
    "    def log_unnormed_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in point log(x)\n",
    "        Args:\n",
    "            log_x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     12,
     78,
     100,
     112
    ]
   },
   "outputs": [],
   "source": [
    "class SteinLinear(nn.Module):\n",
    "    \"\"\"\n",
    "        Custom full connected layer for Stein Gradient Neural Networks\n",
    "        Transformation: y = xA + b\n",
    "        Parameters prior:\n",
    "            1 - p(w, a) = p(w|a)p(a) = П p(w_i|a_i)p(a_i)\n",
    "                p(w_i|a_i) = N(w_i|0, a_i^(-1)); p(a_i) = G(1e-4, 1e-4)\n",
    "\n",
    "            2 - p(w) = П p(w_i)\n",
    "                p(w_i) = N(w_i|0, alpha^(-1))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, n_particles=1, use_bias=True, use_var_prior=True, alpha=1e-2):\n",
    "        super(SteinLinear, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features (int): size of each input sample\n",
    "            out_features (int): size of each output sample\n",
    "            n_particles (int): number of particles\n",
    "            use_bias (bool): If set to False, the layer will not learn an additive bias.\n",
    "                Default: True\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of weight variance\n",
    "                Default: True\n",
    "            alpha (float): If use_var_prior == False - defines weight variance\n",
    "                Default: 1e-2\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_particles = n_particles\n",
    "        self.use_bias = use_bias\n",
    "        self.use_var_prior = use_var_prior\n",
    "\n",
    "        # if alpha is None use GLOROT prior\n",
    "        if alpha is None:\n",
    "            self.alpha_weight = (self.in_features + self.out_features) / 2.\n",
    "            self.alpha_bias = self.out_features / 2.\n",
    "        else:\n",
    "            self.alpha_weight = alpha\n",
    "            self.alpha_bias = alpha\n",
    "\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.zeros([in_features, out_features, n_particles], dtype=t_type, device=device))\n",
    "        if self.use_var_prior:\n",
    "            self.log_weight_alpha = torch.nn.Parameter(\n",
    "                torch.zeros([in_features * out_features, n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_weight_alpha = torch.tensor([math.log(self.alpha_weight)], dtype=t_type, device=device,\n",
    "                                                 requires_grad=False)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros([1, out_features, n_particles], dtype=t_type, device=device))\n",
    "            if self.use_var_prior:\n",
    "                self.log_bias_alpha = torch.nn.Parameter(\n",
    "                    torch.zeros([out_features, n_particles], dtype=t_type, device=device))\n",
    "            else:\n",
    "                self.log_bias_alpha = torch.tensor([math.log(self.alpha_bias)], dtype=t_type, device=device,\n",
    "                                                   requires_grad=False)\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            # define prior on alpha p(a) = G(1e-4, 1e-4)\n",
    "            self.weight_alpha_log_prior = lambda x: (GammaDensity(n=self.log_weight_alpha.shape[0],\n",
    "                                                                  alpha=1e-4,\n",
    "                                                                  betta=1e-4,\n",
    "                                                                  n_particles_second=True\n",
    "                                                                  ).log_unnormed_density_log_x(x))\n",
    "            if self.use_bias:\n",
    "                self.bias_alpha_log_prior = lambda x: (GammaDensity(n=self.log_bias_alpha.shape[0],\n",
    "                                                                    alpha=1e-4,\n",
    "                                                                    betta=1e-4,\n",
    "                                                                    n_particles_second=True\n",
    "                                                                    ).log_unnormed_density_log_x(x))\n",
    "\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # useless function - all initialization defined in DistributionMover class\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.log_weight_alpha.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.log_bias_alpha.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Apply transformation: x_out[i, :, :] = x_in[i, :, :] * W + b[i]\n",
    "        Args:\n",
    "            x (torch.tensor): tensor\n",
    "        Shape:\n",
    "            Input: [n_particles, batch_size, in_features]\n",
    "            Output: [n_particles, batch_size, out_features]\n",
    "        \"\"\"\n",
    "        # NEED SOME OPTIMIZATION TO OMMIT .permute\n",
    "        if self.use_bias:\n",
    "            return torch.bmm(x, self.weight.permute(2, 0, 1)) + self.bias.permute(2, 0, 1)\n",
    "        return torch.bmm(x, self.weight.permute(2, 0, 1))\n",
    "\n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())\n",
    "\n",
    "    def calc_log_prior(self):\n",
    "        \"\"\"\n",
    "            Evaluate log prior of trainable parameters\n",
    "        log p(w,a) = log p(w|a) + log p(a)\n",
    "        \"\"\"\n",
    "        # define prior on weight p(w|a) = N(0, 1 / alpha)\n",
    "        weight_log_prior = lambda x: (NormalDensity(n=self.weight.numel() // self.n_particles,\n",
    "                                                    mu=0.,\n",
    "                                                    std=self.one / torch.exp(self.log_weight_alpha),\n",
    "                                                    n_particles_second=True\n",
    "                                                    ).log_unnormed_density(x))\n",
    "\n",
    "        bias_log_prior = lambda x: (NormalDensity(self.bias.numel() // self.n_particles,\n",
    "                                                  mu=0.,\n",
    "                                                  std=self.one / torch.exp(self.log_bias_alpha),\n",
    "                                                  n_particles_second=True\n",
    "                                                  ).log_unnormed_density(x))\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.use_var_prior:\n",
    "                return (weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(\n",
    "                    self.log_weight_alpha) +\n",
    "                        bias_log_prior(self.bias.view(-1, self.n_particles)) + self.bias_alpha_log_prior(\n",
    "                            self.log_bias_alpha))\n",
    "            return (weight_log_prior(self.weight.view(-1, self.n_particles)) +\n",
    "                    bias_log_prior(self.bias.view(-1, self.n_particles)))\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            return weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(\n",
    "                self.log_weight_alpha)\n",
    "        return weight_log_prior(self.weight.view(-1, self.n_particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     5,
     50,
     61,
     72,
     87
    ]
   },
   "outputs": [],
   "source": [
    "class LinearTransform:\n",
    "    \"\"\"\n",
    "        Class for various linear transformations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_dims, n_hidden_dims, use_identity=False, normalize=False, A=None, theta_0=None, dummy=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_dims (int): dimension of the space\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_identity (bool): If set to True, use 'eye' matrix for transformations\n",
    "            normalize (bool): If set to True, columns of the transformation matrix is an orthonormal basis\n",
    "            A (2D array_like, None): Initial value for transformation matrix\n",
    "                If None then matrix will be sampled from uniform distribution and then orthonormate\n",
    "                Default: None\n",
    "            theta_0 (1D array_like, None): Initial value for bias\n",
    "                If None then matrix will be sampled from uniform distribution\n",
    "                Default: None\n",
    "            dummy (bool): \n",
    "                If True then matrixes initialization ommited and class should be loaded via load_state_dict\n",
    "        \"\"\"\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_identity = use_identity\n",
    "        self.normalize = normalize\n",
    "\n",
    "        if self.use_identity or dummy:\n",
    "            return\n",
    "\n",
    "        self.A = A\n",
    "        self.theta_0 = theta_0\n",
    "\n",
    "        if self.A is None:\n",
    "            self.A = torch.zeros([self.n_dims, self.n_hidden_dims], dtype=t_type, device=device)\n",
    "            self.A.uniform_(-1., 1.)\n",
    "            if self.normalize:\n",
    "                # normalize columns of matrix A\n",
    "                self.A = torch.tensor(orth(self.A.data.cpu().numpy()), dtype=t_type, device=device)\n",
    "\n",
    "        if self.theta_0 is None:\n",
    "            self.theta_0 = torch.zeros([self.n_dims, 1], dtype=t_type, device=device)\n",
    "            self.theta_0.uniform_(-1., 1.)\n",
    "\n",
    "        # A^(t)A\n",
    "        self.AtA = torch.matmul(self.A.t(), self.A)\n",
    "        # (A^(t)A)^(-1)\n",
    "        self.AtA_1 = torch.inverse(self.AtA)\n",
    "        # (A^(t)A)^(-1)A^(t)\n",
    "        self.inverse_base = torch.matmul(self.AtA_1, self.A.t())\n",
    "\n",
    "    def transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Transform thetas as follows:\n",
    "                theta = Atheta` + theta_0\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.A, theta) + self.theta_0\n",
    "        return (torch.matmul(self.A, theta.t()) + self.theta_0).t()\n",
    "\n",
    "    def inverse_transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Apply inverse transformation:\n",
    "                theta` = (A^(t)A)^(-1)A^(t)(theta - theta_0)\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.inverse_base, theta - self.theta_0)\n",
    "        return torch.matmul(self.inverse_base, theta.t() - self.theta_0).t()\n",
    "\n",
    "    def project_inverse(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Project and then apply inverse transform to theta - theta_0:\n",
    "                theta_s_p_i = T^(-1)P(theta - theta_0)= (A^(t)A)^(-1)A^(t)theta\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        # This optimization severely reduces performance!!!!\n",
    "        # use solver trick: theta_s_p_i : A^(t)Atheta_s_p_i = A^(t)theta\n",
    "        if n_particles_second:\n",
    "            # return torch.gesv(torch.matmul(self.A.t(), theta), self.AtA)[0]\n",
    "            return torch.matmul(self.inverse_base, theta)\n",
    "        # return torch.gesv(torch.matmul(self.A.t(), theta.t()), self.AtA)[0].t()\n",
    "        return torch.matmul(theta, self.inverse_base.t())\n",
    "\n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        Example::\n",
    "            >>> module.state_dict().keys()\n",
    "            ['bias', 'weight']\n",
    "        \"\"\"\n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=1)\n",
    "\n",
    "        destination[prefix + 'A'] = self.A if keep_vars else self.A.data\n",
    "        destination[prefix + 'theta_0'] = self.theta_0 if keep_vars else self.theta_0.data\n",
    "        destination[prefix + 'n_dims'] = self.n_dims\n",
    "        destination[prefix + 'n_hidden_dims'] = self.n_hidden_dims\n",
    "        destination[prefix + 'use_identity'] = self.use_identity\n",
    "        destination[prefix + 'normalize'] = self.normalize\n",
    "\n",
    "        return destination\n",
    "\n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        self.__init__(state_dict[prefix + 'n_dims'],\n",
    "                      state_dict[prefix + 'n_hidden_dims'],\n",
    "                      state_dict[prefix + 'use_identity'],\n",
    "                      state_dict[prefix + 'normalize'],\n",
    "                      state_dict[prefix + 'A'],\n",
    "                      state_dict[prefix + 'theta_0'],\n",
    "                      dummy=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0,
     5,
     37,
     75,
     78
    ]
   },
   "outputs": [],
   "source": [
    "class RegressionDistribution(nn.Module):\n",
    "    \"\"\"\n",
    "        Distribution over data for regression task: p(D|w) = N(y_predicted|y, )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_particles, use_var_prior=True, betta=1e-1):\n",
    "        super(RegressionDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of prediction variance\n",
    "                Default: True\n",
    "            betta (float): If use_var_prior == False - defines variance of prediction\n",
    "                Default: 1e-1\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_particles = n_particles\n",
    "        self.use_var_prior = use_var_prior\n",
    "        self.betta = betta\n",
    "\n",
    "        # define betta - variance of data distribution for regression tasks (betta = 1 / std ** 2)\n",
    "        if self.use_var_prior:\n",
    "            self.log_betta = torch.nn.Parameter(torch.zeros([1, self.n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_betta = torch.tensor([math.log(self.betta)], dtype=t_type, device=device, requires_grad=False)\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            # define prior on betta p(betta)\n",
    "            self.betta_log_prior = lambda x: (GammaDensity(n=1,\n",
    "                                                           alpha=1e-4,\n",
    "                                                           betta=1e-4,\n",
    "                                                           n_particles_second=True\n",
    "                                                           ).log_unnormed_density_log_x(x))\n",
    "\n",
    "        # Support tensors for computations\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "\n",
    "    def calc_log_data(self, x, y, y_predict, train_size):\n",
    "        \"\"\"\n",
    "            Evaluate log p(theta)\n",
    "        Args:\n",
    "            x (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predicted values\n",
    "            train_size (int) - size of training dataset\n",
    "        Shapes:\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, 1]\n",
    "        \"\"\"\n",
    "        # squeeze last axis because regression task is being solved\n",
    "        y_predict.squeeze_(2)\n",
    "\n",
    "        batch_size = torch.tensor(y.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "\n",
    "        # define distribution over data p(D|w)\n",
    "        # n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        if self.use_var_prior:\n",
    "            log_data_distr = lambda x: (NormalDensity(n=x.shape[1],\n",
    "                                                      mu=y,\n",
    "                                                      std=self.one / torch.sqrt(torch.exp(\n",
    "                                                          self.log_betta.expand(x.shape[1], self.n_particles).t())),\n",
    "                                                      n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "        else:\n",
    "            log_data_distr = lambda x: (NormalDensity(n=x.shape[1],\n",
    "                                                      mu=y,\n",
    "                                                      std=self.one / torch.sqrt(torch.exp(self.log_betta)),\n",
    "                                                      n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            return train_size / batch_size * log_data_distr(y_predict) + self.betta_log_prior(self.log_betta)\n",
    "        return train_size / batch_size * log_data_distr(y_predict)\n",
    "\n",
    "    def modules(self):\n",
    "        yield self\n",
    "\n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0,
     1,
     10,
     36,
     39
    ]
   },
   "outputs": [],
   "source": [
    "class ClassificationDistribution(nn.Module):\n",
    "    def __init__(self, n_particles):\n",
    "        super(ClassificationDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        \"\"\"\n",
    "        self.n_particles = n_particles\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_log_data(x, y, y_predict, train_size):\n",
    "        \"\"\"\n",
    "            Evaluate log p(theta)\n",
    "        Args:\n",
    "            x (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shapes:\n",
    "            x.shape = [batch_size, in_features]\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        batch_size = torch.tensor(x.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "\n",
    "        # define distribution over data p(D|w)\n",
    "        # n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "\n",
    "        probas = nn.LogSoftmax(dim=2)(y_predict)\n",
    "        probas_selected = torch.gather(input=probas, dim=2,\n",
    "                                       index=y.view(1, -1, 1).expand(probas.shape[0], probas.shape[1], 1)).squeeze(2)\n",
    "        log_data = torch.sum(probas_selected, dim=1)\n",
    "\n",
    "        return train_size / batch_size * log_data\n",
    "\n",
    "    def modules(self):\n",
    "        yield self\n",
    "\n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     1,
     77,
     82,
     130,
     143,
     240,
     271,
     281,
     301,
     340,
     346,
     365,
     421,
     481,
     492,
     499,
     505,
     532,
     541
    ]
   },
   "outputs": [],
   "source": [
    "class DistributionMover(nn.Module):\n",
    "    def __init__(self,\n",
    "                 task='app',\n",
    "                 n_particles=None,\n",
    "                 particles=None,\n",
    "                 target_density=None,\n",
    "                 n_dims=None,\n",
    "                 n_hidden_dims=None,\n",
    "                 use_latent=False,\n",
    "                 net=None,\n",
    "                 data_distribution=None,\n",
    "                 dummy=False\n",
    "                 ):\n",
    "        super(DistributionMover, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task (str):\n",
    "                'app' | 'net_reg' | 'net_class'\n",
    "                - approximate target distribution\n",
    "                - solve regression task using net\n",
    "                - solve classification task using net\n",
    "            n_particles (int): number of particles\n",
    "            particles (2D array_like): array which contains initialized particles\n",
    "            target_density (callable): computes probability density function of target distribution (for 'app' task)\n",
    "            n_dims (int): dimension of the space where optimization is performed\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_latent (bool): If set to True, Subspace Stein is used\n",
    "            acr (list): List contains architecture of object which is used to \n",
    "                        make predictions (for 'net_reg' and' net_class' tasks)\n",
    "            data_distribution (callable): computes probability over data p(D|w) (for 'net_reg' and' net_class' tasks)\n",
    "            dummy (bool): \n",
    "                If True then no initialization performed and class should be loaded via load_state_dict\n",
    "        \"\"\"\n",
    "\n",
    "        self.task = task\n",
    "\n",
    "        self.n_particles = n_particles\n",
    "        self.particles = particles\n",
    "        self.target_density = target_density\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_latent = use_latent\n",
    "        self.net = net\n",
    "        self.data_distribution = data_distribution\n",
    "\n",
    "        if self.task == 'net_reg' or self.task == 'net_class':\n",
    "            self.n_dims = self.numel() // self.n_particles\n",
    "        if not self.use_latent:\n",
    "            self.n_hidden_dims = self.n_dims\n",
    "\n",
    "        # Learnable samples from the target distribution\n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_hidden_dims, self.n_particles],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device\n",
    "        ).uniform_(-2., 2.)\n",
    "\n",
    "        # Class for performing linear transformations\n",
    "        if self.use_latent:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=False,\n",
    "                normalize=True,\n",
    "                dummy=dummy\n",
    "            )\n",
    "        else:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=True,\n",
    "                normalize=True,\n",
    "                dummy=dummy\n",
    "            )\n",
    "\n",
    "        # Functions of probability density of target distribution\n",
    "        if self.net is None:\n",
    "            # use unnormed probability density to speedup computations\n",
    "            if target_density is not None:\n",
    "                self.target_density = target_density\n",
    "                self.real_target_density = target_density\n",
    "            else:\n",
    "                self.target_density = lambda x, *args, **kwargs: (\n",
    "                            0.3 * NormalDensity(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x,\n",
    "                                                                                                                *args,\n",
    "                                                                                                                **kwargs\n",
    "                                                                                                                ) +\n",
    "                            0.7 * NormalDensity(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x,\n",
    "                                                                                                               *args,\n",
    "                                                                                                               **kwargs\n",
    "                                                                                                               )\n",
    "                )\n",
    "\n",
    "                self.real_target_density = lambda x, *args, **kwargs: (\n",
    "                            0.3 * NormalDensity(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                            0.7 * NormalDensity(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs)\n",
    "                )\n",
    "\n",
    "        # Number of iterations since beginning\n",
    "        self.iter = 0\n",
    "        # Number of epochs since beginning\n",
    "        self.epoch = 0\n",
    "        # Burn in coefficient for grag kernel term\n",
    "        self.burn_in_coeff = 0\n",
    "\n",
    "        # Adagrad parameters\n",
    "        self.fudge_factor = torch.tensor(1e-6, dtype=t_type, device=device)\n",
    "        self.step_size = torch.tensor(1e-2, dtype=t_type, device=device)\n",
    "        self.auto_corr = torch.tensor(0.9, dtype=t_type, device=device)\n",
    "\n",
    "        # Gradient history term for adagrad optimization\n",
    "        if self.use_latent:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_hidden_dims, n_particles], dtype=t_type, device=device)\n",
    "            self.historical_grad_theta_0 = torch.zeros(\n",
    "                [self.n_dims, 1], dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_dims, n_particles], dtype=t_type, device=device)\n",
    "\n",
    "        # Factor from kernel\n",
    "        self.h = torch.tensor(0., dtype=t_type, device=device)\n",
    "\n",
    "        # Support tensors for computations\n",
    "        self.N = torch.tensor(self.n_particles, dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.three = torch.tensor(3., dtype=t_type, device=device)\n",
    "\n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        cnt = 0\n",
    "        for module in self.children():\n",
    "            if 'numel' in dir(module):\n",
    "                cnt += module.numel(trainable)\n",
    "        return cnt\n",
    "\n",
    "    def calc_kernel_term_latent(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float):\n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel\n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output:\n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        # power for rational kernel\n",
    "        p = torch.tensor(p, dtype=t_type, device=device) if p is not None else None\n",
    "\n",
    "        # theta = Atheta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        # diffs[i, j] = A(theta`_i - theta`_j)\n",
    "        diffs = pairwise_diffs(real_particles, real_particles, n_particles_second=True)\n",
    "        # dists[i, j] = ||A(theta`_i - theta`_j)||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        # sq_dists[i, j] = ||A(theta`_i - theta`_j)||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "\n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * self.n_dims\n",
    "        elif h_type == 2:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1) * self.n_dims\n",
    "        elif h_type == 5:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / (torch.pow(self.N, self.two) - self.one)\n",
    "        elif h_type == 6:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / (torch.pow(self.N, -self.one / p) - self.one)\n",
    "        elif h_type == 7:\n",
    "            med = torch.median(torch.matmul(real_particles.t(), real_particles)) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "\n",
    "        kernel = None\n",
    "        grad_kernel = None\n",
    "        if kernel_type == 'rbf':\n",
    "            # RBF Kernel:\n",
    "            # kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "            kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "            # grad_kernel[i, j] = -2/h * A(theta`_i - theta`_j) * kernel[i, j]\n",
    "            grad_kernel = -self.two / self.h * kernel.unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'imq':\n",
    "            # IMQ Kernel:\n",
    "            # kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, -self.one / self.two)\n",
    "            # grad_kernel[i, j] = -1/h * A(theta`_i - theta`_j) * kernel^(3)[i, j]\n",
    "            grad_kernel = -self.one / self.h * torch.pow(kernel, self.three).unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'exp':\n",
    "            # Exponential Kernel:\n",
    "            # kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "            kernel = torch.exp(self.one / self.h * torch.matmul(real_particles.t(), real_particles))\n",
    "            # grad_kernel[i, j] = 1/h * (Atheta`_j + theta_0) * kernel[i, j]\n",
    "            grad_kernel = 1. / self.h * kernel.unsqueeze(0) * real_particles.unsqueeze(1)\n",
    "        elif kernel_type == 'rat':\n",
    "            # RAT Kernel:\n",
    "            # kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, p)\n",
    "            # grad_kernel[i, j] = p/h * A(theta`_i - theta`_j) * kernel^((p - 1)/p)[i, j]\n",
    "            grad_kernel = p / self.h * torch.pow(kernel, (self.p - self.one) / p).unsqueeze(0) * diffs\n",
    "\n",
    "        return kernel, grad_kernel\n",
    "\n",
    "    def calc_kernel_term_latent_net(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float):\n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel\n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output:\n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        return self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "\n",
    "    def calc_log_prior_net(self):\n",
    "        \"\"\"\n",
    "            Traverse all modules and evaluate weights log prior\n",
    "        \"\"\"\n",
    "        log_prior = 0\n",
    "        for module in self.children():\n",
    "            if 'calc_log_prior' in dir(module):\n",
    "                log_prior += module.calc_log_prior()\n",
    "        return log_prior\n",
    "\n",
    "    def calc_log_term_latent(self):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "\n",
    "        # theta = A theta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        # compute log data term log p(D|w)\n",
    "        log_term = torch.log(self.target_density(real_particles))\n",
    "\n",
    "        # evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "\n",
    "        grad_log_term = real_particles.grad\n",
    "\n",
    "        return grad_log_term\n",
    "\n",
    "    def calc_log_term_latent_net(self, x, y, train_size):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta))\n",
    "        Args:\n",
    "            x (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input:\n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output:\n",
    "                [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "\n",
    "        # get real net parameters: theta_i = A theta`_i + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        # init net with real parameters\n",
    "        self.vector_to_parameters(real_particles.view(-1), self.parameters_net())\n",
    "        # compute log prior of all weight in the net\n",
    "        log_prior = self.calc_log_prior_net()\n",
    "\n",
    "        # get prediction for the batch of data\n",
    "        y_predict = self.predict_net(x)\n",
    "        # compute log data term log p(D|w)\n",
    "        log_data = self.data_distribution.calc_log_data(x, y, y_predict, train_size)\n",
    "\n",
    "        # log_term = log p(theta) = log p_prior(theta) + log p_data(D|theta)\n",
    "        log_term = log_prior + log_data\n",
    "\n",
    "        # evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "\n",
    "        # collect all gradients into one vector\n",
    "        grad_log_term = self.parameters_grad_to_vector(self.parameters_net()).view(-1, self.n_particles)\n",
    "\n",
    "        return grad_log_term\n",
    "\n",
    "    def parameters_net(self):\n",
    "        \"\"\"\n",
    "            Return all trainable parameters\n",
    "        \"\"\"\n",
    "        return chain(self.net.parameters(), self.data_distribution.parameters())\n",
    "\n",
    "    def predict_net(self, x, inference=False):\n",
    "        \"\"\"\n",
    "            Use net to make predictions\n",
    "            Args:\n",
    "                x (array_like): batch of data\n",
    "                inference (bool): if False return logits instead of logp\n",
    "        \"\"\"\n",
    "        predictions = self.net(x.unsqueeze(0).expand(self.n_particles, *x.shape))\n",
    "        if self.task == 'net_reg':\n",
    "            if inference:\n",
    "                return torch.mean(predictions, dim=0)\n",
    "            else:\n",
    "                return predictions\n",
    "        elif self.task == 'net_class':\n",
    "            if inference:\n",
    "                return torch.log(torch.mean(torch.nn.Softmax(dim=2)(predictions), dim=0))\n",
    "            else:\n",
    "                return predictions\n",
    "\n",
    "    def update_latent(self,\n",
    "                      h_type, kernel_type='rbf', p=None,\n",
    "                      step_size=None,\n",
    "                      move_theta_0=False,\n",
    "                      burn_in=False, burn_in_coeff=None,\n",
    "                      epoch=None\n",
    "                      ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "\n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "\n",
    "        self.iter += 1\n",
    "\n",
    "        # Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent()\n",
    "\n",
    "        # Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "\n",
    "        # Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        # Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        # Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        # Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "        # Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            # Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "\n",
    "            # Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (\n",
    "                            self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            # Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "\n",
    "    def update_latent_net(self,\n",
    "                          h_type, kernel_type='rbf', p=None,\n",
    "                          x_batch=None, y_batch=None, train_size=None,\n",
    "                          step_size=None,\n",
    "                          move_theta_0=False,\n",
    "                          burn_in=False, burn_in_coeff=None,\n",
    "                          epoch=None\n",
    "                          ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "\n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "\n",
    "        self.iter += 1\n",
    "        self.net.zero_grad()\n",
    "        self.data_distribution.zero_grad()\n",
    "\n",
    "        # Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent_net(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_net(x_batch, y_batch, train_size)\n",
    "\n",
    "        # Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "\n",
    "        # Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        # Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        # Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        # Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "        # Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            # Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "\n",
    "            # Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (\n",
    "                            self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            # Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_to_parameters(vec, parameters):\n",
    "        pointer = 0\n",
    "        for param in parameters:\n",
    "            # The length of the parameter\n",
    "            num_param = param.numel()\n",
    "            # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "            param.data = vec[pointer:pointer + num_param].view_as(param).data\n",
    "            # Increment the pointer\n",
    "            pointer += num_param\n",
    "\n",
    "    @staticmethod\n",
    "    def parameters_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.view(-1))\n",
    "        return torch.cat(vec)\n",
    "\n",
    "    @staticmethod\n",
    "    def parameters_grad_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.grad.view(-1))\n",
    "        return torch.cat(vec)\n",
    "\n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        \"\"\"\n",
    "        destination = super(DistributionMover, self).state_dict(destination, prefix, keep_vars)\n",
    "\n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=self._version)\n",
    "\n",
    "        destination[prefix + 'particles'] = self.particles if keep_vars else self.particles.data\n",
    "        destination[prefix + 'historical_grad'] = self.historical_grad if keep_vars else self.historical_grad.data\n",
    "        if self.use_latent:\n",
    "            destination[prefix + 'historical_grad_theta_0'] = (self.historical_grad_theta_0 if keep_vars\n",
    "                                                               else self.historical_grad_theta_0.data)\n",
    "            self.lt.state_dict(destination, prefix + 'lt' + '.', keep_vars=keep_vars)\n",
    "        destination[prefix + 'step_size'] = self.step_size\n",
    "        destination[prefix + 'iter'] = self.iter\n",
    "        destination[prefix + 'epoch'] = self.epoch\n",
    "\n",
    "        return destination\n",
    "\n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        super(DistributionMover, self).load_state_dict(state_dict, prefix)\n",
    "\n",
    "        self.particles.copy_(state_dict[prefix + 'particles'])\n",
    "        self.historical_grad.copy_(state_dict[prefix + 'historical_grad'])\n",
    "        self.step_size = state_dict[prefix + 'step_size']\n",
    "        self.iter = state_dict[prefix + 'iter']\n",
    "        self.epoch = state_dict[prefix + 'epoch']\n",
    "\n",
    "        if self.use_latent:\n",
    "            self.historical_grad_theta_0.copy_(state_dict[prefix + 'historical_grad_theta_0'])\n",
    "            self.lt.load_state_dict(state_dict, prefix + 'lt' + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0,
     1,
     12
    ]
   },
   "outputs": [],
   "source": [
    "class LRStrategy:\n",
    "    def __init__(self, step_size, factor=0.1, n_epochs=1, patience=10):\n",
    "        \"\"\"\n",
    "            Multiply @step_size by factor each @n_epochs epochs\n",
    "            Freeze @step_size after @patience epochs\n",
    "        \"\"\"\n",
    "        self.step_size = step_size\n",
    "        self.factor = factor\n",
    "        self.n_epochs = n_epochs\n",
    "        self.patience = patience\n",
    "        self.iter = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.iter += 1\n",
    "        if self.iter < self.patience and self.iter % self.n_epochs == 0:\n",
    "            self.step_size *= self.factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add some methods to nn.Sequential to make code clear \n",
    "\n",
    "setattr(nn.Sequential, \"numel\", DistributionMover.numel)\n",
    "setattr(nn.Sequential, \"calc_log_prior\", DistributionMover.calc_log_prior_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Bostor housing dataset for regression task\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "x = boston['data']\n",
    "y = boston['target']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "x_train, x_test, y_train, y_test = (torch.tensor(x_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(x_test, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_test, dtype=t_type, device=device))\n",
    "\n",
    "### Linear Regression baseline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.tensor(model.predict(x_test), dtype=t_type, device=device), y_test), \n",
    "      torch.nn.MSELoss()(torch.tensor(model.predict(x_train), dtype=t_type, device=device), y_train))\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.mean(y_train).expand(y_test.shape[0]), y_test), \n",
    "      torch.nn.MSELoss()(torch.mean(y_train).expand(y_train.shape[0]), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Check all functions\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=5, n_particles=10, use_latent=True)\n",
    "dm.calc_log_term_latent()\n",
    "dm.calc_kernel_term_latent(0)\n",
    "dm.update_latent(0)\n",
    "\n",
    "net = nn.Sequential(SteinLinear(13, 1, 10, use_var_prior=True))\n",
    "dd = RegressionDistribution(n_particles=10, use_var_prior=False)\n",
    "dm = DistributionMover(task='net_reg', n_hidden_dims=5, n_particles=10, use_latent=True, net=net, data_distribution=dd)\n",
    "dm.calc_log_term_latent_net(x_train, y_train, x_train.shape[0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(13, 1, 100, use_var_prior=True, alpha=1e2, use_bias=True))\n",
    "data_distr = RegressionDistribution(100, use_var_prior=True, betta=1e-1)\n",
    "dm = DistributionMover(task='net_reg', n_particles=100, use_latent=False, net=net, data_distribution=data_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alpha = dm.net[0].alpha_weight\n",
    "betta = dm.data_distribution.betta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Append column of ones to data \n",
    "# xx = x_train\n",
    "# xx_test = x_test\n",
    "xx = torch.cat([x_train, torch.ones([x_train.shape[0], 1], dtype=t_type, device=device)], dim=1)\n",
    "xx_test = torch.cat([x_test, torch.ones([x_test.shape[0], 1], dtype=t_type, device=device)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sigma = torch.inverse(betta * xx.t() @ xx + alpha * torch.eye(xx.shape[1], dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu = betta * sigma @ xx.t() @ y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu @ xx.t(), y_train), torch.nn.MSELoss()(mu @ xx_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import *\n",
    "\n",
    "mod = WLS(y_train.data.cpu().numpy(), xx.data.cpu().numpy())\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stein_mu = (torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu, torch.tensor(results.params, dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, particles_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(sum(np.logical_and(results.conf_int()[:,0] < stein_mu.data.cpu().numpy(), stein_mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < mu.data.cpu().numpy(), mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < particles_mean.data.cpu().numpy(), particles_mean.data.cpu().numpy() < results.conf_int()[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    step_size = 0.002\n",
    "    dm.historical_grad.zero_()\n",
    "    for _ in range(100000):\n",
    "        dm.update_latent_net(h_type=1, kernel_type='rbf', p=-1, x_batch=x_train, y_batch=y_train, train_size=x_train.shape[0], step_size=step_size)\n",
    "        train_loss = torch.nn.MSELoss()(dm.predict_net(x_train, inference=True).view(-1), y_train)\n",
    "        test_loss = torch.nn.MSELoss()(dm.predict_net(x_test, inference=True).view(-1), y_test)\n",
    "        \n",
    "        if _ % 10 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train): {1:.3f}\\t Empirical Loss(Test): {2:.3f}\\t Kernel factor: {3:.3f}'.format(\n",
    "                            _, train_loss, test_loss, dm.h))\n",
    "            \n",
    "#             plot_projections(dm, use_real=True)\n",
    "#             plot_projections(dm, use_real=False)\n",
    "            plt.pause(1e-300)\n",
    "            \n",
    "        if _ % 3000 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset with class selection\n",
    "\n",
    "sys.path.insert(0, '/home/m.nakhodnov/Samsung-Tasks/Datasets/MyMNIST')\n",
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform)\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform)\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=100, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=100, shuffle=False)\n",
    "\n",
    "\n",
    "transform_aug = transforms.Compose([\n",
    "        transforms.RandomAffine(degrees=90., \n",
    "                                translate=(0.25, 0.25),\n",
    "                                scale=(0.8, 1.2)\n",
    "                               ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset_m_train_aug = MNIST_Class_Selection('.', train=True, download=True, transform=transform_aug)\n",
    "dataset_m_test_aug = MNIST_Class_Selection('.', train=False, transform=transform_aug)\n",
    "\n",
    "\n",
    "dataloader_m_train_aug = DataLoader(dataset_m_train_aug, batch_size=100, shuffle=True)\n",
    "dataloader_m_test_aug = DataLoader(dataset_m_test_aug, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(dm,\n",
    "          dataloader_train, dataloader_test,\n",
    "          lr_str, start_epoch, end_epoch, n_epochs_save=20, n_epochs_log=1,\n",
    "          move_theta_0=False, plot_graphs=True, verbose=False,\n",
    "          checkpoint_file_name=None, plots_file_name=None, log_file_name=None,\n",
    "          n_warmup_epochs=16, n_previous=10\n",
    "          ):\n",
    "    # Get all y_test in one tensor\n",
    "    y_test_all = torch.tensor([], dtype=torch.int64, device=device)\n",
    "    for _, y_test in dataloader_test:\n",
    "        y_test = y_test.to(device=device)\n",
    "        y_test_all = torch.cat([y_test_all, y_test.data.detach().clone()], dim=0)\n",
    "    # WARNING: May be incorrect if output features of dm.net != n_classes\n",
    "    n_classes = len(dataloader_train.dataset.class_nums)\n",
    "\n",
    "    # Train loss/accuracy\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    # Test loss/accuracy\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    # Mean loss/accuracy from @n_warmup_epochs to current epoch\n",
    "    test_losses_mean = []\n",
    "    test_accs_mean = []\n",
    "    predictions_test_cumulative = torch.zeros([1, 1], dtype=t_type, device=device)\n",
    "    # Mean loss/accuracy from (current epoch - n_previous)  to current epoch\n",
    "    test_losses_mean_previous = []\n",
    "    test_accs_mean_previous = []\n",
    "    predictions_test_previous = torch.zeros([n_previous, y_test_all.shape[0], n_classes], dtype=t_type, device=device)\n",
    "    # Index of 'oldest' element in predictions_test_previous\n",
    "    pointer_to_the_back = 0\n",
    "\n",
    "    if log_file_name is not None:\n",
    "        log_file = open(log_file_name, 'a')\n",
    "        log_file.write('\\rNew run of training.\\r')\n",
    "        log_file.close()\n",
    "\n",
    "    epoch = start_epoch\n",
    "    try:\n",
    "        for epoch in range(start_epoch, end_epoch):\n",
    "            epoch_since_start = epoch - start_epoch\n",
    "\n",
    "            # One update of particles via all dataloader_train\n",
    "            for x, y in dataloader_train:\n",
    "                x = x.double().to(device=device).view(x.shape[0], -1)\n",
    "                y = y.to(device=device)\n",
    "                burn_in_coeff = max(1. - (1. - 1.) / 20. * epoch, 1.)\n",
    "                dm.update_latent_net(h_type=0, kernel_type='rbf', p=None,\n",
    "                                     x_batch=x, y_batch=y,\n",
    "                                     train_size=len(dataloader_train.dataset),\n",
    "                                     step_size=lr_str.step_size,\n",
    "                                     move_theta_0=move_theta_0,\n",
    "                                     burn_in=True, burn_in_coeff=burn_in_coeff,\n",
    "                                     epoch=epoch\n",
    "                                     )\n",
    "\n",
    "            # Evaluate cross entropy and accuracy over dataloader_train\n",
    "            train_loss = 0.\n",
    "            train_acc = 0.\n",
    "            for x_train, y_train in dataloader_train:\n",
    "                x_train = x_train.double().to(device=device).view(x_train.shape[0], -1)\n",
    "                y_train = y_train.to(device=device)\n",
    "\n",
    "                net_pred = dm.predict_net(x_train, inference=True)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                train_loss -= torch.sum(torch.gather(net_pred, 1, y_train.view(-1, 1)))\n",
    "                train_acc += torch.sum(y_pred == y_train).float()\n",
    "            train_loss /= (len(dataloader_train.dataset) + 0.)\n",
    "            train_acc /= (len(dataloader_train.dataset) + 0.)\n",
    "\n",
    "            # Evaluate cross entropy and accuracy over dataloader_test\n",
    "            test_loss = 0.\n",
    "            test_acc = 0.\n",
    "            predictions_test_current = torch.tensor([], dtype=t_type, device=device)\n",
    "            for x_test, y_test in dataloader_test:\n",
    "                x_test = x_test.double().to(device=device).view(x.shape[0], -1)\n",
    "                y_test = y_test.to(device=device)\n",
    "\n",
    "                # Get output of net before Softmax, mean and log, Shape = [n_particles, batch_size, output_features]\n",
    "                net_pred_pure = dm.predict_net(x_test, inference=False)\n",
    "                net_pred_pure = torch.mean(torch.nn.Softmax(dim=2)(net_pred_pure), dim=0)\n",
    "                predictions_test_current = torch.cat([predictions_test_current, net_pred_pure.data.detach().clone()],\n",
    "                                                     dim=0)\n",
    "\n",
    "                net_pred = torch.log(net_pred_pure)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                test_loss -= torch.sum(torch.gather(net_pred, 1, y_test.view(-1, 1)))\n",
    "                test_acc += torch.sum(y_pred == y_test).float()\n",
    "            test_loss /= (len(dataloader_test.dataset) + 0.)\n",
    "            test_acc /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            # Evaluate cross entropy and accuracy over dataloader_test using\n",
    "            # all predictions from previous (@epoch_since_start - @n_warmup_epochs) epochs\n",
    "            test_loss_mean = 0.\n",
    "            test_acc_mean = 0.\n",
    "            if epoch_since_start >= n_warmup_epochs:\n",
    "                predictions_test_cumulative = (\n",
    "                        predictions_test_cumulative * (epoch_since_start - n_warmup_epochs) / (\n",
    "                            epoch_since_start - n_warmup_epochs + 1.) +\n",
    "                        predictions_test_current / (epoch_since_start - n_warmup_epochs + 1.))\n",
    "                log_predictions_test = torch.log(predictions_test_cumulative)\n",
    "                y_pred_all = torch.argmax(log_predictions_test, dim=1)\n",
    "\n",
    "                test_loss_mean = -torch.sum(torch.gather(log_predictions_test, 1, y_test_all.view(-1, 1)))\n",
    "                test_acc_mean = torch.sum(y_pred_all == y_test_all).float()\n",
    "                test_loss_mean /= (len(dataloader_test.dataset) + 0.)\n",
    "                test_acc_mean /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            # Evaluate cross entropy and accuracy over dataloader_test using\n",
    "            # all predictions from previous @n_previous epochs\n",
    "            test_loss_mean_previous = 0.\n",
    "            test_acc_mean_previous = 0.\n",
    "            predictions_test_previous[pointer_to_the_back] = predictions_test_current\n",
    "            if pointer_to_the_back + 1 == n_previous:\n",
    "                pointer_to_the_back = 0\n",
    "            else:\n",
    "                pointer_to_the_back += 1\n",
    "            if epoch_since_start + 1 >= n_previous:\n",
    "                log_predictions_test = torch.log(torch.mean(predictions_test_previous, dim=0))\n",
    "                y_pred_all = torch.argmax(log_predictions_test, dim=1)\n",
    "                test_loss_mean_previous = -torch.sum(torch.gather(log_predictions_test, 1, y_test_all.view(-1, 1)))\n",
    "                test_acc_mean_previous = torch.sum(y_pred_all == y_test_all).float()\n",
    "                test_loss_mean_previous /= (len(dataloader_test.dataset) + 0.)\n",
    "                test_acc_mean_previous /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            # Append evaluated losses and accuracies\n",
    "            train_losses.append(train_loss.data[0].cpu().numpy())\n",
    "            train_accs.append(train_acc.data[0].cpu().numpy())\n",
    "            test_losses.append(test_loss.data[0].cpu().numpy())\n",
    "            test_accs.append(test_acc.data[0].cpu().numpy())\n",
    "            if epoch_since_start >= n_warmup_epochs:\n",
    "                test_losses_mean.append(test_loss_mean.data[0].cpu().numpy())\n",
    "                test_accs_mean.append(test_acc_mean.data[0].cpu().numpy())\n",
    "            else:\n",
    "                test_losses_mean.append(None)\n",
    "                test_accs_mean.append(None)\n",
    "            if epoch_since_start + 1 >= n_previous:\n",
    "                test_losses_mean_previous.append(test_loss_mean_previous.data[0].cpu().numpy())\n",
    "                test_accs_mean_previous.append(test_acc_mean_previous.data[0].cpu().numpy())\n",
    "            else:\n",
    "                test_losses_mean_previous.append(None)\n",
    "                test_accs_mean_previous.append(None)\n",
    "\n",
    "            # Print log into console and file\n",
    "            if epoch % n_epochs_log == 0:\n",
    "                sys.stdout.write(\n",
    "                    ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                     '\\nEmpirical Loss (Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {4:.3f}/{5:.3f}/{6:.3f}/{7:.3f}' +\n",
    "                     '\\nAccuracy (Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {8:.3f}/{9:.3f}/{10:.3f}/{11:.3f}\\t'\n",
    "                     ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                              train_loss, test_loss, test_loss_mean, test_loss_mean_previous,\n",
    "                              train_acc, test_acc, test_acc_mean, test_acc_mean_previous\n",
    "                              )\n",
    "                )\n",
    "                if log_file_name is not None:\n",
    "                    log_file = open(log_file_name, 'a')\n",
    "                    log_file.write(\n",
    "                        ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                         '\\nEmpirical Loss(Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {4:.3f}/{5:.3f}/{6:.3f}/{7:.3f}' +\n",
    "                         '\\nAccuracy(Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {8:.3f}/{9:.3f}/{10:.3f}/{11:.3f}\\t'\n",
    "                         ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                                  train_loss, test_loss, test_loss_mean, test_loss_mean_previous,\n",
    "                                  train_acc, test_acc, test_acc_mean, test_acc_mean_previous\n",
    "                                  )\n",
    "                    )\n",
    "                    log_file.close()\n",
    "\n",
    "            if epoch % n_epochs_save == 0 and epoch > start_epoch and checkpoint_file_name is not None:\n",
    "                torch.save(dm.state_dict(), checkpoint_file_name.format(start_epoch, epoch))\n",
    "\n",
    "            # Update step_size\n",
    "            lr_str.step()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    if plot_graphs:\n",
    "        print_plots([[train_losses, test_losses, test_losses_mean, test_losses_mean_previous],\n",
    "                     [train_accs, test_accs, test_accs_mean, test_accs_mean_previous]],\n",
    "                    [['Epochs', ''],\n",
    "                     ['Epochs', '% * 1e-2']],\n",
    "                    [['Cross Entropy Loss (Train)', 'Cross Entropy Loss (Test)', 'Cross Entropy Loss (Test (Mean))',\n",
    "                      'Cross Entropy Loss (Test (Mean (n_prev)))'],\n",
    "                     ['Accuracy (Train)', 'Accuracy (Test)', 'Accuracy (Mean)', 'Accuracy (Mean (n_prev))']\n",
    "                     ],\n",
    "                    plots_file_name.format(start_epoch, epoch)\n",
    "                    )\n",
    "    if checkpoint_file_name is not None:\n",
    "        torch.save(dm.state_dict(), checkpoint_file_name.format(start_epoch, epoch))\n",
    "\n",
    "    if verbose:\n",
    "        return (train_losses, test_losses, test_losses_mean, test_losses_mean_previous,\n",
    "                train_accs, test_accs, test_accs_mean, test_accs_mean_previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 1. particles - 1 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### MAP estimate for MNIST classification task\n",
    "net_1 = nn.Sequential(SteinLinear(28 * 28, 18, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 1, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_1 = ClassificationDistribution(1)\n",
    "dm_1 = DistributionMover(task='net_class', n_particles=1, use_latent=False, net=net_1, data_distribution=data_distr_1)\n",
    "lr_str_1 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_1 = 'model_1'\n",
    "version_1 = 19\n",
    "checkpoint_file_name_1 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_1 + '.pth'\n",
    "plots_file_name_1 = './Experiments/Plots/' + own_name_1 + '.png'\n",
    "log_file_name_1 = './Experiments/Logs/' + own_name_1 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_1.format(version_1)):\n",
    "    dm_1.load_state_dict(torch.load(checkpoint_file_name_1.format(version_1)))\n",
    "    lr_str_1.step_size = dm_1.step_size\n",
    "    lr_str_1.iter = dm_1.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_1,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_1, start_epoch=lr_str_1.iter, end_epoch=26,\n",
    "      checkpoint_file_name=checkpoint_file_name_1, plots_file_name=plots_file_name_1, log_file_name=log_file_name_1,\n",
    "      n_warmup_epochs=5\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(dataloader_m_train))\n",
    "# x = x.double().to(device=device).view(x.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_1.update_latent_net(h_type=0, kernel_type='rbf', p=None, x_batch=x, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_1.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2. particles - 5 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_2 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_2 = ClassificationDistribution(5)\n",
    "dm_2 = DistributionMover(task='net_class',\n",
    "                         n_particles=5,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_2,\n",
    "                         data_distribution=data_distr_2\n",
    "                        )\n",
    "lr_str_2 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_2 = 'model_2'\n",
    "version_2 = 0\n",
    "checkpoint_file_name_2 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_2 + '.pth'\n",
    "plots_file_name_2 = './Experiments/Plots/' + own_name_2 + '.png'\n",
    "log_file_name_2 = './Experiments/Logs/' + own_name_2 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_2.format(version_2)):\n",
    "    dm_2.load_state_dict(torch.load(checkpoint_file_name_2.format(version_2)))\n",
    "    lr_str_2.step_size = dm_2.step_size\n",
    "    lr_str_2.iter = dm_2.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_2,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_2, start_epoch=lr_str_2.iter, end_epoch=lr_str_2.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_2, plots_file_name=plots_file_name_2, log_file_name=log_file_name_2,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(dataloader_m_train))\n",
    "# x = x.double().to(device=device).view(x.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_2.update_latent_net(h_type=0, kernel_type='rbf', p=None, x_batch=x, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_2.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3. particles - 5 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_3 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_3 = ClassificationDistribution(5)\n",
    "dm_3 = DistributionMover(task='net_class',\n",
    "                       n_particles=5,\n",
    "                       use_latent=False,\n",
    "                       net=net_3,\n",
    "                       data_distribution=data_distr_3\n",
    "                      )\n",
    "lr_str_3 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_3 = 'model_3'\n",
    "version_3 = 0\n",
    "checkpoint_file_name_3 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_3 + '.pth'\n",
    "plots_file_name_3 = './Experiments/Plots/' + own_name_3 + '.png'\n",
    "log_file_name_3 = './Experiments/Logs/' + own_name_3 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_3.format(version_3)):\n",
    "    dm_3.load_state_dict(torch.load(checkpoint_file_name_3.format(version_3)))\n",
    "    lr_str_3.step_size = dm_3.step_size\n",
    "    lr_str_3.iter = dm_3.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_3,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_3, start_epoch=lr_str_3.iter, end_epoch=lr_str_3.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_3, plots_file_name=plots_file_name_3, log_file_name=log_file_name_3,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(dataloader_m_train))\n",
    "# x = x.double().to(device=device).view(x.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_3.update_latent_net(h_type=0, kernel_type='rbf', p=None, x_batch=x, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_3.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 4. particles - 20 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_4 = nn.Sequential(SteinLinear(28 * 28, 18, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 20, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_4 = ClassificationDistribution(20)\n",
    "dm_4 = DistributionMover(task='net_class',\n",
    "                         n_particles=20,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_4,\n",
    "                         data_distribution=data_distr_4\n",
    "                        )\n",
    "lr_str_4 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_4 = 'model_4'\n",
    "version_4 = 0\n",
    "checkpoint_file_name_4 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_4 + '.pth'\n",
    "plots_file_name_4 = './Experiments/Plots/' + own_name_4 + '.png'\n",
    "log_file_name_4 = './Experiments/Logs/' + own_name_4 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_4.format(version_4)):\n",
    "    dm_4.load_state_dict(torch.load(checkpoint_file_name_4.format(version_4)))\n",
    "    lr_str_4.step_size = dm_4.step_size\n",
    "    lr_str_4.iter = dm_4.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_4,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_4, start_epoch=lr_str_4.iter, end_epoch=lr_str_4.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_4, plots_file_name=plots_file_name_4, log_file_name=log_file_name_4,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "config_name = 'config_model_33'\n",
    "config = importlib.import_module('Experiments.Configs.' + config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dm = nn.Sequential(SteinLinear(28 * 28, 300, config.n_particles, use_var_prior=config.use_var_prior, alpha=config.alpha),\n",
    "                       nn.Tanh(),\n",
    "                       SteinLinear(300, 100, config.n_particles, use_var_prior=config.use_var_prior, alpha=config.alpha),\n",
    "                       nn.Tanh(),\n",
    "                       SteinLinear(100, 10, config.n_particles, use_var_prior=config.use_var_prior, alpha=config.alpha)\n",
    "                      ).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distr = ClassificationDistribution(config.n_particles)\n",
    "dm = DistributionMover(task='net_class',\n",
    "                       n_particles=config.n_particles,\n",
    "                       n_hidden_dims=config.n_hidden_dims,\n",
    "                       use_latent=config.use_latent,\n",
    "                       net=net_dm,\n",
    "                       data_distribution=data_distr,\n",
    "                       dummy=True\n",
    "                      )\n",
    "lr_str = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file_name = ('./Experiments/Checkpoints/' + 'e{0}-{1}_' + config.experiment_name + '.pth').format(0, 60)\n",
    "dm.load_state_dict(torch.load(checkpoint_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3323, device='cuda:0', dtype=torch.float64),\n",
       " tensor(1.2464e-06, device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.mean(torch.var(dm.particles, dim=1)),\n",
    "torch.mean(torch.var(dm.lt.transform(dm.particles, n_particles_second=True), dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 300),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(300, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(100, 10)\n",
    ").to(device=device).double()\n",
    "\n",
    "\n",
    "checkpoint_file_name = ('./Experiments/Checkpoints/' + 'e{0}-{1}_' + 'ml_est' + '.pth').format(0,199)\n",
    "net.load_state_dict(torch.load(checkpoint_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3323, device='cuda:0', dtype=torch.float64),\n",
       " tensor(1.2464e-06, device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.mean(torch.var(dm.particles, dim=1)),\n",
    "torch.mean(torch.var(dm.lt.transform(dm.particles, n_particles_second=True), dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modify_sample_net(net, samples, real_labels, target_labels=None, eps=1e-2, n_iters=1):\n",
    "    for idx in range(n_iters):\n",
    "        samples = samples.detach().clone().requires_grad_(True)\n",
    "        \n",
    "        predictions = net(samples)\n",
    "        predictions = torch.log(torch.nn.Softmax()(predictions))\n",
    "        \n",
    "        train_loss = -torch.sum(torch.gather(predictions, 1, real_labels.view(-1, 1)))\n",
    "    \n",
    "        train_loss.backward()\n",
    "        perturbation = torch.sign(samples.grad)    \n",
    "        samples = torch.clamp(samples + eps * perturbation, -0.42, 2.8)\n",
    "        \n",
    "    return samples.detach(), perturbation.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modify_sample(dm, samples, real_labels, target_labels=None, eps=1e-2, n_iters=1, use_full=False):\n",
    "    for idx in range(n_iters):\n",
    "        samples = samples.detach().clone().requires_grad_(True)\n",
    "        \n",
    "        predictions = dm.predict_net(samples, inference=False)\n",
    "        \n",
    "        if use_full:\n",
    "            predictions = predictions[:]\n",
    "        else:\n",
    "            predictions = predictions[0:1]\n",
    "        predictions = torch.log(torch.mean(torch.nn.Softmax(dim=2)(predictions), dim=0))\n",
    "\n",
    "        train_loss = -torch.sum(torch.gather(predictions, 1, real_labels.view(-1, 1)))\n",
    "\n",
    "        train_loss.backward()\n",
    "        perturbation = torch.sign(samples.grad)        \n",
    "        samples = torch.clamp(samples + eps * perturbation, -0.42, 2.8)\n",
    "\n",
    "    return samples.detach(), perturbation.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, real_labels = next(iter(dataloader_m_test))\n",
    "samples = samples.double().to(device=device).view(samples.shape[0], -1)\n",
    "real_labels = real_labels.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_samples, perturbation = modify_sample(dm, samples, real_labels, eps=0.25, n_iters=1)\n",
    "modified_samples_net, perturbation_net = modify_sample_net(net, samples, real_labels, eps=0.25, n_iters=1)\n",
    "\n",
    "modified_samples_aug, _ = next(iter(dataloader_m_test_aug))\n",
    "modified_samples_aug = modified_samples_aug.double().to(device=device).view(modified_samples_aug.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADmFJREFUeJzt3X+MVfWZx/HPwwjxB8QIBHa0snRHs4okS9dRVtts2GysdtPIDy1gjKHxByQWsyjRVaJCXIm62u42ZtOEtqQ0tpYm+AMqWdqQjfaPShyNol1wq2S2ZZnMLKFaEBLQefaPOZgR537P5dxz77kzz/uVkPvjueecJ1c/c86933Pu19xdAOIZV3UDAKpB+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBHVGKzdmZpxOiJYZN672vm3atGnJZQ8fPpysf/TRR4V6agV3t3pe11D4zew6Sd+V1CHpB+7+eCPrA8p05pln1qzddtttyWV37tyZrL/66quFemonhQ/7zaxD0r9L+pqkWZJuMrNZZTUGoLka+cx/paT33H2fux+X9DNJ88tpC0CzNRL+CyT9Ydjj/dlzn2Fmy82sx8x6GtgWgJI18pl/pC8VPveFnrtvkLRB4gs/oJ00suffL+nCYY+/IOlAY+0AaJVGwv+apIvN7ItmNkHSUklby2kLQLMVPux394/NbKWkHRoa6tvo7r8trTMgx9NPP1142TvvvDNZP378eLI+Fob6Ghrnd/ftkraX1AuAFuL0XiAowg8ERfiBoAg/EBThB4Ii/EBQLb2eHyhTR0dH1S2Mauz5gaAIPxAU4QeCIvxAUIQfCIrwA0Ex1Icxa8WKFYWX7e7uLrGT9sSeHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Iydy++sFmvpMOSPpH0sbsnL4I2s+IbA05x0UUXJevvvvtuzVreFNx5zjrrrIaWbyZ3t3peV8aPefydux8sYT0AWojDfiCoRsPvkn5pZq+b2fIyGgLQGo0e9n/Z3Q+Y2TRJvzKzve7+yvAXZH8U+MMAtJmG9vzufiC7HZD0vKQrR3jNBnfvzvsyEEBrFQ6/mZ1jZpNO3pf0VUnvlNUYgOZq5LB/uqTnzezken7q7v9RSlcAmq5w+N19n6S/KrEXoG208zh+WRjqA4Ii/EBQhB8IivADQRF+ICjCDwTFFN0YtT744IPCy44bl97vXXLJJYXXLUl79+5taPlWYM8PBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzo+Qzjgj/b/+DTfckKyvX7++zHYqwZ4fCIrwA0ERfiAowg8ERfiBoAg/EBThB4IaVeP8M2bMqFm7+eabk8tOmTIlWX/ppZcK9VSPXbt2JetHjx5t2rbHsg8//DBZ37x5c83akiVLym5n1GHPDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB5Y7zm9lGSV+XNODus7PnJkvaLGmmpF5Ji939j402c8sttyTrDz30UM1aV1dXQ9u+++67Cy+bN46f54EHHkjWX3755YbWP1adOHEiWe/v7y+87pkzZxZedrSoZ8//I0nXnfLc/ZJ2uvvFknZmjwGMIrnhd/dXJB065en5kjZl9zdJWlByXwCarOhn/unu3idJ2e208loC0ApNP7ffzJZLWt7s7QA4PUX3/P1m1ilJ2e1ArRe6+wZ373b37oLbAtAERcO/VdKy7P4ySS+W0w6AVskNv5k9K+k3kv7SzPab2W2SHpd0jZn9TtI12WMAo0juZ353v6lG6e9L7kVXXXVVst7IWP5bb72VrM+aNStZHz9+fM3a3LlzC/WExqT+m0hSZ2dn4XXfeuutyfodd9xReN3tgjP8gKAIPxAU4QeCIvxAUIQfCIrwA0GZu7duY2bJjeX1Mjg4WLO2bdu25LKLFy9O1levXp2sL126tGZt9uzZyWUHBmqeAFmX1E9QS9KDDz5Ys3bkyJGGtt3Opk6dmqw3cklvno6Ojqatu1HubvW8jj0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTV0im6L7/8cvX09DRl3fv27UvWjx8/nqw/9thjyXpqnD9v3dOmNfYTh3fddVeyfu6559asPfroo8ll33///UI9YfRjzw8ERfiBoAg/EBThB4Ii/EBQhB8IivADQbX0ev6uri5/4oknCi+/aNGimrW9e/cml73ssssKbzfPvffem6w/8sgjyfqECRPKbOczDh48mKw/88wzyfratWuT9Sp/L4Dr+UfG9fwAkgg/EBThB4Ii/EBQhB8IivADQRF+IKjc6/nNbKOkr0sacPfZ2XPrJN0h6f+yl61x9+156zp27Jh2795ds75u3br8jguaOHFist7IePWTTz6ZrO/ZsydZ7+7uLrxtSbrvvvtq1vLGwletWpWsz5kzJ1l/8803k/XUeQJ573le782UN1fCWFDPnv9Hkq4b4fl/dfc52b/c4ANoL7nhd/dXJB1qQS8AWqiRz/wrzWy3mW00s/NK6whASxQN//ckdUmaI6lP0rdrvdDMlptZj5n1HD16tODmAJStUPjdvd/dP3H3QUnfl3Rl4rUb3L3b3bvPPvvson0CKFmh8JtZ57CHCyW9U047AFqlnqG+ZyXNkzTVzPZLWitpnpnNkeSSeiWtaGKPAJqgpdfzn3/++X777bfXrDdznL+rqytZ7+3tbdq2G/XCCy8UXvbqq69O1qdMmVJ43fVIzSnQ7HH+Rq7nf/jhh5P19evXF153s3E9P4Akwg8ERfiBoAg/EBThB4Ii/EBQLZ2ie3BwUMeOHWvlJj+1devWZP3aa69N1vv6+sps57QsWLCg8LKdnZ3J+pYtW5L1uXPnFt62JG3atKlmbeXKlcllJ0+enKynLg9HPvb8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUS8f5+/v79dRTT9WsX3HFFcnlb7zxxsLbzpuie8eOHcn6vHnzatYOHRq7v2964sSJZH38+PHJeuochUbOX0Dj2PMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFAtHefPs2TJkmQ9NW3ywoULG9r2pZdemqxPmjSpZq2dx/nzfocg76e9r7/++mR9zZo1yXreuRvtKu/8hrGAPT8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBJU7RbeZXSjpx5L+TNKgpA3u/l0zmyxps6SZknolLXb3P+asq2nzgXd3dze0fN7v22/btq2h9Y9VeVN8r169umbtnnvuKbudz9i+fXvN2qJFi5q67SqVOUX3x5JWu/ulkv5G0rfMbJak+yXtdPeLJe3MHgMYJXLD7+597v5Gdv+wpD2SLpA0X9LJ6Vg2SeJnWYBR5LQ+85vZTElfkrRL0nR375OG/kBImlZ2cwCap+5z+81soqQtkla5+5/M6vpYITNbLml5sfYANEtde34zG6+h4P/E3Z/Lnu43s86s3ilpYKRl3X2Du3e7e2PfyAEoVW74bWgX/0NJe9z9O8NKWyUty+4vk/Ri+e0BaJZ6hvq+IunXkt7W0FCfJK3R0Of+n0uaIen3kr7h7slrW5s51AdgSL1DfbnhLxPhB5qvzHF+AGMQ4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo3PCb2YVm9p9mtsfMfmtm/5g9v87M/tfM3sz+/UPz2wVQFnP39AvMOiV1uvsbZjZJ0uuSFkhaLOmIuz9V98bM0hsD0DB3t3ped0YdK+qT1JfdP2xmeyRd0Fh7AKp2Wp/5zWympC9J2pU9tdLMdpvZRjM7r8Yyy82sx8x6GuoUQKlyD/s/faHZREkvS1rv7s+Z2XRJByW5pH/W0EeDW3PWwWE/0GT1HvbXFX4zGy/pF5J2uPt3RqjPlPQLd5+dsx7CDzRZveGv59t+k/RDSXuGBz/7IvCkhZLeOd0mAVSnnm/7vyLp15LeljSYPb1G0k2S5mjosL9X0orsy8HUutjzA01W6mF/WQg/0HylHfYDGJsIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQeX+gGfJDkr6n2GPp2bPtaN27a1d+5Loragye/vzel/Y0uv5P7dxsx53766sgYR27a1d+5LoraiqeuOwHwiK8ANBVR3+DRVvP6Vde2vXviR6K6qS3ir9zA+gOlXv+QFUpJLwm9l1Zvaumb1nZvdX0UMtZtZrZm9nMw9XOsVYNg3agJm9M+y5yWb2KzP7XXY74jRpFfXWFjM3J2aWrvS9a7cZr1t+2G9mHZL+W9I1kvZLek3STe7+Xy1tpAYz65XU7e6Vjwmb2d9KOiLpxydnQzKzf5F0yN0fz/5wnufu/9Qmva3Tac7c3KTeas0s/U1V+N6VOeN1GarY818p6T133+fuxyX9TNL8Cvpoe+7+iqRDpzw9X9Km7P4mDf3P03I1emsL7t7n7m9k9w9LOjmzdKXvXaKvSlQR/gsk/WHY4/1qrym/XdIvzex1M1tedTMjmH5yZqTsdlrF/Zwqd+bmVjplZum2ee+KzHhdtirCP9JsIu005PBld/9rSV+T9K3s8Bb1+Z6kLg1N49Yn6dtVNpPNLL1F0ip3/1OVvQw3Ql+VvG9VhH+/pAuHPf6CpAMV9DEidz+Q3Q5Iel5DH1PaSf/JSVKz24GK+/mUu/e7+yfuPijp+6rwvctmlt4i6Sfu/lz2dOXv3Uh9VfW+VRH+1yRdbGZfNLMJkpZK2lpBH59jZudkX8TIzM6R9FW13+zDWyUty+4vk/Rihb18RrvM3FxrZmlV/N6124zXlZzkkw1l/JukDkkb3X19y5sYgZn9hYb29tLQFY8/rbI3M3tW0jwNXfXVL2mtpBck/VzSDEm/l/QNd2/5F281epun05y5uUm91ZpZepcqfO/KnPG6lH44ww+IiTP8gKAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E9f+C0yy5IxlzZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_idx = 1\n",
    "plt.figure()\n",
    "plt.imshow(samples[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(modified_samples[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(modified_samples_net[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(modified_samples_aug[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stein (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(dm.predict_net(samples, inference=True), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0001, 0.0027, 0.9712, 0.0233, 0.0001, 0.0006, 0.0018, 0.0000, 0.0003,\n",
       "        0.0000], device='cuda:0', dtype=torch.float64, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(dm.predict_net(samples, inference=True)[plot_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Net (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_net = torch.argmax(net(samples), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4230e-11, 1.2675e-05, 9.9999e-01, 7.5929e-07, 5.3609e-13, 1.2665e-08,\n",
       "        6.7657e-11, 5.1346e-16, 1.6951e-10, 2.0722e-17],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax()(net(samples))[plot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stein (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_modified = torch.argmax(dm.predict_net(modified_samples, inference=True), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1201e-05, 6.1823e-04, 9.8281e-01, 1.6485e-02, 3.3294e-07, 2.0252e-05,\n",
       "        8.6087e-06, 5.7676e-06, 3.2724e-05, 2.6098e-06],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(dm.predict_net(modified_samples, inference=True)[plot_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Net (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_modified_net = torch.argmax(net(modified_samples_net), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0008e-10, 1.3194e-03, 9.9864e-01, 3.2380e-05, 2.8716e-11, 1.0054e-05,\n",
       "        5.6417e-08, 6.6006e-13, 1.5085e-06, 4.6557e-15],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax()(net(modified_samples_net))[plot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stein (Aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_modified_aug = torch.argmax(dm.predict_net(modified_samples_aug, inference=True), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6820e-05, 1.0982e-07, 1.5986e-04, 3.0386e-05, 9.8096e-01, 6.5219e-04,\n",
       "        1.6718e-02, 1.4618e-04, 1.4715e-04, 1.1728e-03],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(dm.predict_net(modified_samples_aug, inference=True)[plot_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Net (Aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_modified_net_aug = torch.argmax(net(modified_samples_aug), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9844e-09, 3.8393e-09, 2.8324e-10, 1.9330e-12, 1.3026e-01, 3.1028e-10,\n",
       "        8.6974e-01, 3.1221e-09, 7.6997e-09, 4.7386e-08],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax()(net(modified_samples_aug))[plot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stein (Original) - Stein (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(predictions - predictions_modified != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Net (Original) - Net (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(predictions_net - predictions_modified_net != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stein (Original) - Stein (Aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(predictions - predictions_modified_aug != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Net (Original) - Net (Aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(predictions_net - predictions_modified_net_aug != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def perform_adv_attack(dataloader, dm, net, modifier, modifier_net, **modifier_kwargs):\n",
    "    cnt = 0.\n",
    "    cnt_net = 0.\n",
    "    cross_entr = 0.\n",
    "    cross_entr_net = 0.\n",
    "    cross_entr_modified = 0.\n",
    "    cross_entr_modified_net = 0.\n",
    "    for samples, real_labels in dataloader:\n",
    "        samples = samples.double().to(device=device).view(samples.shape[0], -1)\n",
    "        real_labels = real_labels.to(device=device)\n",
    "\n",
    "        ### modify samples\n",
    "        modified_samples, _ = modifier(dm=dm, samples=samples, real_labels=real_labels, target_labels=None, **modifier_kwargs)\n",
    "        modified_samples_net, _ = modifier_net(net=net, samples=samples, real_labels=real_labels, target_labels=None, **modifier_kwargs)\n",
    "\n",
    "        ### evaluate crossentropy on real samples\n",
    "        pred = dm.predict_net(samples, inference=False)\n",
    "        pred_slice = pred[2:3]\n",
    "        log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "        cross_entr -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        pred_net = net(samples)\n",
    "        log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "        cross_entr_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        ### evaluate crossentropy on modified samples\n",
    "        pred = dm.predict_net(modified_samples, inference=False)\n",
    "        pred_slice = pred[2:3]\n",
    "        log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "        cross_entr_modified -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        pred_net = net(modified_samples_net)\n",
    "        log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "        cross_entr_modified_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        ### evaluate error rate\n",
    "        pred = torch.argmax(dm.predict_net(samples, inference=True), dim=1)\n",
    "        pred_net = torch.argmax(net(samples), dim=1)\n",
    "\n",
    "        pred_modified = torch.argmax(dm.predict_net(modified_samples, inference=True), dim=1)\n",
    "        pred_modified_net = torch.argmax(net(modified_samples_net), dim=1)\n",
    "\n",
    "        cnt += len(np.argwhere(pred - pred_modified != 0).view(-1))\n",
    "        cnt_net += len(np.argwhere(pred_net - pred_modified_net != 0).view(-1))\n",
    "    print(('Error Rate (Stein/Net): {0:.3f}/{1:.3f}\\t' + \n",
    "           'Cross Entropy (Original)(Stein/Net): {2:.3f}/{3:.3f}\\t' +  \n",
    "           'Cross Entropy (Modified)(Stein/Net): {4:.3f}/{5:.3f}'\n",
    "          ).format(cnt / len(dataloader_m_test.dataset), cnt_net / len(dataloader_m_test.dataset), \n",
    "                   cross_entr / len(dataloader_m_test.dataset), cross_entr_net / len(dataloader_m_test.dataset),\n",
    "                   cross_entr_modified / len(dataloader_m_test.dataset), cross_entr_modified_net / len(dataloader_m_test.dataset)\n",
    "                  )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def perform_aug_attack(dataloader, dataloader_aug, dm, net):\n",
    "    cnt = 0.\n",
    "    cnt_net = 0.\n",
    "    cross_entr = 0.\n",
    "    cross_entr_net = 0.\n",
    "    cross_entr_modified = 0.\n",
    "    cross_entr_modified_net = 0.\n",
    "    \n",
    "    iterator = iter(dataloader)\n",
    "    iterator_aug = iter(dataloader_aug)\n",
    "    for _ in range(len(dataloader)):\n",
    "        samples, real_labels = next(iterator)\n",
    "        modified_samples, _ = next(iterator_aug)\n",
    "        \n",
    "        samples = samples.double().to(device=device).view(samples.shape[0], -1)\n",
    "        modified_samples = modified_samples.double().to(device=device).view(modified_samples.shape[0], -1)\n",
    "        modified_samples_net = modified_samples.detach()\n",
    "        real_labels = real_labels.to(device=device)\n",
    "\n",
    "        ### evaluate crossentropy on real samples\n",
    "        pred = dm.predict_net(samples, inference=False)\n",
    "        pred_slice = pred[2:3]\n",
    "        log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "        cross_entr -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        pred_net = net(samples)\n",
    "        log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "        cross_entr_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        ### evaluate crossentropy on modified samples\n",
    "        pred = dm.predict_net(modified_samples, inference=False)\n",
    "        pred_slice = pred[2:3]\n",
    "        log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "        cross_entr_modified -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        pred_net = net(modified_samples_net)\n",
    "        log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "        cross_entr_modified_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        ### evaluate error rate\n",
    "        pred = torch.argmax(dm.predict_net(samples, inference=True), dim=1)\n",
    "        pred_net = torch.argmax(net(samples), dim=1)\n",
    "\n",
    "        pred_modified = torch.argmax(dm.predict_net(modified_samples, inference=True), dim=1)\n",
    "        pred_modified_net = torch.argmax(net(modified_samples_net), dim=1)\n",
    "\n",
    "        cnt += len(np.argwhere(pred - pred_modified != 0).view(-1))\n",
    "        cnt_net += len(np.argwhere(pred_net - pred_modified_net != 0).view(-1))\n",
    "    print(('Error Rate (Stein/Net): {0:.3f}/{1:.3f}\\t' + \n",
    "           'Cross Entropy (Original)(Stein/Net): {2:.3f}/{3:.3f}\\t' +  \n",
    "           'Cross Entropy (Modified)(Stein/Net): {4:.3f}/{5:.3f}'\n",
    "          ).format(cnt / len(dataloader.dataset), cnt_net / len(dataloader.dataset), \n",
    "                   cross_entr / len(dataloader.dataset), cross_entr_net / len(dataloader.dataset),\n",
    "                   cross_entr_modified / len(dataloader.dataset), cross_entr_modified_net / len(dataloader.dataset)\n",
    "                  )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate (Stein/Net): 0.163/0.210\tCross Entropy (Original)(Stein/Net): 0.135/0.124\tCross Entropy (Modified)(Stein/Net): 0.741/1.762\n"
     ]
    }
   ],
   "source": [
    "perform_adv_attack(dataloader_m_test, dm, net, modify_sample, modify_sample_net, eps=0.01, n_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate (Stein/Net): 0.860/0.844\tCross Entropy (Original)(Stein/Net): 0.135/0.124\tCross Entropy (Modified)(Stein/Net): 7.038/14.797\n"
     ]
    }
   ],
   "source": [
    "perform_aug_attack(dataloader_m_test, dataloader_m_test_aug, dm, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MNIST with FC neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_nn = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ").double()\n",
    "optim_nn = torch.optim.Adam(net_nn.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for _, (x, y) in enumerate(dataloader_m_train):\n",
    "        x = x.double().view(x.shape[0], -1)\n",
    "        \n",
    "        optim_nn.zero_grad()\n",
    "        train_loss = nn.CrossEntropyLoss()(net_nn(x), y)\n",
    "        train_loss.backward()\n",
    "        optim_nn.step()\n",
    "   \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(30):\n",
    "            x_train, y_train = next(iter(dataloader_m_train))\n",
    "            x_train = x_train.double().view(x.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(x_train)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(net_nn(x_train), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 30.\n",
    "        train_acc /= (30. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(30):\n",
    "            x_test, y_test = next(iter(dataloader_m_test))\n",
    "            x_test = x_test.double().view(x.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(x_test)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(net_nn(x_test), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 30.\n",
    "        test_acc /= (30. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            _, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_test, y_test = next(iter(dataloader_m_test))\n",
    "x_test = x_test.double().view(x.shape[0], -1)\n",
    "y_pred = net_nn(x_test)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marginal_density = lambda x : (0.3 * NormalDensity(mu=-2., std=1., n=1)(x) + 0.7 * NormalDensity(mu=2., std=1., n=1)(x))\n",
    "#marginal_density = lambda x : (NormalDensity(mu=0., std=2., n=1)(x))\n",
    "\n",
    "pdf = []\n",
    "for _ in np.linspace(-10, 10, 1000): \n",
    "    _ = torch.tensor(_, dtype=t_type, device=device)\n",
    "    pdf.append(marginal_density(_))\n",
    "    \n",
    "from pynverse import inversefunc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "cum_density = interp1d(np.linspace(-20, 20, 250), [quad(marginal_density, -20, x)[0] for x in np.linspace(-20, 20, 250)])\n",
    "inv_cum_density = inversefunc(cum_density)\n",
    "real_samples = [inv_cum_density(np.random.random()) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean = quad(lambda x : marginal_density(x) * x, -20, 20)[0]\n",
    "var = quad(lambda x : marginal_density(x) * (x - mean) ** 2, -20, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10, 10, 1000), pdf)\n",
    "plt.plot(real_samples, np.zeros_like(real_samples))\n",
    "sns.kdeplot(real_samples, kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=20, n_hidden_dims=5, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot_projections(dm, use_real=True, n_plots_max=1, pdf=pdf)\n",
    "# plot_projections(dm, use_real=False, n_plots_max=1, pdf=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_projections(dm, use_real=True, kernel='tri', pdf=pdf)\n",
    "            plot_projections(dm, use_real=False, kernel='tri', pdf=pdf)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(torch.mean(dm.lt.transform(dm.particles, n_particles_second=True)),\n",
    " torch.mean(torch.std(dm.lt.transform(dm.particles, n_particles_second=True), dim=1) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conditional Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=2, n_hidden_dims=1, use_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_condition_distribution(dm, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(200): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "            \n",
    "            #plot_projections(dm, use_real=True, pdf=pdf)\n",
    "            plot_condition_distribution(dm, 100000)\n",
    "            \n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# Make data.\n",
    "x = np.arange(-5, 5, 0.025)\n",
    "Y = np.arange(-5, 5, 0.025)\n",
    "xx, YY = np.meshgrid(x, Y)\n",
    "xY = torch.stack([torch.tensor(xx, dtype=t_type), torch.tensor(YY, dtype=t_type)], dim=2)\n",
    "Z = torch.zeros([xY.shape[0], xY.shape[1]])\n",
    "\n",
    "Z = dm.real_target_density(xY.permute(2, 0, 1).view(2, -1)).view(Z.shape)\n",
    "Z = Z.cpu().data.numpy()\n",
    "\n",
    "# Plot the surface.\n",
    "plt.contour(x, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "# Plot conditioning line\n",
    "xxx, YYY = dm.lt.transform(torch.tensor(x, dtype=t_type, device=device).view(1, -1), n_particles_second=True).data.cpu().numpy()\n",
    "plt.plot(xxx, YYY, 'r', label='Linear manifold', )\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Density contour lines\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare implementations of Stein Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class SVGD():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def svgd_kernel(self, theta, h = -1):\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = h / np.log(theta.shape[0]+1)\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = 2. * dxkxy / (h)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    " \n",
    "    def update(self, x0, lnprob, n_iter = 1000, stepsize = 1e-3, bandwidth = -1, alpha = 0.9, debug = False):\n",
    "        # Check input\n",
    "        if x0 is None or lnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        theta = np.copy(x0) \n",
    "        \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter+1) % 1000 == 0:\n",
    "                print( 'iter ' + str(iter+1) )\n",
    "            \n",
    "            lnpgrad = lnprob(theta)\n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(theta, h = bandwidth)  \n",
    "            grad_theta = (np.matmul(kxy, lnpgrad) + dxkxy) / x0.shape[0]  \n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + grad_theta ** 2\n",
    "            else:\n",
    "                historical_grad = alpha * historical_grad + (1 - alpha) * (grad_theta ** 2)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            theta = theta + stepsize * adj_grad \n",
    "            \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_ex = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lnprob(particles):\n",
    "    grad_log_term = torch.zeros([particles.shape[0], particles.shape[1]], dtype=t_type, device=device)\n",
    "        \n",
    "    for idx in range(100):\n",
    "        particle = torch.tensor(particles[idx:idx+1], dtype=t_type, device=device, requires_grad=True)\n",
    "        log_term = torch.log(dm_ex.target_density(particle.t()))\n",
    "        grad_log_term[idx] = torch.autograd.grad(log_term, particle,\n",
    "                                                     only_inputs=True, retain_graph=False, \n",
    "                                                     create_graph=False, allow_unused=False)[0]\n",
    "    return grad_log_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = dm_ex.particles.data.clone().detach().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare kernel implementation\n",
    "\n",
    "ker, grad_ker = dm_ex.calc_kernel_term_latent(10.)\n",
    "print(dm_ex.h)\n",
    "grad_ker_sum = torch.sum(grad_ker, dim=1)\n",
    "\n",
    "ker_l, grad_ker_sum_l = dm_svgd.svgd_kernel(result, h=10.)\n",
    "\n",
    "print((ker - torch.tensor(ker_l, dtype=t_type, device=device)).norm())\n",
    "print((grad_ker_sum - torch.tensor(grad_ker_sum_l, dtype=t_type, device=device).t()).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare log_term implementation\n",
    "(lnprob(result) - dm_ex.calc_log_term_latent().t()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(201): \n",
    "    try:\n",
    "        result = dm_svgd.update(result, lnprob, stepsize=1e-2, bandwidth=-1, n_iter=20)\n",
    "        result = torch.tensor(result, dtype=t_type, device=device)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(result.data.cpu().numpy(), torch.zeros_like(result).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(result.data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(2000): \n",
    "    try:\n",
    "        dm_ex.update_latent(h_type=0)\n",
    "\n",
    "        if _ % 30 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm_ex.h))\n",
    "            \n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(dm_ex.particles.t().data.cpu().numpy(), torch.zeros_like(dm_ex.particles.t()).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(dm_ex.particles.t().data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(result - dm_ex.particles.t()).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"Stein-Variational-Gradient-Descent/python/\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.matlib as nm\n",
    "from svgd import SVGD\n",
    "\n",
    "r\"\"\"\n",
    "    Example of Bayesian Logistic Regression (the same setting as Gershman et al. 2012):\n",
    "    The observed data D = {x, y} consist of N binary class labels, \n",
    "    y_t \\in {-1,+1}, and d covariates for each datapoint, x_t \\in R^d.\n",
    "    The hidden variables \\theta = {w, \\alpha} consist of d regression coefficients w_k \\in R,\n",
    "    and a precision parameter \\alpha \\in R_+. We assume the following model:\n",
    "        p(\\alpha) = Gamma(\\alpha; a, b)\n",
    "        p(w_k | a) = N(w_k; 0, \\alpha^-1)\n",
    "        p(y_t = 1| x_t, w) = 1 / (1+exp(-w^T x_t))\n",
    "\"\"\"\n",
    "class BayesianLR:\n",
    "    def __init__(self, x, Y, batchsize=100, a0=1, b0=0.01):\n",
    "        self.x, self.Y = x, Y\n",
    "        # TODO. Y in \\in{+1, -1}\n",
    "        self.batchsize = min(batchsize, x.shape[0])\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        \n",
    "        self.N = x.shape[0]\n",
    "        self.permutation = np.random.permutation(self.N)\n",
    "        self.iter = 0\n",
    "    \n",
    "        \n",
    "    def dlnprob(self, theta):\n",
    "        \n",
    "        if self.batchsize > 0:\n",
    "            batch = [ i % self.N for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize) ]\n",
    "            ridx = self.permutation[batch]\n",
    "            self.iter += 1\n",
    "        else:\n",
    "            ridx = np.random.permutation(self.x.shape[0])\n",
    "            \n",
    "        xs = self.x[ridx, :]\n",
    "        Ys = self.Y[ridx]\n",
    "        \n",
    "        w = theta[:, :-1]  # logistic weights\n",
    "        alpha = np.exp(theta[:, -1])  # the last column is logalpha\n",
    "        d = w.shape[1]\n",
    "        \n",
    "        wt = np.multiply((alpha / 2), np.sum(w ** 2, axis=1))\n",
    "        \n",
    "        coff = np.matmul(xs, w.T)\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-1 * coff))\n",
    "        \n",
    "        dw_data = np.matmul(((nm.repmat(np.vstack(Ys), 1, theta.shape[0]) + 1) / 2.0 - y_hat).T, xs)  # Y \\in {-1,1}\n",
    "        dw_prior = -np.multiply(nm.repmat(np.vstack(alpha), 1, d) , w)\n",
    "        dw = dw_data * 1.0 * self.x.shape[0] / xs.shape[0] + dw_prior  # re-scale\n",
    "        \n",
    "        dalpha = d / 2.0 - wt + (self.a0 - 1) - self.b0 * alpha + 1  # the last term is the jacobian term\n",
    "        \n",
    "        return np.hstack([dw, np.vstack(dalpha)])  # % first order derivative \n",
    "    \n",
    "    def evaluation(self, theta, x_test, y_test):\n",
    "        theta = theta[:, :-1]\n",
    "        M, n_test = theta.shape[0], len(y_test)\n",
    "\n",
    "        prob = np.zeros([n_test, M])\n",
    "        for t in range(M):\n",
    "            coff = np.multiply(y_test, np.sum(-1 * np.multiply(nm.repmat(theta[t, :], n_test, 1), x_test), axis=1))\n",
    "            prob[:, t] = np.divide(np.ones(n_test), (1 + np.exp(coff)))\n",
    "        \n",
    "        prob = np.mean(prob, axis=1)\n",
    "        acc = np.mean(prob > 0.5)\n",
    "        llh = np.mean(np.log(prob))\n",
    "        return [acc, llh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = x_test.shape[0] + x_train.shape[0]\n",
    "x_input = np.hstack([x, np.ones([N, 1])])\n",
    "y_input = y\n",
    "d = x_input.shape[1]\n",
    "D = d + 1\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_input, y_input, test_size=0.2, random_state=42)\n",
    "    \n",
    "a0, b0 = 1, 0.01 #hyper-parameters\n",
    "model = BayesianLR(x_train, y_train, 100, a0, b0) # batchsize = 100\n",
    "    \n",
    "# initialization\n",
    "M = 100  # number of particles\n",
    "theta0 = np.zeros([M, D]);\n",
    "alpha0 = np.random.gamma(a0, b0, M); \n",
    "for i in range(M):\n",
    "    theta0[i, :] = np.hstack([np.random.normal(0, np.sqrt(1 / alpha0[i]), d), np.log(alpha0[i])])\n",
    "    \n",
    "theta = SVGD().update(x0=theta0, lnprob=model.dlnprob, bandwidth=-1, n_iter=6000, stepsize=0.05, alpha=0.9, debug=True)\n",
    "    \n",
    "print('[accuracy, log-likelihood]')\n",
    "print(model.evaluation(theta, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Metropolis–Hastings algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MHAlgorithm():\n",
    "    def __init__(self, n_dims): \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.target_density = lambda x, *args, **kwargs : (0.3 * NormalDensity(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                           0.7 * NormalDensity(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "        self.real_target_density = lambda x, *args, **kwargs : (0.3 * NormalDensity(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                0.7 * NormalDensity(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "        \n",
    "#         self.target_density = lambda x : (NormalDensity(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "#         self.real_target_density = lambda x : (NormalDensity(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "    \n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_dims, 1],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., -1.)\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "    def generate_next(self):\n",
    "        previous = self.particles[:, -1].unsqueeze(1)\n",
    "#         self.additional_distribution = NormalDensity(self.n_dims, previous, 1, n_particles_second=True)\n",
    "#         candidate = self.additional_distribution.get_sample()\n",
    "        \n",
    "#         alpha = (self.target_density(candidate) * self.additional_distribution.unnormed_density(previous) / \n",
    "#                 (self.target_density(previous) * self.additional_distribution.unnormed_density(candidate)))\n",
    "        candidate = previous + torch.zeros([self.n_dims, 1], device=device, dtype=t_type).uniform_(-1., 1.)\n",
    "        alpha = self.target_density(candidate) / self.target_density(previous)\n",
    "        \n",
    "#         print(self.target_density(candidate)[0].data.numpy(), self.additional_distribution.unnormed_density(previous)[0].data.numpy(),\n",
    "#               self.target_density(previous)[0].data.numpy(), self.additional_distribution.unnormed_density(candidate)[0].data.numpy())\n",
    "        \n",
    "        if alpha > self.one:\n",
    "            self.particles = torch.cat([self.particles, candidate], dim=1)\n",
    "        else:\n",
    "            random = torch.bernoulli(alpha)\n",
    "            if random:\n",
    "                self.particles = torch.cat([self.particles, candidate], dim=1)                \n",
    "            else:\n",
    "                self.particles = torch.cat([self.particles, previous], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mh = MHAlgorithm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    mh.generate_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(mh.particles.t().data.cpu().numpy()[:,1], \n",
    "            kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(mh.particles.t().data.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import time\n",
    "\n",
    "mu,sig,N = 0,1,1000000\n",
    "pts = []\n",
    "\n",
    "def q(x):\n",
    "#     return (1/(math.sqrt(2*math.pi*sig**2)))*(math.e**(-((x-mu)**2)/(2*sig**2)))\n",
    "#     return NormalDensity(1, 0., 2., n_particles_second=True).unnormed_density(x)\n",
    "    return (0.3 * NormalDensity(1, -2., 1., n_particles_second=True).unnormed_density(x) +\n",
    "            0.7 * NormalDensity(1, 2., 1., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "def metropolis(N):\n",
    "    r = np.zeros(1)\n",
    "    p = q(r[0])\n",
    "    pts = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        rn = r + np.random.uniform(-1,1)\n",
    "        pn = q(rn[0])\n",
    "        if pn >= p:\n",
    "            p = pn\n",
    "            r = rn\n",
    "        else:\n",
    "            u = np.random.rand()\n",
    "            if u < pn/p:\n",
    "                p = pn\n",
    "                r = rn\n",
    "        pts.append(r)\n",
    "    \n",
    "    pts = np.array(pts)\n",
    "    return pts\n",
    "    \n",
    "def hist_plot(array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1,)\n",
    "    ax.hist(array, bins=1000)    \n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(metropolis(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gg =  lambda : (NormalDensity(1, -60., 20., n_particles_second=True).get_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot([gg()[0,0] for _ in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

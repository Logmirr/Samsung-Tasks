{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orth\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "device = None\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    use_cuda = True\n",
    "\n",
    "# Ignore cuda\n",
    "# use_cuda = False\n",
    "# device = None\n",
    "\n",
    "# Set DoubleTensor as a base type for computations\n",
    "t_type = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import functools\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Support classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_plots(data, axis, labels, file_name=None):\n",
    "    n_plots = len(data)\n",
    "    plt.figure(figsize=(30, (n_plots // 3 + 1) * 10))\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        plt.subplot(n_plots // 3 + 1, 3, idx + 1)\n",
    "        for jdx in range(len(data[idx])):\n",
    "            plt.plot(data[idx][jdx], label=labels[idx][jdx])\n",
    "        plt.xlabel(axis[idx][0], fontsize=16)\n",
    "        plt.ylabel(axis[idx][1], fontsize=16)\n",
    "        plt.legend(loc=0, fontsize=16)\n",
    "    if file_name is not None:\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_projections(dm=None, use_real=True, kernel='tri', pdf=None, n_plots_max=10):\n",
    "    \"\"\"\n",
    "        Plot marginal kernel density estimation\n",
    "    Args:\n",
    "        dm (DistributionMover): class containing particles which define distribution\n",
    "        use_real (bool): If set to True then apply transformation dm.lt.transform before creating plot\n",
    "        kernel (str): Kernel type for kernel density estimation\n",
    "        pdf (array_like, None): Samples from target distribution\n",
    "        n_plots_max (int): Maximum number of plots\n",
    "    \"\"\"\n",
    "    if use_real:\n",
    "        if not dm.use_latent:\n",
    "            return\n",
    "        n_plots = dm.lt.A.shape[0]\n",
    "    else:\n",
    "        n_plots = dm.particles.shape[0]\n",
    "    if n_plots > 6:\n",
    "        scale_factor = 15\n",
    "    else:\n",
    "        scale_factor = 5\n",
    "\n",
    "    n_plots = min(n_plots, n_plots_max)\n",
    "\n",
    "    plt.figure(figsize=(3 * scale_factor, (n_plots // 3 + 1) * scale_factor))\n",
    "\n",
    "    for idx in range(n_plots):\n",
    "        slice_dim = idx\n",
    "\n",
    "        plt.subplot(n_plots // 3 + 1, 3, idx + 1)\n",
    "\n",
    "        if use_real:\n",
    "            particles = dm.lt.transform(dm.particles, n_particles_second=True).t()[:, slice_dim]\n",
    "        else:\n",
    "            particles = dm.particles.t()[:, slice_dim]\n",
    "\n",
    "        if pdf is not None:\n",
    "            plt.plot(np.linspace(-10, 10, len(pdf), dtype=np.float64), pdf)\n",
    "        plt.plot(particles.data.cpu().numpy(), torch.zeros_like(particles).data.cpu().numpy(), 'ro')\n",
    "        sns.kdeplot(particles.data.cpu().numpy(),\n",
    "                    kernel=kernel, color='darkblue', linewidth=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_condition_distribution(dm, n_samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dm (DistributionMover): object contains unconditioned density, linear manifold and particles\n",
    "        n_samples (int): number of samples\n",
    "    Return:\n",
    "        (points, weight)\n",
    "    \"\"\"\n",
    "    if not dm.use_latent:\n",
    "        return\n",
    "\n",
    "    points = torch.zeros([dm.n_hidden_dims, n_samples], dtype=t_type, device=device).uniform_(-10, 10)\n",
    "    weight = dm.real_target_density(dm.lt.transform(points, n_particles_second=True))\n",
    "    points = points.view(-1)\n",
    "\n",
    "    plt.hist(points.data.cpu().numpy(), weights=weight.data.cpu().numpy(), density=True, bins=100, alpha=0.5,\n",
    "             label='True conditional density')\n",
    "    plt.plot(dm.particles.data.cpu().numpy(), torch.zeros_like(dm.particles).data.cpu().numpy(), 'ro')\n",
    "    sns.kdeplot(dm.particles[0, :].data.cpu().numpy(),\n",
    "                kernel='tri', color='darkblue', linewidth=4, label='Approximated conditional density')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pairwise_diffs(x, y, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (2D array_like)\n",
    "        y (2D array_like)\n",
    "        n_particles_second (bool)\n",
    "        if n_particles_second == True:\n",
    "            x is a dxN matrix\n",
    "            y is an optional dxM matrix\n",
    "            Output - diffs is a dxNxM matrix where diffs[i,j] is the subtraction between x[:,i] and y[:,j]\n",
    "                    i.e. diffs[i,j] = x[:,i] - y[:,j]\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            x is a Nxd matrix\n",
    "            y is an optional Mxd matrix\n",
    "            Output - diffs is a NxMxd matrix where diffs[i,j] is the subtraction between x[i,:] and y[j,:]\n",
    "                    i.e. diffs[i,j] = x[i,:]-y[j,:]\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return x[:, :, np.newaxis] - y[:, np.newaxis, :]\n",
    "    return x[:, np.newaxis, :] - y[np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pairwise_dists(diffs=None, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        diffs (2D or 3D array_like)\n",
    "        n_particles_second (bool)\n",
    "        if n_particles_second == True:\n",
    "            diffs is a dxNxM matrix where diffs[i,j] = x[:,i] - y[:,j]\n",
    "            Output - dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "                    i.e. dist[i,j] = ||x[:,i] - y[:,j]||\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            diffs is a NxMxd matrix where diffs[i,j] = x[i,:] - y[j, :]\n",
    "            Output - dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "                i.e. dist[i,j] = ||x[i,:]-y[j,:]||\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return torch.norm(diffs, dim=0)\n",
    "    return torch.norm(diffs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     72,
     87,
     100,
     128
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NormalDensity:\n",
    "    \"\"\"\n",
    "        Multinomial normal density for independent random variables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=1, mu=0., std=1., n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            mu (float, array_like): mean of distribution\n",
    "                Default: 0.\n",
    "                if mu is float - use same mean across all dimensions\n",
    "                if mu is 1D array_like - use different mean for each dimension but same for each particles dimension\n",
    "                if mu is 2D array_like - use different mean for each dimension\n",
    "            std (float, array_like): std of distribution\n",
    "                Default: 1.\n",
    "                if std is float - use same std across all dimensions\n",
    "                if std is 1D array_like - use different std for each dimension but same for each particles dimension\n",
    "                if std is 2D array_like - use different std for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis\n",
    "                Input will be reduced along all axis exclude particle axis\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "\n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "\n",
    "        if isinstance(self.mu, float):\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device).expand(n)\n",
    "        if len(self.mu.shape) == 1:\n",
    "            if self.mu.shape[0] == 1:\n",
    "                self.mu = self.mu.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.mu.shape) == 2:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "        if isinstance(self.std, float):\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device).expand(n)\n",
    "        if len(self.std.shape) == 1:\n",
    "            if len(self.std) == 1:\n",
    "                self.std = self.std.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "        self.zero = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.pi = torch.tensor(math.pi, dtype=t_type, device=device)\n",
    "\n",
    "        # specify axis to reduce\n",
    "        # if n_particles_second == True - n_axis == 0\n",
    "        # if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.pow(self.two * self.pi, -self.n / self.two) /\n",
    "                torch.prod(self.std, dim=n_axis) *\n",
    "                torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)))\n",
    "\n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "\n",
    "    def log_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (-self.n / self.two * torch.log(self.two * self.pi) +\n",
    "                torch.sum(torch.log(self.std), dim=n_axis) -\n",
    "                self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "\n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return -self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)\n",
    "\n",
    "    def get_sample(self):\n",
    "        \"\"\"\n",
    "            Sample from normal distribution\n",
    "        \"\"\"\n",
    "        sample = torch.normal(self.mu, self.std)\n",
    "        if self.n_particles_second:\n",
    "            return sample.view(-1, 1)\n",
    "        else:\n",
    "            return sample.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     74,
     89,
     103,
     118,
     132,
     147
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class GammaDensity:\n",
    "    \"\"\"\n",
    "        Multinomial gamma density for independent random variables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=1, alpha=1, betta=1, n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            alpha (float, array_like): shape of distribution\n",
    "                Default: 1.\n",
    "                if alpha is float - use same shape across all dimensions\n",
    "                if alpha is 1D array_like - use different shape for each dimension but same for each particles dimension\n",
    "                if alpha is 2D array_like - use different shape for each dimension\n",
    "            betta (float, array_like): rate of distribution\n",
    "                Default: 1.\n",
    "                if betta is float - use same rate across all dimensions\n",
    "                if betta is 1D array_like - use different rate for each dimension but same for each particles dimension\n",
    "                if betta is 2D array_like - use different rate for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.betta = betta\n",
    "\n",
    "        if isinstance(self.alpha, float):\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).expand(n)\n",
    "        if len(self.alpha.shape) == 1:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.alpha.shape) == 2:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "        if isinstance(self.betta, float):\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device).expand(n)\n",
    "        if len(self.betta.shape) == 1:\n",
    "            if len(self.betta) == 1:\n",
    "                self.betta = self.betta.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "\n",
    "        # specify axis to reduce\n",
    "        # if n_particles_second == True - n_axis == 0\n",
    "        # if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "\n",
    "        # log Г(alpha)\n",
    "        self.lgamma = torch.lgamma(self.alpha)\n",
    "        # Г(alpha)\n",
    "        self.gamma = torch.exp(self.lgamma)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(self.betta, self.alpha) / self.gamma * torch.pow(x, self.alpha - self.one),\n",
    "                           dim=n_axis) *\n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "\n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(x, self.alpha - self.one), dim=n_axis) *\n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "\n",
    "    def log_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * torch.log(x),\n",
    "                          dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "\n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "\n",
    "    def log_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in point log(x)\n",
    "        Args:\n",
    "            log_x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * log_x,\n",
    "                          dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))\n",
    "\n",
    "    def log_unnormed_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in point log(x)\n",
    "        Args:\n",
    "            log_x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default:\n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     12,
     78,
     100,
     112
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SteinLinear(nn.Module):\n",
    "    \"\"\"\n",
    "        Custom full connected layer for Stein Gradient Neural Networks\n",
    "        Transformation: y = xA + b\n",
    "        Parameters prior:\n",
    "            1 - p(w, a) = p(w|a)p(a) = П p(w_i|a_i)p(a_i)\n",
    "                p(w_i|a_i) = N(w_i|0, a_i^(-1)); p(a_i) = G(1e-4, 1e-4)\n",
    "\n",
    "            2 - p(w) = П p(w_i)\n",
    "                p(w_i) = N(w_i|0, alpha^(-1))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, n_particles=1, use_bias=True, use_var_prior=True, alpha=1e-2):\n",
    "        super(SteinLinear, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features (int): size of each input sample\n",
    "            out_features (int): size of each output sample\n",
    "            n_particles (int): number of particles\n",
    "            use_bias (bool): If set to False, the layer will not learn an additive bias.\n",
    "                Default: True\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of weight variance\n",
    "                Default: True\n",
    "            alpha (float): If use_var_prior == False - defines weight variance\n",
    "                Default: 1e-2\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_particles = n_particles\n",
    "        self.use_bias = use_bias\n",
    "        self.use_var_prior = use_var_prior\n",
    "\n",
    "        # if alpha is None use GLOROT prior\n",
    "        if alpha is None:\n",
    "            self.alpha_weight = (self.in_features + self.out_features) / 2.\n",
    "            self.alpha_bias = self.out_features / 2.\n",
    "        else:\n",
    "            self.alpha_weight = alpha\n",
    "            self.alpha_bias = alpha\n",
    "\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.zeros([in_features, out_features, n_particles], dtype=t_type, device=device))\n",
    "        if self.use_var_prior:\n",
    "            self.log_weight_alpha = torch.nn.Parameter(\n",
    "                torch.zeros([in_features * out_features, n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_weight_alpha = torch.tensor([math.log(self.alpha_weight)], dtype=t_type, device=device,\n",
    "                                                 requires_grad=False)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros([1, out_features, n_particles], dtype=t_type, device=device))\n",
    "            if self.use_var_prior:\n",
    "                self.log_bias_alpha = torch.nn.Parameter(\n",
    "                    torch.zeros([out_features, n_particles], dtype=t_type, device=device))\n",
    "            else:\n",
    "                self.log_bias_alpha = torch.tensor([math.log(self.alpha_bias)], dtype=t_type, device=device,\n",
    "                                                   requires_grad=False)\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            # define prior on alpha p(a) = G(1e-4, 1e-4)\n",
    "            self.weight_alpha_log_prior = lambda x: (GammaDensity(n=self.log_weight_alpha.shape[0],\n",
    "                                                                  alpha=1e-4,\n",
    "                                                                  betta=1e-4,\n",
    "                                                                  n_particles_second=True\n",
    "                                                                  ).log_unnormed_density_log_x(x))\n",
    "            if self.use_bias:\n",
    "                self.bias_alpha_log_prior = lambda x: (GammaDensity(n=self.log_bias_alpha.shape[0],\n",
    "                                                                    alpha=1e-4,\n",
    "                                                                    betta=1e-4,\n",
    "                                                                    n_particles_second=True\n",
    "                                                                    ).log_unnormed_density_log_x(x))\n",
    "\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # useless function - all initialization defined in DistributionMover class\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.log_weight_alpha.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.log_bias_alpha.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Apply transformation: x_out[i, :, :] = x_in[i, :, :] * W + b[i]\n",
    "        Args:\n",
    "            x (torch.tensor): tensor\n",
    "        Shape:\n",
    "            Input: [n_particles, batch_size, in_features]\n",
    "            Output: [n_particles, batch_size, out_features]\n",
    "        \"\"\"\n",
    "        # NEED SOME OPTIMIZATION TO OMMIT .permute\n",
    "        if self.use_bias:\n",
    "            return torch.bmm(x, self.weight.permute(2, 0, 1)) + self.bias.permute(2, 0, 1)\n",
    "        return torch.bmm(x, self.weight.permute(2, 0, 1))\n",
    "\n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())\n",
    "\n",
    "    def calc_log_prior(self):\n",
    "        \"\"\"\n",
    "            Evaluate log prior of trainable parameters\n",
    "        log p(w,a) = log p(w|a) + log p(a)\n",
    "        \"\"\"\n",
    "        # define prior on weight p(w|a) = N(0, 1 / alpha)\n",
    "        weight_log_prior = lambda x: (NormalDensity(n=self.weight.numel() // self.n_particles,\n",
    "                                                    mu=0.,\n",
    "                                                    std=self.one / torch.exp(self.log_weight_alpha),\n",
    "                                                    n_particles_second=True\n",
    "                                                    ).log_unnormed_density(x))\n",
    "\n",
    "        bias_log_prior = lambda x: (NormalDensity(self.bias.numel() // self.n_particles,\n",
    "                                                  mu=0.,\n",
    "                                                  std=self.one / torch.exp(self.log_bias_alpha),\n",
    "                                                  n_particles_second=True\n",
    "                                                  ).log_unnormed_density(x))\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.use_var_prior:\n",
    "                return (weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(\n",
    "                    self.log_weight_alpha) +\n",
    "                        bias_log_prior(self.bias.view(-1, self.n_particles)) + self.bias_alpha_log_prior(\n",
    "                            self.log_bias_alpha))\n",
    "            return (weight_log_prior(self.weight.view(-1, self.n_particles)) +\n",
    "                    bias_log_prior(self.bias.view(-1, self.n_particles)))\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            return weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(\n",
    "                self.log_weight_alpha)\n",
    "        return weight_log_prior(self.weight.view(-1, self.n_particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     50,
     61,
     72,
     87
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LinearTransform:\n",
    "    \"\"\"\n",
    "        Class for various linear transformations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_dims, n_hidden_dims, use_identity=False, normalize=False, A=None, theta_0=None, dummy=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_dims (int): dimension of the space\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_identity (bool): If set to True, use 'eye' matrix for transformations\n",
    "            normalize (bool): If set to True, columns of the transformation matrix is an orthonormal basis\n",
    "            A (2D array_like, None): Initial value for transformation matrix\n",
    "                If None then matrix will be sampled from uniform distribution and then orthonormate\n",
    "                Default: None\n",
    "            theta_0 (1D array_like, None): Initial value for bias\n",
    "                If None then matrix will be sampled from uniform distribution\n",
    "                Default: None\n",
    "            dummy (bool): \n",
    "                If True then matrixes initialization ommited and class should be loaded via load_state_dict\n",
    "        \"\"\"\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_identity = use_identity\n",
    "        self.normalize = normalize\n",
    "\n",
    "        if self.use_identity or dummy:\n",
    "            return\n",
    "\n",
    "        self.A = A\n",
    "        self.theta_0 = theta_0\n",
    "\n",
    "        if self.A is None:\n",
    "            self.A = torch.zeros([self.n_dims, self.n_hidden_dims], dtype=t_type, device=device)\n",
    "            self.A.uniform_(-1., 1.)\n",
    "            if self.normalize:\n",
    "                # normalize columns of matrix A\n",
    "                self.A = torch.tensor(orth(self.A.data.cpu().numpy()), dtype=t_type, device=device)\n",
    "\n",
    "        if self.theta_0 is None:\n",
    "            self.theta_0 = torch.zeros([self.n_dims, 1], dtype=t_type, device=device)\n",
    "            self.theta_0.uniform_(-1., 1.)\n",
    "\n",
    "        # A^(t)A\n",
    "        self.AtA = torch.matmul(self.A.t(), self.A)\n",
    "        # (A^(t)A)^(-1)\n",
    "        self.AtA_1 = torch.inverse(self.AtA)\n",
    "        # (A^(t)A)^(-1)A^(t)\n",
    "        self.inverse_base = torch.matmul(self.AtA_1, self.A.t())\n",
    "\n",
    "    def transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Transform thetas as follows:\n",
    "                theta = Atheta` + theta_0\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.A, theta) + self.theta_0\n",
    "        return (torch.matmul(self.A, theta.t()) + self.theta_0).t()\n",
    "\n",
    "    def inverse_transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Apply inverse transformation:\n",
    "                theta` = (A^(t)A)^(-1)A^(t)(theta - theta_0)\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.inverse_base, theta - self.theta_0)\n",
    "        return torch.matmul(self.inverse_base, theta.t() - self.theta_0).t()\n",
    "\n",
    "    def project_inverse(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Project and then apply inverse transform to theta - theta_0:\n",
    "                theta_s_p_i = T^(-1)P(theta - theta_0)= (A^(t)A)^(-1)A^(t)theta\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        # This optimization severely reduces performance!!!!\n",
    "        # use solver trick: theta_s_p_i : A^(t)Atheta_s_p_i = A^(t)theta\n",
    "        if n_particles_second:\n",
    "            # return torch.gesv(torch.matmul(self.A.t(), theta), self.AtA)[0]\n",
    "            return torch.matmul(self.inverse_base, theta)\n",
    "        # return torch.gesv(torch.matmul(self.A.t(), theta.t()), self.AtA)[0].t()\n",
    "        return torch.matmul(theta, self.inverse_base.t())\n",
    "\n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        Example::\n",
    "            >>> module.state_dict().keys()\n",
    "            ['bias', 'weight']\n",
    "        \"\"\"\n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=1)\n",
    "\n",
    "        destination[prefix + 'A'] = self.A if keep_vars else self.A.data\n",
    "        destination[prefix + 'theta_0'] = self.theta_0 if keep_vars else self.theta_0.data\n",
    "        destination[prefix + 'n_dims'] = self.n_dims\n",
    "        destination[prefix + 'n_hidden_dims'] = self.n_hidden_dims\n",
    "        destination[prefix + 'use_identity'] = self.use_identity\n",
    "        destination[prefix + 'normalize'] = self.normalize\n",
    "\n",
    "        return destination\n",
    "\n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        self.__init__(state_dict[prefix + 'n_dims'],\n",
    "                      state_dict[prefix + 'n_hidden_dims'],\n",
    "                      state_dict[prefix + 'use_identity'],\n",
    "                      state_dict[prefix + 'normalize'],\n",
    "                      state_dict[prefix + 'A'],\n",
    "                      state_dict[prefix + 'theta_0'],\n",
    "                      dummy=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     37,
     75,
     78
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RegressionDistribution(nn.Module):\n",
    "    \"\"\"\n",
    "        Distribution over data for regression task: p(D|w) = N(y_predicted|y, )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_particles, use_var_prior=True, betta=1e-1):\n",
    "        super(RegressionDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of prediction variance\n",
    "                Default: True\n",
    "            betta (float): If use_var_prior == False - defines variance of prediction\n",
    "                Default: 1e-1\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_particles = n_particles\n",
    "        self.use_var_prior = use_var_prior\n",
    "        self.betta = betta\n",
    "\n",
    "        # define betta - variance of data distribution for regression tasks (betta = 1 / std ** 2)\n",
    "        if self.use_var_prior:\n",
    "            self.log_betta = torch.nn.Parameter(torch.zeros([1, self.n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_betta = torch.tensor([math.log(self.betta)], dtype=t_type, device=device, requires_grad=False)\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            # define prior on betta p(betta)\n",
    "            self.betta_log_prior = lambda x: (GammaDensity(n=1,\n",
    "                                                           alpha=1e-4,\n",
    "                                                           betta=1e-4,\n",
    "                                                           n_particles_second=True\n",
    "                                                           ).log_unnormed_density_log_x(x))\n",
    "\n",
    "        # Support tensors for computations\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "\n",
    "    def calc_log_data(self, x, y, y_predict, train_size):\n",
    "        \"\"\"\n",
    "            Evaluate log p(theta)\n",
    "        Args:\n",
    "            x (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predicted values\n",
    "            train_size (int) - size of training dataset\n",
    "        Shapes:\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, 1]\n",
    "        \"\"\"\n",
    "        # squeeze last axis because regression task is being solved\n",
    "        y_predict.squeeze_(2)\n",
    "\n",
    "        batch_size = torch.tensor(y.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "\n",
    "        # define distribution over data p(D|w)\n",
    "        # n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        if self.use_var_prior:\n",
    "            log_data_distr = lambda x: (NormalDensity(n=x.shape[1],\n",
    "                                                      mu=y,\n",
    "                                                      std=self.one / torch.sqrt(torch.exp(\n",
    "                                                          self.log_betta.expand(x.shape[1], self.n_particles).t())),\n",
    "                                                      n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "        else:\n",
    "            log_data_distr = lambda x: (NormalDensity(n=x.shape[1],\n",
    "                                                      mu=y,\n",
    "                                                      std=self.one / torch.sqrt(torch.exp(self.log_betta)),\n",
    "                                                      n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            return train_size / batch_size * log_data_distr(y_predict) + self.betta_log_prior(self.log_betta)\n",
    "        return train_size / batch_size * log_data_distr(y_predict)\n",
    "\n",
    "    def modules(self):\n",
    "        yield self\n",
    "\n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1,
     10,
     36,
     39
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ClassificationDistribution(nn.Module):\n",
    "    def __init__(self, n_particles):\n",
    "        super(ClassificationDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        \"\"\"\n",
    "        self.n_particles = n_particles\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_log_data(x, y, y_predict, train_size):\n",
    "        \"\"\"\n",
    "            Evaluate log p(theta)\n",
    "        Args:\n",
    "            x (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shapes:\n",
    "            x.shape = [batch_size, in_features]\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        batch_size = torch.tensor(x.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "\n",
    "        # define distribution over data p(D|w)\n",
    "        # n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "\n",
    "        probas = nn.LogSoftmax(dim=2)(y_predict)\n",
    "        probas_selected = torch.gather(input=probas, dim=2,\n",
    "                                       index=y.view(1, -1, 1).expand(probas.shape[0], probas.shape[1], 1)).squeeze(2)\n",
    "        log_data = torch.sum(probas_selected, dim=1)\n",
    "\n",
    "        return train_size / batch_size * log_data\n",
    "\n",
    "    def modules(self):\n",
    "        yield self\n",
    "\n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1,
     77,
     82,
     130,
     143,
     240,
     271,
     281,
     301,
     340,
     346,
     365,
     421,
     481,
     492,
     499,
     505,
     532,
     541
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DistributionMover(nn.Module):\n",
    "    def __init__(self,\n",
    "                 task='app',\n",
    "                 n_particles=None,\n",
    "                 particles=None,\n",
    "                 target_density=None,\n",
    "                 n_dims=None,\n",
    "                 n_hidden_dims=None,\n",
    "                 use_latent=False,\n",
    "                 net=None,\n",
    "                 data_distribution=None,\n",
    "                 dummy=False\n",
    "                 ):\n",
    "        super(DistributionMover, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task (str):\n",
    "                'app' | 'net_reg' | 'net_class'\n",
    "                - approximate target distribution\n",
    "                - solve regression task using net\n",
    "                - solve classification task using net\n",
    "            n_particles (int): number of particles\n",
    "            particles (2D array_like): array which contains initialized particles\n",
    "            target_density (callable): computes probability density function of target distribution (for 'app' task)\n",
    "            n_dims (int): dimension of the space where optimization is performed\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_latent (bool): If set to True, Subspace Stein is used\n",
    "            acr (list): List contains architecture of object which is used to \n",
    "                        make predictions (for 'net_reg' and' net_class' tasks)\n",
    "            data_distribution (callable): computes probability over data p(D|w) (for 'net_reg' and' net_class' tasks)\n",
    "            dummy (bool): \n",
    "                If True then no initialization performed and class should be loaded via load_state_dict\n",
    "        \"\"\"\n",
    "\n",
    "        self.task = task\n",
    "\n",
    "        self.n_particles = n_particles\n",
    "        self.particles = particles\n",
    "        self.target_density = target_density\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_latent = use_latent\n",
    "        self.net = net\n",
    "        self.data_distribution = data_distribution\n",
    "\n",
    "        if self.task == 'net_reg' or self.task == 'net_class':\n",
    "            self.n_dims = self.numel() // self.n_particles\n",
    "        if not self.use_latent:\n",
    "            self.n_hidden_dims = self.n_dims\n",
    "\n",
    "        # Learnable samples from the target distribution\n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_hidden_dims, self.n_particles],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device\n",
    "        ).uniform_(-2., 2.)\n",
    "\n",
    "        # Class for performing linear transformations\n",
    "        if self.use_latent:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=False,\n",
    "                normalize=True,\n",
    "                dummy=dummy\n",
    "            )\n",
    "        else:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=True,\n",
    "                normalize=True,\n",
    "                dummy=dummy\n",
    "            )\n",
    "\n",
    "        # Functions of probability density of target distribution\n",
    "        if self.net is None:\n",
    "            # use unnormed probability density to speedup computations\n",
    "            if target_density is not None:\n",
    "                self.target_density = target_density\n",
    "                self.real_target_density = target_density\n",
    "            else:\n",
    "                self.target_density = lambda x, *args, **kwargs: (\n",
    "                            0.3 * NormalDensity(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x,\n",
    "                                                                                                                *args,\n",
    "                                                                                                                **kwargs\n",
    "                                                                                                                ) +\n",
    "                            0.7 * NormalDensity(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x,\n",
    "                                                                                                               *args,\n",
    "                                                                                                               **kwargs\n",
    "                                                                                                               )\n",
    "                )\n",
    "\n",
    "                self.real_target_density = lambda x, *args, **kwargs: (\n",
    "                            0.3 * NormalDensity(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                            0.7 * NormalDensity(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs)\n",
    "                )\n",
    "\n",
    "        # Number of iterations since beginning\n",
    "        self.iter = 0\n",
    "        # Number of epochs since beginning\n",
    "        self.epoch = 0\n",
    "        # Burn in coefficient for grag kernel term\n",
    "        self.burn_in_coeff = 0\n",
    "\n",
    "        # Adagrad parameters\n",
    "        self.fudge_factor = torch.tensor(1e-6, dtype=t_type, device=device)\n",
    "        self.step_size = torch.tensor(1e-2, dtype=t_type, device=device)\n",
    "        self.auto_corr = torch.tensor(0.9, dtype=t_type, device=device)\n",
    "\n",
    "        # Gradient history term for adagrad optimization\n",
    "        if self.use_latent:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_hidden_dims, n_particles], dtype=t_type, device=device)\n",
    "            self.historical_grad_theta_0 = torch.zeros(\n",
    "                [self.n_dims, 1], dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_dims, n_particles], dtype=t_type, device=device)\n",
    "\n",
    "        # Factor from kernel\n",
    "        self.h = torch.tensor(0., dtype=t_type, device=device)\n",
    "\n",
    "        # Support tensors for computations\n",
    "        self.N = torch.tensor(self.n_particles, dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.three = torch.tensor(3., dtype=t_type, device=device)\n",
    "\n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        cnt = 0\n",
    "        for module in self.children():\n",
    "            if 'numel' in dir(module):\n",
    "                cnt += module.numel(trainable)\n",
    "        return cnt\n",
    "\n",
    "    def calc_kernel_term_latent(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float):\n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel\n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output:\n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        # power for rational kernel\n",
    "        p = torch.tensor(p, dtype=t_type, device=device) if p is not None else None\n",
    "\n",
    "        # theta = Atheta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        # diffs[i, j] = A(theta`_i - theta`_j)\n",
    "        diffs = pairwise_diffs(real_particles, real_particles, n_particles_second=True)\n",
    "        # dists[i, j] = ||A(theta`_i - theta`_j)||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        # sq_dists[i, j] = ||A(theta`_i - theta`_j)||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "\n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * self.n_dims\n",
    "        elif h_type == 2:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1) * self.n_dims\n",
    "        elif h_type == 5:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / (torch.pow(self.N, self.two) - self.one)\n",
    "        elif h_type == 6:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / (torch.pow(self.N, -self.one / p) - self.one)\n",
    "        elif h_type == 7:\n",
    "            med = torch.median(torch.matmul(real_particles.t(), real_particles)) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "\n",
    "        kernel = None\n",
    "        grad_kernel = None\n",
    "        if kernel_type == 'rbf':\n",
    "            # RBF Kernel:\n",
    "            # kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "            kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "            # grad_kernel[i, j] = -2/h * A(theta`_i - theta`_j) * kernel[i, j]\n",
    "            grad_kernel = -self.two / self.h * kernel.unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'imq':\n",
    "            # IMQ Kernel:\n",
    "            # kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, -self.one / self.two)\n",
    "            # grad_kernel[i, j] = -1/h * A(theta`_i - theta`_j) * kernel^(3)[i, j]\n",
    "            grad_kernel = -self.one / self.h * torch.pow(kernel, self.three).unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'exp':\n",
    "            # Exponential Kernel:\n",
    "            # kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "            kernel = torch.exp(self.one / self.h * torch.matmul(real_particles.t(), real_particles))\n",
    "            # grad_kernel[i, j] = 1/h * (Atheta`_j + theta_0) * kernel[i, j]\n",
    "            grad_kernel = 1. / self.h * kernel.unsqueeze(0) * real_particles.unsqueeze(1)\n",
    "        elif kernel_type == 'rat':\n",
    "            # RAT Kernel:\n",
    "            # kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, p)\n",
    "            # grad_kernel[i, j] = p/h * A(theta`_i - theta`_j) * kernel^((p - 1)/p)[i, j]\n",
    "            grad_kernel = p / self.h * torch.pow(kernel, (self.p - self.one) / p).unsqueeze(0) * diffs\n",
    "\n",
    "        return kernel, grad_kernel\n",
    "\n",
    "    def calc_kernel_term_latent_net(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float):\n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel\n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output:\n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        return self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "\n",
    "    def calc_log_prior_net(self):\n",
    "        \"\"\"\n",
    "            Traverse all modules and evaluate weights log prior\n",
    "        \"\"\"\n",
    "        log_prior = 0\n",
    "        for module in self.children():\n",
    "            if 'calc_log_prior' in dir(module):\n",
    "                log_prior += module.calc_log_prior()\n",
    "        return log_prior\n",
    "\n",
    "    def calc_log_term_latent(self):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "\n",
    "        # theta = A theta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        # compute log data term log p(D|w)\n",
    "        log_term = torch.log(self.target_density(real_particles))\n",
    "\n",
    "        # evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "\n",
    "        grad_log_term = real_particles.grad\n",
    "\n",
    "        return grad_log_term\n",
    "\n",
    "    def calc_log_term_latent_net(self, x, y, train_size):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta))\n",
    "        Args:\n",
    "            x (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input:\n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output:\n",
    "                [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "\n",
    "        # get real net parameters: theta_i = A theta`_i + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        # init net with real parameters\n",
    "        self.vector_to_parameters(real_particles.view(-1), self.parameters_net())\n",
    "        # compute log prior of all weight in the net\n",
    "        log_prior = self.calc_log_prior_net()\n",
    "\n",
    "        # get prediction for the batch of data\n",
    "        y_predict = self.predict_net(x)\n",
    "        # compute log data term log p(D|w)\n",
    "        log_data = self.data_distribution.calc_log_data(x, y, y_predict, train_size)\n",
    "\n",
    "        # log_term = log p(theta) = log p_prior(theta) + log p_data(D|theta)\n",
    "        log_term = log_prior + log_data\n",
    "\n",
    "        # evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "\n",
    "        # collect all gradients into one vector\n",
    "        grad_log_term = self.parameters_grad_to_vector(self.parameters_net()).view(-1, self.n_particles)\n",
    "\n",
    "        return grad_log_term\n",
    "\n",
    "    def parameters_net(self):\n",
    "        \"\"\"\n",
    "            Return all trainable parameters\n",
    "        \"\"\"\n",
    "        return chain(self.net.parameters(), self.data_distribution.parameters())\n",
    "\n",
    "    def predict_net(self, x, inference=False):\n",
    "        \"\"\"\n",
    "            Use net to make predictions\n",
    "            Args:\n",
    "                x (array_like): batch of data\n",
    "                inference (bool): if False return logits instead of logp\n",
    "        \"\"\"\n",
    "        predictions = self.net(x.unsqueeze(0).expand(self.n_particles, *x.shape))\n",
    "        if self.task == 'net_reg':\n",
    "            if inference:\n",
    "                return torch.mean(predictions, dim=0)\n",
    "            else:\n",
    "                return predictions\n",
    "        elif self.task == 'net_class':\n",
    "            if inference:\n",
    "                return torch.log(torch.mean(torch.nn.Softmax(dim=2)(predictions), dim=0))\n",
    "            else:\n",
    "                return predictions\n",
    "\n",
    "    def update_latent(self,\n",
    "                      h_type, kernel_type='rbf', p=None,\n",
    "                      step_size=None,\n",
    "                      move_theta_0=False,\n",
    "                      burn_in=False, burn_in_coeff=None,\n",
    "                      epoch=None\n",
    "                      ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "\n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "\n",
    "        self.iter += 1\n",
    "\n",
    "        # Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent()\n",
    "\n",
    "        # Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "\n",
    "        # Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        # Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        # Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        # Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "        # Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            # Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "\n",
    "            # Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (\n",
    "                            self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            # Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "\n",
    "    def update_latent_net(self,\n",
    "                          h_type, kernel_type='rbf', p=None,\n",
    "                          x_batch=None, y_batch=None, train_size=None,\n",
    "                          step_size=None,\n",
    "                          move_theta_0=False,\n",
    "                          burn_in=False, burn_in_coeff=None,\n",
    "                          epoch=None\n",
    "                          ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "\n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "\n",
    "        self.iter += 1\n",
    "        self.net.zero_grad()\n",
    "        self.data_distribution.zero_grad()\n",
    "\n",
    "        # Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent_net(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_net(x_batch, y_batch, train_size)\n",
    "\n",
    "        # Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "\n",
    "        # Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        # Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        # Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        # Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "        # Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            # Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "\n",
    "            # Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (\n",
    "                            self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            # Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_to_parameters(vec, parameters):\n",
    "        pointer = 0\n",
    "        for param in parameters:\n",
    "            # The length of the parameter\n",
    "            num_param = param.numel()\n",
    "            # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "            param.data = vec[pointer:pointer + num_param].view_as(param).data\n",
    "            # Increment the pointer\n",
    "            pointer += num_param\n",
    "\n",
    "    @staticmethod\n",
    "    def parameters_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.view(-1))\n",
    "        return torch.cat(vec)\n",
    "\n",
    "    @staticmethod\n",
    "    def parameters_grad_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.grad.view(-1))\n",
    "        return torch.cat(vec)\n",
    "\n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        \"\"\"\n",
    "        destination = super(DistributionMover, self).state_dict(destination, prefix, keep_vars)\n",
    "\n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=self._version)\n",
    "\n",
    "        destination[prefix + 'particles'] = self.particles if keep_vars else self.particles.data\n",
    "        destination[prefix + 'historical_grad'] = self.historical_grad if keep_vars else self.historical_grad.data\n",
    "        if self.use_latent:\n",
    "            destination[prefix + 'historical_grad_theta_0'] = (self.historical_grad_theta_0 if keep_vars\n",
    "                                                               else self.historical_grad_theta_0.data)\n",
    "            self.lt.state_dict(destination, prefix + 'lt' + '.', keep_vars=keep_vars)\n",
    "        destination[prefix + 'step_size'] = self.step_size\n",
    "        destination[prefix + 'iter'] = self.iter\n",
    "        destination[prefix + 'epoch'] = self.epoch\n",
    "\n",
    "        return destination\n",
    "\n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        super(DistributionMover, self).load_state_dict(state_dict, prefix)\n",
    "\n",
    "        self.particles.copy_(state_dict[prefix + 'particles'])\n",
    "        self.historical_grad.copy_(state_dict[prefix + 'historical_grad'])\n",
    "        self.step_size = state_dict[prefix + 'step_size']\n",
    "        self.iter = state_dict[prefix + 'iter']\n",
    "        self.epoch = state_dict[prefix + 'epoch']\n",
    "\n",
    "        if self.use_latent:\n",
    "            self.historical_grad_theta_0.copy_(state_dict[prefix + 'historical_grad_theta_0'])\n",
    "            self.lt.load_state_dict(state_dict, prefix + 'lt' + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     1,
     12
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LRStrategy:\n",
    "    def __init__(self, step_size, factor=0.1, n_epochs=1, patience=10):\n",
    "        \"\"\"\n",
    "            Multiply @step_size by factor each @n_epochs epochs\n",
    "            Freeze @step_size after @patience epochs\n",
    "        \"\"\"\n",
    "        self.step_size = step_size\n",
    "        self.factor = factor\n",
    "        self.n_epochs = n_epochs\n",
    "        self.patience = patience\n",
    "        self.iter = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.iter += 1\n",
    "        if self.iter < self.patience and self.iter % self.n_epochs == 0:\n",
    "            self.step_size *= self.factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Add some methods to nn.Sequential to make code clear \n",
    "\n",
    "setattr(nn.Sequential, \"numel\", DistributionMover.numel)\n",
    "setattr(nn.Sequential, \"calc_log_prior\", DistributionMover.calc_log_prior_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Bostor housing dataset for regression task\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "x = boston['data']\n",
    "y = boston['target']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "x_train, x_test, y_train, y_test = (torch.tensor(x_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(x_test, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_test, dtype=t_type, device=device))\n",
    "\n",
    "### Linear Regression baseline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.tensor(model.predict(x_test), dtype=t_type, device=device), y_test), \n",
    "      torch.nn.MSELoss()(torch.tensor(model.predict(x_train), dtype=t_type, device=device), y_train))\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.mean(y_train).expand(y_test.shape[0]), y_test), \n",
    "      torch.nn.MSELoss()(torch.mean(y_train).expand(y_train.shape[0]), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Check all functions\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=5, n_particles=10, use_latent=True)\n",
    "dm.calc_log_term_latent()\n",
    "dm.calc_kernel_term_latent(0)\n",
    "dm.update_latent(0)\n",
    "\n",
    "net = nn.Sequential(SteinLinear(13, 1, 10, use_var_prior=True))\n",
    "dd = RegressionDistribution(n_particles=10, use_var_prior=False)\n",
    "dm = DistributionMover(task='net_reg', n_hidden_dims=5, n_particles=10, use_latent=True, net=net, data_distribution=dd)\n",
    "dm.calc_log_term_latent_net(x_train, y_train, x_train.shape[0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(13, 1, 100, use_var_prior=True, alpha=1e2, use_bias=True))\n",
    "data_distr = RegressionDistribution(100, use_var_prior=True, betta=1e-1)\n",
    "dm = DistributionMover(task='net_reg', n_particles=100, use_latent=False, net=net, data_distribution=data_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alpha = dm.net[0].alpha_weight\n",
    "betta = dm.data_distribution.betta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Append column of ones to data \n",
    "# xx = x_train\n",
    "# xx_test = x_test\n",
    "xx = torch.cat([x_train, torch.ones([x_train.shape[0], 1], dtype=t_type, device=device)], dim=1)\n",
    "xx_test = torch.cat([x_test, torch.ones([x_test.shape[0], 1], dtype=t_type, device=device)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sigma = torch.inverse(betta * xx.t() @ xx + alpha * torch.eye(xx.shape[1], dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu = betta * sigma @ xx.t() @ y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu @ xx.t(), y_train), torch.nn.MSELoss()(mu @ xx_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import *\n",
    "\n",
    "mod = WLS(y_train.data.cpu().numpy(), xx.data.cpu().numpy())\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stein_mu = (torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu, torch.tensor(results.params, dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, particles_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(sum(np.logical_and(results.conf_int()[:,0] < stein_mu.data.cpu().numpy(), stein_mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < mu.data.cpu().numpy(), mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < particles_mean.data.cpu().numpy(), particles_mean.data.cpu().numpy() < results.conf_int()[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    step_size = 0.002\n",
    "    dm.historical_grad.zero_()\n",
    "    for _ in range(100000):\n",
    "        dm.update_latent_net(h_type=1, kernel_type='rbf', p=-1, x_batch=x_train, y_batch=y_train, train_size=x_train.shape[0], step_size=step_size)\n",
    "        train_loss = torch.nn.MSELoss()(dm.predict_net(x_train, inference=True).view(-1), y_train)\n",
    "        test_loss = torch.nn.MSELoss()(dm.predict_net(x_test, inference=True).view(-1), y_test)\n",
    "        \n",
    "        if _ % 10 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train): {1:.3f}\\t Empirical Loss(Test): {2:.3f}\\t Kernel factor: {3:.3f}'.format(\n",
    "                            _, train_loss, test_loss, dm.h))\n",
    "            \n",
    "#             plot_projections(dm, use_real=True)\n",
    "#             plot_projections(dm, use_real=False)\n",
    "            plt.pause(1e-300)\n",
    "            \n",
    "        if _ % 3000 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import MNIST dataset with class selection\n",
    "\n",
    "sys.path.insert(0, '/home/m.nakhodnov/Samsung-Tasks/Datasets/MyMNIST')\n",
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform)\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform)\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=100, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=100, shuffle=False)\n",
    "\n",
    "\n",
    "transform_aug = transforms.Compose([\n",
    "        transforms.RandomAffine(degrees=90., \n",
    "                                translate=(0.25, 0.25),\n",
    "                                scale=(0.8, 1.2)\n",
    "                               ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "dataset_m_train_aug = MNIST_Class_Selection('.', train=True, download=True, transform=transform_aug)\n",
    "dataset_m_test_aug = MNIST_Class_Selection('.', train=False, transform=transform_aug)\n",
    "\n",
    "\n",
    "dataloader_m_train_aug = DataLoader(dataset_m_train_aug, batch_size=100, shuffle=True)\n",
    "dataloader_m_test_aug = DataLoader(dataset_m_test_aug, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MNIST with Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(dm,\n",
    "          dataloader_train, dataloader_test,\n",
    "          lr_str, start_epoch, end_epoch, n_epochs_save=20, n_epochs_log=1,\n",
    "          move_theta_0=False, plot_graphs=True, verbose=False,\n",
    "          checkpoint_file_name=None, plots_file_name=None, log_file_name=None,\n",
    "          n_warmup_epochs=16, n_previous=10\n",
    "          ):\n",
    "    # Get all y_test in one tensor\n",
    "    y_test_all = torch.tensor([], dtype=torch.int64, device=device)\n",
    "    for _, y_test in dataloader_test:\n",
    "        y_test = y_test.to(device=device)\n",
    "        y_test_all = torch.cat([y_test_all, y_test.data.detach().clone()], dim=0)\n",
    "    # WARNING: May be incorrect if output features of dm.net != n_classes\n",
    "    n_classes = len(dataloader_train.dataset.class_nums)\n",
    "\n",
    "    # Train loss/accuracy\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    # Test loss/accuracy\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    # Mean loss/accuracy from @n_warmup_epochs to current epoch\n",
    "    test_losses_mean = []\n",
    "    test_accs_mean = []\n",
    "    predictions_test_cumulative = torch.zeros([1, 1], dtype=t_type, device=device)\n",
    "    # Mean loss/accuracy from (current epoch - n_previous)  to current epoch\n",
    "    test_losses_mean_previous = []\n",
    "    test_accs_mean_previous = []\n",
    "    predictions_test_previous = torch.zeros([n_previous, y_test_all.shape[0], n_classes], dtype=t_type, device=device)\n",
    "    # Index of 'oldest' element in predictions_test_previous\n",
    "    pointer_to_the_back = 0\n",
    "\n",
    "    if log_file_name is not None:\n",
    "        log_file = open(log_file_name, 'a')\n",
    "        log_file.write('\\rNew run of training.\\r')\n",
    "        log_file.close()\n",
    "\n",
    "    epoch = start_epoch\n",
    "    try:\n",
    "        for epoch in range(start_epoch, end_epoch):\n",
    "            epoch_since_start = epoch - start_epoch\n",
    "\n",
    "            # One update of particles via all dataloader_train\n",
    "            for x, y in dataloader_train:\n",
    "                x = x.double().to(device=device).view(x.shape[0], -1)\n",
    "                y = y.to(device=device)\n",
    "                burn_in_coeff = max(1. - (1. - 1.) / 20. * epoch, 1.)\n",
    "                dm.update_latent_net(h_type=0, kernel_type='rbf', p=None,\n",
    "                                     x_batch=x, y_batch=y,\n",
    "                                     train_size=len(dataloader_train.dataset),\n",
    "                                     step_size=lr_str.step_size,\n",
    "                                     move_theta_0=move_theta_0,\n",
    "                                     burn_in=True, burn_in_coeff=burn_in_coeff,\n",
    "                                     epoch=epoch\n",
    "                                     )\n",
    "\n",
    "            # Evaluate cross entropy and accuracy over dataloader_train\n",
    "            train_loss = 0.\n",
    "            train_acc = 0.\n",
    "            for x_train, y_train in dataloader_train:\n",
    "                x_train = x_train.double().to(device=device).view(x_train.shape[0], -1)\n",
    "                y_train = y_train.to(device=device)\n",
    "\n",
    "                net_pred = dm.predict_net(x_train, inference=True)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                train_loss -= torch.sum(torch.gather(net_pred, 1, y_train.view(-1, 1)))\n",
    "                train_acc += torch.sum(y_pred == y_train).float()\n",
    "            train_loss /= (len(dataloader_train.dataset) + 0.)\n",
    "            train_acc /= (len(dataloader_train.dataset) + 0.)\n",
    "\n",
    "            # Evaluate cross entropy and accuracy over dataloader_test\n",
    "            test_loss = 0.\n",
    "            test_acc = 0.\n",
    "            predictions_test_current = torch.tensor([], dtype=t_type, device=device)\n",
    "            for x_test, y_test in dataloader_test:\n",
    "                x_test = x_test.double().to(device=device).view(x.shape[0], -1)\n",
    "                y_test = y_test.to(device=device)\n",
    "\n",
    "                # Get output of net before Softmax, mean and log, Shape = [n_particles, batch_size, output_features]\n",
    "                net_pred_pure = dm.predict_net(x_test, inference=False)\n",
    "                net_pred_pure = torch.mean(torch.nn.Softmax(dim=2)(net_pred_pure), dim=0)\n",
    "                predictions_test_current = torch.cat([predictions_test_current, net_pred_pure.data.detach().clone()],\n",
    "                                                     dim=0)\n",
    "\n",
    "                net_pred = torch.log(net_pred_pure)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                test_loss -= torch.sum(torch.gather(net_pred, 1, y_test.view(-1, 1)))\n",
    "                test_acc += torch.sum(y_pred == y_test).float()\n",
    "            test_loss /= (len(dataloader_test.dataset) + 0.)\n",
    "            test_acc /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            # Evaluate cross entropy and accuracy over dataloader_test using\n",
    "            # all predictions from previous (@epoch_since_start - @n_warmup_epochs) epochs\n",
    "            test_loss_mean = 0.\n",
    "            test_acc_mean = 0.\n",
    "            if epoch_since_start >= n_warmup_epochs:\n",
    "                predictions_test_cumulative = (\n",
    "                        predictions_test_cumulative * (epoch_since_start - n_warmup_epochs) / (\n",
    "                            epoch_since_start - n_warmup_epochs + 1.) +\n",
    "                        predictions_test_current / (epoch_since_start - n_warmup_epochs + 1.))\n",
    "                log_predictions_test = torch.log(predictions_test_cumulative)\n",
    "                y_pred_all = torch.argmax(log_predictions_test, dim=1)\n",
    "\n",
    "                test_loss_mean = -torch.sum(torch.gather(log_predictions_test, 1, y_test_all.view(-1, 1)))\n",
    "                test_acc_mean = torch.sum(y_pred_all == y_test_all).float()\n",
    "                test_loss_mean /= (len(dataloader_test.dataset) + 0.)\n",
    "                test_acc_mean /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            # Evaluate cross entropy and accuracy over dataloader_test using\n",
    "            # all predictions from previous @n_previous epochs\n",
    "            test_loss_mean_previous = 0.\n",
    "            test_acc_mean_previous = 0.\n",
    "            predictions_test_previous[pointer_to_the_back] = predictions_test_current\n",
    "            if pointer_to_the_back + 1 == n_previous:\n",
    "                pointer_to_the_back = 0\n",
    "            else:\n",
    "                pointer_to_the_back += 1\n",
    "            if epoch_since_start + 1 >= n_previous:\n",
    "                log_predictions_test = torch.log(torch.mean(predictions_test_previous, dim=0))\n",
    "                y_pred_all = torch.argmax(log_predictions_test, dim=1)\n",
    "                test_loss_mean_previous = -torch.sum(torch.gather(log_predictions_test, 1, y_test_all.view(-1, 1)))\n",
    "                test_acc_mean_previous = torch.sum(y_pred_all == y_test_all).float()\n",
    "                test_loss_mean_previous /= (len(dataloader_test.dataset) + 0.)\n",
    "                test_acc_mean_previous /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            # Append evaluated losses and accuracies\n",
    "            train_losses.append(train_loss.data[0].cpu().numpy())\n",
    "            train_accs.append(train_acc.data[0].cpu().numpy())\n",
    "            test_losses.append(test_loss.data[0].cpu().numpy())\n",
    "            test_accs.append(test_acc.data[0].cpu().numpy())\n",
    "            if epoch_since_start >= n_warmup_epochs:\n",
    "                test_losses_mean.append(test_loss_mean.data[0].cpu().numpy())\n",
    "                test_accs_mean.append(test_acc_mean.data[0].cpu().numpy())\n",
    "            else:\n",
    "                test_losses_mean.append(None)\n",
    "                test_accs_mean.append(None)\n",
    "            if epoch_since_start + 1 >= n_previous:\n",
    "                test_losses_mean_previous.append(test_loss_mean_previous.data[0].cpu().numpy())\n",
    "                test_accs_mean_previous.append(test_acc_mean_previous.data[0].cpu().numpy())\n",
    "            else:\n",
    "                test_losses_mean_previous.append(None)\n",
    "                test_accs_mean_previous.append(None)\n",
    "\n",
    "            # Print log into console and file\n",
    "            if epoch % n_epochs_log == 0:\n",
    "                sys.stdout.write(\n",
    "                    ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                     '\\nEmpirical Loss (Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {4:.3f}/{5:.3f}/{6:.3f}/{7:.3f}' +\n",
    "                     '\\nAccuracy (Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {8:.3f}/{9:.3f}/{10:.3f}/{11:.3f}\\t'\n",
    "                     ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                              train_loss, test_loss, test_loss_mean, test_loss_mean_previous,\n",
    "                              train_acc, test_acc, test_acc_mean, test_acc_mean_previous\n",
    "                              )\n",
    "                )\n",
    "                if log_file_name is not None:\n",
    "                    log_file = open(log_file_name, 'a')\n",
    "                    log_file.write(\n",
    "                        ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                         '\\nEmpirical Loss(Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {4:.3f}/{5:.3f}/{6:.3f}/{7:.3f}' +\n",
    "                         '\\nAccuracy(Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {8:.3f}/{9:.3f}/{10:.3f}/{11:.3f}\\t'\n",
    "                         ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                                  train_loss, test_loss, test_loss_mean, test_loss_mean_previous,\n",
    "                                  train_acc, test_acc, test_acc_mean, test_acc_mean_previous\n",
    "                                  )\n",
    "                    )\n",
    "                    log_file.close()\n",
    "\n",
    "            if epoch % n_epochs_save == 0 and epoch > start_epoch and checkpoint_file_name is not None:\n",
    "                torch.save(dm.state_dict(), checkpoint_file_name.format(start_epoch, epoch))\n",
    "\n",
    "            # Update step_size\n",
    "            lr_str.step()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    if plot_graphs:\n",
    "        print_plots([[train_losses, test_losses, test_losses_mean, test_losses_mean_previous],\n",
    "                     [train_accs, test_accs, test_accs_mean, test_accs_mean_previous]],\n",
    "                    [['Epochs', ''],\n",
    "                     ['Epochs', '% * 1e-2']],\n",
    "                    [['Cross Entropy Loss (Train)', 'Cross Entropy Loss (Test)', 'Cross Entropy Loss (Test (Mean))',\n",
    "                      'Cross Entropy Loss (Test (Mean (n_prev)))'],\n",
    "                     ['Accuracy (Train)', 'Accuracy (Test)', 'Accuracy (Mean)', 'Accuracy (Mean (n_prev))']\n",
    "                     ],\n",
    "                    plots_file_name.format(start_epoch, epoch)\n",
    "                    )\n",
    "    if checkpoint_file_name is not None:\n",
    "        torch.save(dm.state_dict(), checkpoint_file_name.format(start_epoch, epoch))\n",
    "\n",
    "    if verbose:\n",
    "        return (train_losses, test_losses, test_losses_mean, test_losses_mean_previous,\n",
    "                train_accs, test_accs, test_accs_mean, test_accs_mean_previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1. particles - 1 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### MAP estimate for MNIST classification task\n",
    "net_1 = nn.Sequential(SteinLinear(28 * 28, 18, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 1, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_1 = ClassificationDistribution(1)\n",
    "dm_1 = DistributionMover(task='net_class', n_particles=1, use_latent=False, net=net_1, data_distribution=data_distr_1)\n",
    "lr_str_1 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_1 = 'model_1'\n",
    "version_1 = 19\n",
    "checkpoint_file_name_1 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_1 + '.pth'\n",
    "plots_file_name_1 = './Experiments/Plots/' + own_name_1 + '.png'\n",
    "log_file_name_1 = './Experiments/Logs/' + own_name_1 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_1.format(version_1)):\n",
    "    dm_1.load_state_dict(torch.load(checkpoint_file_name_1.format(version_1)))\n",
    "    lr_str_1.step_size = dm_1.step_size\n",
    "    lr_str_1.iter = dm_1.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_1,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_1, start_epoch=lr_str_1.iter, end_epoch=26,\n",
    "      checkpoint_file_name=checkpoint_file_name_1, plots_file_name=plots_file_name_1, log_file_name=log_file_name_1,\n",
    "      n_warmup_epochs=5\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(dataloader_m_train))\n",
    "# x = x.double().to(device=device).view(x.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_1.update_latent_net(h_type=0, kernel_type='rbf', p=None, x_batch=x, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_1.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2. particles - 5 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_2 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_2 = ClassificationDistribution(5)\n",
    "dm_2 = DistributionMover(task='net_class',\n",
    "                         n_particles=5,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_2,\n",
    "                         data_distribution=data_distr_2\n",
    "                        )\n",
    "lr_str_2 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_2 = 'model_2'\n",
    "version_2 = 0\n",
    "checkpoint_file_name_2 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_2 + '.pth'\n",
    "plots_file_name_2 = './Experiments/Plots/' + own_name_2 + '.png'\n",
    "log_file_name_2 = './Experiments/Logs/' + own_name_2 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_2.format(version_2)):\n",
    "    dm_2.load_state_dict(torch.load(checkpoint_file_name_2.format(version_2)))\n",
    "    lr_str_2.step_size = dm_2.step_size\n",
    "    lr_str_2.iter = dm_2.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_2,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_2, start_epoch=lr_str_2.iter, end_epoch=lr_str_2.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_2, plots_file_name=plots_file_name_2, log_file_name=log_file_name_2,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(dataloader_m_train))\n",
    "# x = x.double().to(device=device).view(x.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_2.update_latent_net(h_type=0, kernel_type='rbf', p=None, x_batch=x, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_2.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3. particles - 5 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_3 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_3 = ClassificationDistribution(5)\n",
    "dm_3 = DistributionMover(task='net_class',\n",
    "                       n_particles=5,\n",
    "                       use_latent=False,\n",
    "                       net=net_3,\n",
    "                       data_distribution=data_distr_3\n",
    "                      )\n",
    "lr_str_3 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_3 = 'model_3'\n",
    "version_3 = 0\n",
    "checkpoint_file_name_3 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_3 + '.pth'\n",
    "plots_file_name_3 = './Experiments/Plots/' + own_name_3 + '.png'\n",
    "log_file_name_3 = './Experiments/Logs/' + own_name_3 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_3.format(version_3)):\n",
    "    dm_3.load_state_dict(torch.load(checkpoint_file_name_3.format(version_3)))\n",
    "    lr_str_3.step_size = dm_3.step_size\n",
    "    lr_str_3.iter = dm_3.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_3,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_3, start_epoch=lr_str_3.iter, end_epoch=lr_str_3.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_3, plots_file_name=plots_file_name_3, log_file_name=log_file_name_3,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(dataloader_m_train))\n",
    "# x = x.double().to(device=device).view(x.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_3.update_latent_net(h_type=0, kernel_type='rbf', p=None, x_batch=x, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_3.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4. particles - 20 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_4 = nn.Sequential(SteinLinear(28 * 28, 18, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 20, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_4 = ClassificationDistribution(20)\n",
    "dm_4 = DistributionMover(task='net_class',\n",
    "                         n_particles=20,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_4,\n",
    "                         data_distribution=data_distr_4\n",
    "                        )\n",
    "lr_str_4 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_4 = 'model_4'\n",
    "version_4 = 0\n",
    "checkpoint_file_name_4 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_4 + '.pth'\n",
    "plots_file_name_4 = './Experiments/Plots/' + own_name_4 + '.png'\n",
    "log_file_name_4 = './Experiments/Logs/' + own_name_4 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_4.format(version_4)):\n",
    "    dm_4.load_state_dict(torch.load(checkpoint_file_name_4.format(version_4)))\n",
    "    lr_str_4.step_size = dm_4.step_size\n",
    "    lr_str_4.iter = dm_4.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_4,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_4, start_epoch=lr_str_4.iter, end_epoch=lr_str_4.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_4, plots_file_name=plots_file_name_4, log_file_name=log_file_name_4,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "config_name = 'config_model_24'\n",
    "config = importlib.import_module('Experiments.Configs.' + config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_dm = nn.Sequential(SteinLinear(28 * 28, 300, config.n_particles, use_var_prior=config.use_var_prior, alpha=config.alpha),\n",
    "                       nn.Tanh(),\n",
    "                       SteinLinear(300, 100, config.n_particles, use_var_prior=config.use_var_prior, alpha=config.alpha),\n",
    "                       nn.Tanh(),\n",
    "                       SteinLinear(100, 10, config.n_particles, use_var_prior=config.use_var_prior, alpha=config.alpha)\n",
    "                      ).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_distr = ClassificationDistribution(config.n_particles)\n",
    "dm = DistributionMover(task='net_class',\n",
    "                       n_particles=config.n_particles,\n",
    "                       n_hidden_dims=config.n_hidden_dims,\n",
    "                       use_latent=config.use_latent,\n",
    "                       net=net_dm,\n",
    "                       data_distribution=data_distr,\n",
    "                       dummy=True\n",
    "                      )\n",
    "lr_str = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "checkpoint_file_name = ('./Experiments/Checkpoints/' + 'e{0}-{1}_' + config.experiment_name + '.pth').format(0, 199)\n",
    "dm.load_state_dict(torch.load(checkpoint_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 300),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(300, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(100, 10)\n",
    ").to(device=device).double()\n",
    "\n",
    "\n",
    "checkpoint_file_name = ('./Experiments/Checkpoints/' + 'e{0}-{1}_' + 'ml_est' + '.pth').format(0,199)\n",
    "net.load_state_dict(torch.load(checkpoint_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(torch.mean(torch.var(dm.particles, dim=1)),\n",
    "torch.mean(torch.var(dm.lt.transform(dm.particles, n_particles_second=True), dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def modify_sample_net(net, samples, real_labels, target_labels=None, eps=1e-2, n_iters=1):\n",
    "    for idx in range(n_iters):\n",
    "        samples = samples.detach().clone().requires_grad_(True)\n",
    "        \n",
    "        predictions = net(samples)\n",
    "        predictions = torch.log(torch.nn.Softmax()(predictions))\n",
    "        \n",
    "        train_loss = -torch.sum(torch.gather(predictions, 1, real_labels.view(-1, 1)))\n",
    "    \n",
    "        train_loss.backward()\n",
    "        perturbation = torch.sign(samples.grad)    \n",
    "        samples = torch.clamp(samples + eps * perturbation, -0.42, 2.8)\n",
    "        \n",
    "    return samples.detach(), perturbation.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def modify_sample(dm, samples, real_labels, target_labels=None, eps=1e-2, n_iters=1, use_full=False):\n",
    "    for idx in range(n_iters):\n",
    "        samples = samples.detach().clone().requires_grad_(True)\n",
    "        \n",
    "        predictions = dm.predict_net(samples, inference=False)\n",
    "        \n",
    "        if use_full:\n",
    "            predictions = predictions[:]\n",
    "        else:\n",
    "            predictions = predictions[0:1]\n",
    "        predictions = torch.log(torch.mean(torch.nn.Softmax(dim=2)(predictions), dim=0))\n",
    "\n",
    "        train_loss = -torch.sum(torch.gather(predictions, 1, real_labels.view(-1, 1)))\n",
    "\n",
    "        train_loss.backward()\n",
    "        perturbation = torch.sign(samples.grad)        \n",
    "        samples = torch.clamp(samples + eps * perturbation, -0.42, 2.8)\n",
    "\n",
    "    return samples.detach(), perturbation.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples, real_labels = next(iter(dataloader_m_test))\n",
    "samples = samples.double().to(device=device).view(samples.shape[0], -1)\n",
    "real_labels = real_labels.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "modified_samples, perturbation = modify_sample(dm, samples, real_labels, eps=0.01, n_iters=20)\n",
    "modified_samples_net, perturbation_net = modify_sample_net(net, samples, real_labels, eps=0.01, n_iters=20)\n",
    "\n",
    "modified_samples_aug, _ = next(iter(dataloader_m_test_aug))\n",
    "modified_samples_aug = modified_samples_aug.double().to(device=device).view(modified_samples_aug.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_idx = 1\n",
    "plt.figure()\n",
    "plt.imshow(samples[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(modified_samples[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(modified_samples_net[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(modified_samples_aug[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Stein (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = torch.argmax(dm.predict_net(samples, inference=True), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.exp(dm.predict_net(samples, inference=True)[plot_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Net (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions_net = torch.argmax(net(samples), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nn.Softmax()(net(samples))[plot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Stein (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions_modified = torch.argmax(dm.predict_net(modified_samples, inference=True), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.exp(dm.predict_net(modified_samples, inference=True)[plot_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Net (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions_modified_net = torch.argmax(net(modified_samples_net), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nn.Softmax()(net(modified_samples_net))[plot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Stein (Aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions_modified_aug = torch.argmax(dm.predict_net(modified_samples_aug, inference=True), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.exp(dm.predict_net(modified_samples_aug, inference=True)[plot_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Net (Aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions_modified_net_aug = torch.argmax(net(modified_samples_aug), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nn.Softmax()(net(modified_samples_aug))[plot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Stein (Original) - Stein (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.argwhere(predictions - predictions_modified != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Net (Original) - Net (Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.argwhere(predictions_net - predictions_modified_net != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Stein (Original) - Stein (Aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.argwhere(predictions - predictions_modified_aug != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Net (Original) - Net (Aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.argwhere(predictions_net - predictions_modified_net_aug != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def perform_adv_attack(dataloader, dm, net, modifier, modifier_net, **modifier_kwargs):\n",
    "    cnt = 0.\n",
    "    cnt_net = 0.\n",
    "    cross_entr = 0.\n",
    "    cross_entr_net = 0.\n",
    "    cross_entr_modified = 0.\n",
    "    cross_entr_modified_net = 0.\n",
    "    for samples, real_labels in dataloader:\n",
    "        samples = samples.double().to(device=device).view(samples.shape[0], -1)\n",
    "        real_labels = real_labels.to(device=device)\n",
    "\n",
    "        ### modify samples\n",
    "        modified_samples, _ = modifier(dm=dm, samples=samples, real_labels=real_labels, target_labels=None, **modifier_kwargs)\n",
    "        modified_samples_net, _ = modifier_net(net=net, samples=samples, real_labels=real_labels, target_labels=None, **modifier_kwargs)\n",
    "\n",
    "        ### evaluate crossentropy on real samples\n",
    "        pred = dm.predict_net(samples, inference=False)\n",
    "        pred_slice = pred[2:3]\n",
    "        log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "        cross_entr -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        pred_net = net(samples)\n",
    "        log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "        cross_entr_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        ### evaluate crossentropy on modified samples\n",
    "        pred = dm.predict_net(modified_samples, inference=False)\n",
    "        pred_slice = pred[2:3]\n",
    "        log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "        cross_entr_modified -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        pred_net = net(modified_samples_net)\n",
    "        log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "        cross_entr_modified_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        ### evaluate error rate\n",
    "        pred = torch.argmax(dm.predict_net(samples, inference=True), dim=1)\n",
    "        pred_net = torch.argmax(net(samples), dim=1)\n",
    "\n",
    "        pred_modified = torch.argmax(dm.predict_net(modified_samples, inference=True), dim=1)\n",
    "        pred_modified_net = torch.argmax(net(modified_samples_net), dim=1)\n",
    "\n",
    "        cnt += len(np.argwhere(pred - pred_modified != 0).view(-1))\n",
    "        cnt_net += len(np.argwhere(pred_net - pred_modified_net != 0).view(-1))\n",
    "    print(('Error Rate (Stein/Net): {0:.3f}/{1:.3f}\\t' + \n",
    "           'Cross Entropy (Original)(Stein/Net): {2:.3f}/{3:.3f}\\t' +  \n",
    "           'Cross Entropy (Modified)(Stein/Net): {4:.3f}/{5:.3f}'\n",
    "          ).format(cnt / len(dataloader_m_test.dataset), cnt_net / len(dataloader_m_test.dataset), \n",
    "                   cross_entr / len(dataloader_m_test.dataset), cross_entr_net / len(dataloader_m_test.dataset),\n",
    "                   cross_entr_modified / len(dataloader_m_test.dataset), cross_entr_modified_net / len(dataloader_m_test.dataset)\n",
    "                  )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def perform_aug_attack(dataloader, dataloader_aug, dm, net):\n",
    "    cnt = 0.\n",
    "    cnt_net = 0.\n",
    "    cross_entr = 0.\n",
    "    cross_entr_net = 0.\n",
    "    cross_entr_modified = 0.\n",
    "    cross_entr_modified_net = 0.\n",
    "    \n",
    "    iterator = iter(dataloader)\n",
    "    iterator_aug = iter(dataloader_aug)\n",
    "    for _ in range(len(dataloader)):\n",
    "        samples, real_labels = next(iterator)\n",
    "        modified_samples, _ = next(iterator_aug)\n",
    "        \n",
    "        samples = samples.double().to(device=device).view(samples.shape[0], -1)\n",
    "        modified_samples = modified_samples.double().to(device=device).view(modified_samples.shape[0], -1)\n",
    "        modified_samples_net = modified_samples.detach()\n",
    "        real_labels = real_labels.to(device=device)\n",
    "\n",
    "        ### evaluate crossentropy on real samples\n",
    "        pred = dm.predict_net(samples, inference=False)\n",
    "        pred_slice = pred[2:3]\n",
    "        log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "        cross_entr -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        pred_net = net(samples)\n",
    "        log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "        cross_entr_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        ### evaluate crossentropy on modified samples\n",
    "        pred = dm.predict_net(modified_samples, inference=False)\n",
    "        pred_slice = pred[2:3]\n",
    "        log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "        cross_entr_modified -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        pred_net = net(modified_samples_net)\n",
    "        log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "        cross_entr_modified_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "\n",
    "        ### evaluate error rate\n",
    "        pred = torch.argmax(dm.predict_net(samples, inference=True), dim=1)\n",
    "        pred_net = torch.argmax(net(samples), dim=1)\n",
    "\n",
    "        pred_modified = torch.argmax(dm.predict_net(modified_samples, inference=True), dim=1)\n",
    "        pred_modified_net = torch.argmax(net(modified_samples_net), dim=1)\n",
    "\n",
    "        cnt += len(np.argwhere(pred - pred_modified != 0).view(-1))\n",
    "        cnt_net += len(np.argwhere(pred_net - pred_modified_net != 0).view(-1))\n",
    "    print(('Error Rate (Stein/Net): {0:.3f}/{1:.3f}\\t' + \n",
    "           'Cross Entropy (Original)(Stein/Net): {2:.3f}/{3:.3f}\\t' +  \n",
    "           'Cross Entropy (Modified)(Stein/Net): {4:.3f}/{5:.3f}'\n",
    "          ).format(cnt / len(dataloader.dataset), cnt_net / len(dataloader.dataset), \n",
    "                   cross_entr / len(dataloader.dataset), cross_entr_net / len(dataloader.dataset),\n",
    "                   cross_entr_modified / len(dataloader.dataset), cross_entr_modified_net / len(dataloader.dataset)\n",
    "                  )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perform_adv_attack(dataloader_m_test, dm, net, modify_sample, modify_sample_net, eps=0.01, n_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "perform_aug_attack(dataloader_m_test, dataloader_m_test_aug, dm, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MNIST with FC neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_nn = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ").double()\n",
    "optim_nn = torch.optim.Adam(net_nn.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for _, (x, y) in enumerate(dataloader_m_train):\n",
    "        x = x.double().view(x.shape[0], -1)\n",
    "        \n",
    "        optim_nn.zero_grad()\n",
    "        train_loss = nn.CrossEntropyLoss()(net_nn(x), y)\n",
    "        train_loss.backward()\n",
    "        optim_nn.step()\n",
    "   \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(30):\n",
    "            x_train, y_train = next(iter(dataloader_m_train))\n",
    "            x_train = x_train.double().view(x.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(x_train)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(net_nn(x_train), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 30.\n",
    "        train_acc /= (30. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(30):\n",
    "            x_test, y_test = next(iter(dataloader_m_test))\n",
    "            x_test = x_test.double().view(x.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(x_test)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(net_nn(x_test), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 30.\n",
    "        test_acc /= (30. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            _, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_test, y_test = next(iter(dataloader_m_test))\n",
    "x_test = x_test.double().view(x.shape[0], -1)\n",
    "y_pred = net_nn(x_test)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marginal_density = lambda x : (0.3 * NormalDensity(mu=-2., std=1., n=1)(x) + 0.7 * NormalDensity(mu=2., std=1., n=1)(x))\n",
    "#marginal_density = lambda x : (NormalDensity(mu=0., std=2., n=1)(x))\n",
    "\n",
    "pdf = []\n",
    "for _ in np.linspace(-10, 10, 1000): \n",
    "    _ = torch.tensor(_, dtype=t_type, device=device)\n",
    "    pdf.append(marginal_density(_))\n",
    "    \n",
    "from pynverse import inversefunc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "cum_density = interp1d(np.linspace(-20, 20, 250), [quad(marginal_density, -20, x)[0] for x in np.linspace(-20, 20, 250)])\n",
    "inv_cum_density = inversefunc(cum_density)\n",
    "real_samples = [inv_cum_density(np.random.random()) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean = quad(lambda x : marginal_density(x) * x, -20, 20)[0]\n",
    "var = quad(lambda x : marginal_density(x) * (x - mean) ** 2, -20, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10, 10, 1000), pdf)\n",
    "plt.plot(real_samples, np.zeros_like(real_samples))\n",
    "sns.kdeplot(real_samples, kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=20, n_hidden_dims=5, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot_projections(dm, use_real=True, n_plots_max=1, pdf=pdf)\n",
    "# plot_projections(dm, use_real=False, n_plots_max=1, pdf=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_projections(dm, use_real=True, kernel='tri', pdf=pdf)\n",
    "            plot_projections(dm, use_real=False, kernel='tri', pdf=pdf)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(torch.mean(dm.lt.transform(dm.particles, n_particles_second=True)),\n",
    " torch.mean(torch.std(dm.lt.transform(dm.particles, n_particles_second=True), dim=1) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Conditional Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=2, n_hidden_dims=1, use_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_condition_distribution(dm, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(200): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "            \n",
    "            #plot_projections(dm, use_real=True, pdf=pdf)\n",
    "            plot_condition_distribution(dm, 100000)\n",
    "            \n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# Make data.\n",
    "x = np.arange(-5, 5, 0.025)\n",
    "Y = np.arange(-5, 5, 0.025)\n",
    "xx, YY = np.meshgrid(x, Y)\n",
    "xY = torch.stack([torch.tensor(xx, dtype=t_type), torch.tensor(YY, dtype=t_type)], dim=2)\n",
    "Z = torch.zeros([xY.shape[0], xY.shape[1]])\n",
    "\n",
    "Z = dm.real_target_density(xY.permute(2, 0, 1).view(2, -1)).view(Z.shape)\n",
    "Z = Z.cpu().data.numpy()\n",
    "\n",
    "# Plot the surface.\n",
    "plt.contour(x, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "# Plot conditioning line\n",
    "xxx, YYY = dm.lt.transform(torch.tensor(x, dtype=t_type, device=device).view(1, -1), n_particles_second=True).data.cpu().numpy()\n",
    "plt.plot(xxx, YYY, 'r', label='Linear manifold', )\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Density contour lines\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare implementations of Stein Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class SVGD():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def svgd_kernel(self, theta, h = -1):\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = h / np.log(theta.shape[0]+1)\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = 2. * dxkxy / (h)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    " \n",
    "    def update(self, x0, lnprob, n_iter = 1000, stepsize = 1e-3, bandwidth = -1, alpha = 0.9, debug = False):\n",
    "        # Check input\n",
    "        if x0 is None or lnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        theta = np.copy(x0) \n",
    "        \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter+1) % 1000 == 0:\n",
    "                print( 'iter ' + str(iter+1) )\n",
    "            \n",
    "            lnpgrad = lnprob(theta)\n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(theta, h = bandwidth)  \n",
    "            grad_theta = (np.matmul(kxy, lnpgrad) + dxkxy) / x0.shape[0]  \n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + grad_theta ** 2\n",
    "            else:\n",
    "                historical_grad = alpha * historical_grad + (1 - alpha) * (grad_theta ** 2)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            theta = theta + stepsize * adj_grad \n",
    "            \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_ex = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lnprob(particles):\n",
    "    grad_log_term = torch.zeros([particles.shape[0], particles.shape[1]], dtype=t_type, device=device)\n",
    "        \n",
    "    for idx in range(100):\n",
    "        particle = torch.tensor(particles[idx:idx+1], dtype=t_type, device=device, requires_grad=True)\n",
    "        log_term = torch.log(dm_ex.target_density(particle.t()))\n",
    "        grad_log_term[idx] = torch.autograd.grad(log_term, particle,\n",
    "                                                     only_inputs=True, retain_graph=False, \n",
    "                                                     create_graph=False, allow_unused=False)[0]\n",
    "    return grad_log_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = dm_ex.particles.data.clone().detach().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare kernel implementation\n",
    "\n",
    "ker, grad_ker = dm_ex.calc_kernel_term_latent(10.)\n",
    "print(dm_ex.h)\n",
    "grad_ker_sum = torch.sum(grad_ker, dim=1)\n",
    "\n",
    "ker_l, grad_ker_sum_l = dm_svgd.svgd_kernel(result, h=10.)\n",
    "\n",
    "print((ker - torch.tensor(ker_l, dtype=t_type, device=device)).norm())\n",
    "print((grad_ker_sum - torch.tensor(grad_ker_sum_l, dtype=t_type, device=device).t()).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare log_term implementation\n",
    "(lnprob(result) - dm_ex.calc_log_term_latent().t()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(201): \n",
    "    try:\n",
    "        result = dm_svgd.update(result, lnprob, stepsize=1e-2, bandwidth=-1, n_iter=20)\n",
    "        result = torch.tensor(result, dtype=t_type, device=device)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(result.data.cpu().numpy(), torch.zeros_like(result).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(result.data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(2000): \n",
    "    try:\n",
    "        dm_ex.update_latent(h_type=0)\n",
    "\n",
    "        if _ % 30 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm_ex.h))\n",
    "            \n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(dm_ex.particles.t().data.cpu().numpy(), torch.zeros_like(dm_ex.particles.t()).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(dm_ex.particles.t().data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(result - dm_ex.particles.t()).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"Stein-Variational-Gradient-Descent/python/\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.matlib as nm\n",
    "from svgd import SVGD\n",
    "\n",
    "r\"\"\"\n",
    "    Example of Bayesian Logistic Regression (the same setting as Gershman et al. 2012):\n",
    "    The observed data D = {x, y} consist of N binary class labels, \n",
    "    y_t \\in {-1,+1}, and d covariates for each datapoint, x_t \\in R^d.\n",
    "    The hidden variables \\theta = {w, \\alpha} consist of d regression coefficients w_k \\in R,\n",
    "    and a precision parameter \\alpha \\in R_+. We assume the following model:\n",
    "        p(\\alpha) = Gamma(\\alpha; a, b)\n",
    "        p(w_k | a) = N(w_k; 0, \\alpha^-1)\n",
    "        p(y_t = 1| x_t, w) = 1 / (1+exp(-w^T x_t))\n",
    "\"\"\"\n",
    "class BayesianLR:\n",
    "    def __init__(self, x, Y, batchsize=100, a0=1, b0=0.01):\n",
    "        self.x, self.Y = x, Y\n",
    "        # TODO. Y in \\in{+1, -1}\n",
    "        self.batchsize = min(batchsize, x.shape[0])\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        \n",
    "        self.N = x.shape[0]\n",
    "        self.permutation = np.random.permutation(self.N)\n",
    "        self.iter = 0\n",
    "    \n",
    "        \n",
    "    def dlnprob(self, theta):\n",
    "        \n",
    "        if self.batchsize > 0:\n",
    "            batch = [ i % self.N for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize) ]\n",
    "            ridx = self.permutation[batch]\n",
    "            self.iter += 1\n",
    "        else:\n",
    "            ridx = np.random.permutation(self.x.shape[0])\n",
    "            \n",
    "        xs = self.x[ridx, :]\n",
    "        Ys = self.Y[ridx]\n",
    "        \n",
    "        w = theta[:, :-1]  # logistic weights\n",
    "        alpha = np.exp(theta[:, -1])  # the last column is logalpha\n",
    "        d = w.shape[1]\n",
    "        \n",
    "        wt = np.multiply((alpha / 2), np.sum(w ** 2, axis=1))\n",
    "        \n",
    "        coff = np.matmul(xs, w.T)\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-1 * coff))\n",
    "        \n",
    "        dw_data = np.matmul(((nm.repmat(np.vstack(Ys), 1, theta.shape[0]) + 1) / 2.0 - y_hat).T, xs)  # Y \\in {-1,1}\n",
    "        dw_prior = -np.multiply(nm.repmat(np.vstack(alpha), 1, d) , w)\n",
    "        dw = dw_data * 1.0 * self.x.shape[0] / xs.shape[0] + dw_prior  # re-scale\n",
    "        \n",
    "        dalpha = d / 2.0 - wt + (self.a0 - 1) - self.b0 * alpha + 1  # the last term is the jacobian term\n",
    "        \n",
    "        return np.hstack([dw, np.vstack(dalpha)])  # % first order derivative \n",
    "    \n",
    "    def evaluation(self, theta, x_test, y_test):\n",
    "        theta = theta[:, :-1]\n",
    "        M, n_test = theta.shape[0], len(y_test)\n",
    "\n",
    "        prob = np.zeros([n_test, M])\n",
    "        for t in range(M):\n",
    "            coff = np.multiply(y_test, np.sum(-1 * np.multiply(nm.repmat(theta[t, :], n_test, 1), x_test), axis=1))\n",
    "            prob[:, t] = np.divide(np.ones(n_test), (1 + np.exp(coff)))\n",
    "        \n",
    "        prob = np.mean(prob, axis=1)\n",
    "        acc = np.mean(prob > 0.5)\n",
    "        llh = np.mean(np.log(prob))\n",
    "        return [acc, llh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = x_test.shape[0] + x_train.shape[0]\n",
    "x_input = np.hstack([x, np.ones([N, 1])])\n",
    "y_input = y\n",
    "d = x_input.shape[1]\n",
    "D = d + 1\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_input, y_input, test_size=0.2, random_state=42)\n",
    "    \n",
    "a0, b0 = 1, 0.01 #hyper-parameters\n",
    "model = BayesianLR(x_train, y_train, 100, a0, b0) # batchsize = 100\n",
    "    \n",
    "# initialization\n",
    "M = 100  # number of particles\n",
    "theta0 = np.zeros([M, D]);\n",
    "alpha0 = np.random.gamma(a0, b0, M); \n",
    "for i in range(M):\n",
    "    theta0[i, :] = np.hstack([np.random.normal(0, np.sqrt(1 / alpha0[i]), d), np.log(alpha0[i])])\n",
    "    \n",
    "theta = SVGD().update(x0=theta0, lnprob=model.dlnprob, bandwidth=-1, n_iter=6000, stepsize=0.05, alpha=0.9, debug=True)\n",
    "    \n",
    "print('[accuracy, log-likelihood]')\n",
    "print(model.evaluation(theta, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Metropolis–Hastings algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MHAlgorithm():\n",
    "    def __init__(self, n_dims): \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.target_density = lambda x, *args, **kwargs : (0.3 * NormalDensity(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                           0.7 * NormalDensity(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "        self.real_target_density = lambda x, *args, **kwargs : (0.3 * NormalDensity(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                0.7 * NormalDensity(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "        \n",
    "#         self.target_density = lambda x : (NormalDensity(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "#         self.real_target_density = lambda x : (NormalDensity(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "    \n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_dims, 1],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., -1.)\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "    def generate_next(self):\n",
    "        previous = self.particles[:, -1].unsqueeze(1)\n",
    "#         self.additional_distribution = NormalDensity(self.n_dims, previous, 1, n_particles_second=True)\n",
    "#         candidate = self.additional_distribution.get_sample()\n",
    "        \n",
    "#         alpha = (self.target_density(candidate) * self.additional_distribution.unnormed_density(previous) / \n",
    "#                 (self.target_density(previous) * self.additional_distribution.unnormed_density(candidate)))\n",
    "        candidate = previous + torch.zeros([self.n_dims, 1], device=device, dtype=t_type).uniform_(-1., 1.)\n",
    "        alpha = self.target_density(candidate) / self.target_density(previous)\n",
    "        \n",
    "#         print(self.target_density(candidate)[0].data.numpy(), self.additional_distribution.unnormed_density(previous)[0].data.numpy(),\n",
    "#               self.target_density(previous)[0].data.numpy(), self.additional_distribution.unnormed_density(candidate)[0].data.numpy())\n",
    "        \n",
    "        if alpha > self.one:\n",
    "            self.particles = torch.cat([self.particles, candidate], dim=1)\n",
    "        else:\n",
    "            random = torch.bernoulli(alpha)\n",
    "            if random:\n",
    "                self.particles = torch.cat([self.particles, candidate], dim=1)                \n",
    "            else:\n",
    "                self.particles = torch.cat([self.particles, previous], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mh = MHAlgorithm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    mh.generate_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(mh.particles.t().data.cpu().numpy()[:,1], \n",
    "            kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(mh.particles.t().data.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import time\n",
    "\n",
    "mu,sig,N = 0,1,1000000\n",
    "pts = []\n",
    "\n",
    "def q(x):\n",
    "#     return (1/(math.sqrt(2*math.pi*sig**2)))*(math.e**(-((x-mu)**2)/(2*sig**2)))\n",
    "#     return NormalDensity(1, 0., 2., n_particles_second=True).unnormed_density(x)\n",
    "    return (0.3 * NormalDensity(1, -2., 1., n_particles_second=True).unnormed_density(x) +\n",
    "            0.7 * NormalDensity(1, 2., 1., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "def metropolis(N):\n",
    "    r = np.zeros(1)\n",
    "    p = q(r[0])\n",
    "    pts = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        rn = r + np.random.uniform(-1,1)\n",
    "        pn = q(rn[0])\n",
    "        if pn >= p:\n",
    "            p = pn\n",
    "            r = rn\n",
    "        else:\n",
    "            u = np.random.rand()\n",
    "            if u < pn/p:\n",
    "                p = pn\n",
    "                r = rn\n",
    "        pts.append(r)\n",
    "    \n",
    "    pts = np.array(pts)\n",
    "    return pts\n",
    "    \n",
    "def hist_plot(array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1,)\n",
    "    ax.hist(array, bins=1000)    \n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(metropolis(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gg =  lambda : (NormalDensity(1, -60., 20., n_particles_second=True).get_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot([gg()[0,0] for _ in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

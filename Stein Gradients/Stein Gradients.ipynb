{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orth\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "device = None\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    use_cuda = True\n",
    "\n",
    "# Ignore cuda\n",
    "use_cuda = False\n",
    "device = None\n",
    "\n",
    "# Set DoubleTensor as a base type for computations\n",
    "t_type = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import functools\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_diffs(x, y, n_particles_second=True):\n",
    "    '''\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: x is a dxN matrix\n",
    "                   y is an optional dxM matirx\n",
    "            Output: diffs is a dxNxM matrix where diffs[i,j] is the subtraction between x[:,i] and y[:,j]\n",
    "            i.e. diffs[i,j] = x[:,i] - y[:,j]\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: x is a Nxd matrix\n",
    "                   y is an optional Mxd matirx\n",
    "            Output: diffs is a NxMxd matrix where diffs[i,j] is the subtraction between x[i,:] and y[j,:]\n",
    "            i.e. diffs[i,j] = x[i,:]-y[j,:]\n",
    "    '''\n",
    "    if n_particles_second:\n",
    "        return x[:,:,np.newaxis] - y[:,np.newaxis,:]        \n",
    "    return x[:,np.newaxis,:] - y[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_dists(diffs=None, n_particles_second=True):\n",
    "    '''\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: diffs is a dxNxM matrix where diffs[i,j] = x[:,i] - y[:,j]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[:,i] - y[:,j]||\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: diffs is a NxMxd matrix where diffs[i,j] = x[i,:] - y[j, :]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[i,:]-y[j,:]||\n",
    "    '''\n",
    "    if n_particles_second:\n",
    "        return torch.norm(diffs, dim=0)\n",
    "    return torch.norm(diffs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     71,
     86,
     99,
     114,
     127
    ]
   },
   "outputs": [],
   "source": [
    "class normal_density():\n",
    "    '''\n",
    "        Multinomial normal density for independent random variables. \n",
    "    '''\n",
    "    def __init__(self, n=1, mu=0., std=1., n_particles_second=True):\n",
    "        '''\n",
    "        Args:\n",
    "            n (int): number of dimentions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            mu (float, array_like): mean of distribution\n",
    "                Default: 0.\n",
    "                if mu is float - use same mean across all dimentions\n",
    "                if mu is 1D array_like - use different mean for each dimention but same for each particles dimention\n",
    "                if mu is 2D array_like - use different mean for each dimention\n",
    "            std (float, array_like): std of distribution\n",
    "                Default: 1.\n",
    "                if std is float - use same std across all dimentions\n",
    "                if std is 1D array_like - use different std for each dimention but same for each particles dimention\n",
    "                if std is 2D array_like - use different std for each dimention\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applyed along particles axis\n",
    "                Input will be reduced along all axis exclude particle axis\n",
    "        '''\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        \n",
    "        if type(self.mu) == float:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.mu.shape) == 1:\n",
    "            if self.mu.shape[0] == 1:\n",
    "                self.mu = self.mu.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.mu.shape) == 2:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if type(self.std) == float:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.std.shape) == 1:\n",
    "            if len(self.std) == 1:\n",
    "                self.std = self.std.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.zero = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.pi = torch.tensor(math.pi, dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "    def __call__(self, x, n_axis=None):        \n",
    "        '''\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.pow(self.two * self.pi, -self.n / self.two) / \n",
    "                torch.prod(self.std, dim=n_axis) * \n",
    "                torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):      \n",
    "        '''\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std , self.two), dim=n_axis))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):  \n",
    "        '''\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (-self.n / self.two * torch.log(self.two * self.pi) + \n",
    "                torch.sum(torch.log(self.std), dim=n_axis) -\n",
    "                self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return -self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)\n",
    "    \n",
    "    def get_sample(self):\n",
    "        '''\n",
    "            Sample from normal distribution\n",
    "        '''\n",
    "        sample = torch.normal(self.mu, self.std)\n",
    "        if self.n_particles_second:\n",
    "            return sample.view(-1, 1)\n",
    "        else:\n",
    "            return sample.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     73,
     87,
     101,
     115,
     129,
     143
    ]
   },
   "outputs": [],
   "source": [
    "class gamma_density():\n",
    "    '''\n",
    "        Multinomial gamma density for independent random variables. \n",
    "    '''\n",
    "    def __init__(self, n=1, alpha=1, betta=1, n_particles_second=True):\n",
    "        '''\n",
    "        Args:\n",
    "            n (int): number of dimentions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            alpha (float, array_like): shape of distribution\n",
    "                Default: 1.\n",
    "                if alpha is float - use same shape across all dimentions\n",
    "                if alpha is 1D array_like - use different shape for each dimention but same for each particles dimention\n",
    "                if alpha is 2D array_like - use different shape for each dimention\n",
    "            betta (float, array_like): rate of distribution\n",
    "                Default: 1.\n",
    "                if betta is float - use same rate across all dimentions\n",
    "                if betta is 1D array_like - use different rate for each dimention but same for each particles dimention\n",
    "                if betta is 2D array_like - use different rate for each dimention\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applyed along particles axis  \n",
    "        '''\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.betta = betta\n",
    "        \n",
    "        if type(self.alpha) == float:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.alpha.shape) == 1:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.alpha.shape) == 2:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if type(self.betta) == float:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.betta.shape) == 1:\n",
    "            if len(self.betta) == 1:\n",
    "                self.betta = self.betta.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "        ### log Г(alpha)\n",
    "        self.lgamma = torch.lgamma(self.alpha)\n",
    "        ### Г(alpha)\n",
    "        self.gamma = torch.exp(self.lgamma)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(self.betta, self.alpha) / self.gamma * torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_density_log_x(self, log_x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density_log_x(self, log_x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log unnormed density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     51,
     59,
     71,
     83
    ]
   },
   "outputs": [],
   "source": [
    "class SteinLinear(nn.Module):\n",
    "    '''\n",
    "        Custom full connected layer for Stein Gradient Neural Networks\n",
    "        Transformation: y = xA + b\n",
    "        Parameters prior: p(w, a) = p(w|a)p(a) = П p(w_i|a_i)p(a_i)\n",
    "                          p(w_i|a_i) = N(w_i|0, a_i^(-1)); p(a_i) = G(1e-4, 1e-4)\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, n_particles=1, bias=True):\n",
    "        super(SteinLinear, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            in_features (int): size of each input sample\n",
    "            out_features (int): size of each output sample\n",
    "            n_particles (int): number of particles\n",
    "            bias (bool): If set to False, the layer will not learn an additive bias.\n",
    "                Default: ``True``\n",
    "        '''\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_particles = n_particles\n",
    "        \n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.zeros([in_features, out_features, n_particles], dtype=t_type, device=device))\n",
    "        self.log_weight_alpha = torch.nn.Parameter(torch.zeros([in_features * out_features, n_particles], dtype=t_type, device=device))\n",
    "        \n",
    "        self.bias = None\n",
    "        self.log_bias_alpha = None\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros([1, out_features, n_particles], dtype=t_type, device=device))\n",
    "            self.log_bias_alpha = torch.nn.Parameter(torch.zeros([out_features, n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        ### define prior on alpha p(a) = G(1e-4, 1e-4)\n",
    "        self.weight_alpha_log_prior = lambda x: (gamma_density(n=self.log_weight_alpha.shape[0],\n",
    "                                                               alpha=1e-4,\n",
    "                                                               betta=1e-4,\n",
    "                                                               n_particles_second=True\n",
    "                                                              ).log_unnormed_density_log_x(x))\n",
    "        self.bias_alpha_log_prior = lambda x: (gamma_density(n=self.log_bias_alpha.shape[0],\n",
    "                                                             alpha=1e-4,\n",
    "                                                             betta=1e-4,\n",
    "                                                             n_particles_second=True\n",
    "                                                            ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    ### useless function - all initialization defined in DistributionMover class\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.log_weight_alpha.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.log_bias_alpha.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "            Apply transformation: X_out[i, :, :] = X_in[i, :, :] * W + b[i]\n",
    "        Args:\n",
    "            X (torch.tensor): tensor \n",
    "        Shape:\n",
    "            Input: [n_particles, batch_size, in_features]\n",
    "            Output: [n_particles, batch_size, out_features]\n",
    "        '''\n",
    "        ### NEED SOME OPTIMIZATION TO OMMIT .permute\n",
    "        return torch.bmm(X, self.weight.permute(2, 0, 1)) + self.bias.permute(2, 0, 1)\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        '''\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        '''\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())\n",
    "        \n",
    "    def calc_log_prior(self):\n",
    "        '''\n",
    "            Evaluate log prior of trainable parameters\n",
    "        log p(w,a) = log p(w|a) + log p(a\n",
    "        '''\n",
    "        ### define prior on weight p(w|a) = N(0, 1 / alpha)\n",
    "        self.weight_log_prior = lambda x: (normal_density(n=self.weight.numel() // self.n_particles,\n",
    "                                                          mu=0.,\n",
    "                                                          std=self.one / torch.exp(self.log_weight_alpha),\n",
    "                                                          n_particles_second=True\n",
    "                                                         ).log_unnormed_density(x))\n",
    "        \n",
    "        self.bias_log_prior = lambda x: (normal_density(self.bias.numel() // self.n_particles,\n",
    "                                                        mu=0.,\n",
    "                                                        std=self.one / torch.exp(self.log_bias_alpha),\n",
    "                                                        n_particles_second=True\n",
    "                                                       ).log_unnormed_density(x))\n",
    "        \n",
    "        return (self.weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha) +\n",
    "                self.bias_log_prior(self.bias.view(-1, self.n_particles)) + self.bias_alpha_log_prior(self.log_bias_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     44,
     77
    ]
   },
   "outputs": [],
   "source": [
    "class LinearTransform():\n",
    "    '''\n",
    "        Class for various linear transformations\n",
    "    '''\n",
    "    def __init__(self, n_dims, n_hidden_dims, use_identity=False, normalize=False):\n",
    "        '''\n",
    "        Args:\n",
    "            n_dims (int): dimension of the space\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_identity (bool): If set to True, use 'eye' matrix for transformations\n",
    "            normalize (bool): If set to True, columns of the transformation matrix is an orthonormal basis \n",
    "        '''\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_identity = use_identity\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if self.use_identity:\n",
    "            return\n",
    "        \n",
    "        self.A = torch.zeros([self.n_dims, self.n_hidden_dims], dtype=t_type, device=device)\n",
    "        self.tetta_0 = torch.zeros([self.n_dims, 1], dtype=t_type, device=device)\n",
    "        if not self.use_identity: \n",
    "            self.A.uniform_(-1., 1.)\n",
    "            self.tetta_0.uniform_(-1.,1.)\n",
    "        else:\n",
    "            if self.n_dims != self.n_hidden_dims:\n",
    "                raise RuntimeError(\"Cannot use identity transformation in spaces with differrent dimensions\")\n",
    "            self.A = torch.eye(self.n_dims)\n",
    "            \n",
    "        ### normalize columns of matrix A\n",
    "        if self.normalize:\n",
    "            self.A = torch.tensor(orth(self.A.data.numpy()), dtype=t_type, device=device)\n",
    "        #self.A /= self.A.norm(dim=0)\n",
    "        \n",
    "        ### A^(t)A\n",
    "        self.AtA = torch.matmul(self.A.t(), self.A)\n",
    "        ### (A^(t)A)^(-1)\n",
    "        self.AtA_1 = torch.inverse(self.AtA)\n",
    "        ### (A^(t)A)^(-1)A^(t)\n",
    "        self.inverse_base = torch.matmul(self.AtA_1, self.A.t())\n",
    "#         ### A(A^(t)A)^(-1)A^(t)\n",
    "#         self.projector_base = torch.matmul(self.A, self.inverse_base)\n",
    "        \n",
    "    def transform(self, tetta, n_particles_second=True):\n",
    "        '''\n",
    "            Transform tettas as follows: \n",
    "                tetta = Atetta` + tetta_0\n",
    "        '''\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.A, tetta) + self.tetta_0\n",
    "        return (torch.matmul(self.A, tetta.t()) + self.tetta_0).t()\n",
    "    \n",
    "    def inverse_transform(self, tetta, n_particles_second=True):\n",
    "        '''\n",
    "            Apply inverse transformation: \n",
    "                tetta` = (A^(t)A)^(-1)A^(t)(tetta - tetta_0)\n",
    "        '''\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.inverse_base, tetta - self.tetta_0)\n",
    "        return torch.matmul(self.inverse_base, tetta.t() - self.tetta_0).t()\n",
    "        \n",
    "    def project(self, tetta, n_particles_second=True):\n",
    "        '''\n",
    "            Project tettas onto Linear Space X = {Atetta` + tetta_0 for all tetta` in R^d}:\n",
    "                tetta_projected = A(A^(t)A)^(-1)A^(t)(tetta - tetta_0) + tetta_0\n",
    "        '''\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.projector_base, tetta - self.tetta_0) + self.tetta_0\n",
    "        return (torch.matmul(self.projector_base, tetta.t() - self.tetta_0) + self.tetta_0).t()\n",
    "    \n",
    "    def project_inverse(self, tetta, n_particles_second=True):\n",
    "        '''\n",
    "            Project and then apply inverse transform to tetta - tetta_0:\n",
    "                tetta_s_p_i = T^(-1)P(tetta - tetta_0)= (A^(t)A)^(-1)A^(t)tetta\n",
    "        '''\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        ### use solver trick: tetta_s_p_i : A^(t)Atetta_s_p_i = A^(t)tetta\n",
    "        if n_particles_second:\n",
    "            return torch.gesv(torch.matmul(self.A.t(), tetta), self.AtA)[0]\n",
    "        return torch.gesv(torch.matmul(self.A.t(), tetta.t()), self.AtA)[0].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     4,
     54,
     57
    ]
   },
   "outputs": [],
   "source": [
    "class RegressionDistribution(nn.Module):\n",
    "    '''\n",
    "        Distribution over data for regression task: p(D|w) = N(y_predicted|y, )\n",
    "    '''\n",
    "    def __init__(self, n_particles):\n",
    "        super(RegressionDistribution, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        '''\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        \n",
    "        ### define betta - variance of data distribution for regression tasks (betta = 1 / std ** 2)\n",
    "        self.log_betta = torch.nn.Parameter(torch.zeros([1, self.n_particles], dtype=t_type, device=device))\n",
    "        \n",
    "        ### define prior on betta p(betta)\n",
    "        self.betta_log_prior = lambda x: (gamma_density(n=1,\n",
    "                                                        alpha=1e-4,\n",
    "                                                        betta=1e-4,\n",
    "                                                        n_particles_second=True\n",
    "                                                       ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        ### Support tensors for computations\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "    \n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        '''\n",
    "            Evaluate log p(tetta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predicted values\n",
    "            train_size (int) - size of training dataset\n",
    "        Shapes:\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, 1]\n",
    "        '''\n",
    "        ### squeeze last axis because regression task is being solved\n",
    "        y_predict.squeeze_(2)\n",
    "        \n",
    "        batch_size = torch.tensor(y.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        self.log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                        mu=y,\n",
    "                                                        std=self.one / torch.sqrt(torch.exp(self.log_betta.expand(X.shape[0], self.n_particles).t())),\n",
    "                                                        n_particles_second=False\n",
    "                                                       ).log_unnormed_density(x))\n",
    "\n",
    "        return train_size / batch_size * self.log_data_distr(y_predict) + self.betta_log_prior(self.log_betta)\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        '''\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        '''\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     1,
     10,
     35,
     38
    ]
   },
   "outputs": [],
   "source": [
    "class ClassificationDistribution(nn.Module):\n",
    "    def __init__(self, n_particles):\n",
    "        super(ClassificationDistribution, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        '''\n",
    "        self.n_particles = n_particles\n",
    "    \n",
    "\n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        '''\n",
    "            Evaluate log p(tetta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predictions (array_like): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shapes: \n",
    "            X.shape = [batch_size, in_features]\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, n_classes]\n",
    "        '''\n",
    "        batch_size = torch.tensor(X.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        \n",
    "        probas = nn.LogSoftmax(dim=2)(y_predict)\n",
    "        probas_selected = torch.gather(input=probas, dim=2, index=y.view(1, -1, 1).expand(probas.shape[0], probas.shape[1], 1)).squeeze(2)\n",
    "        log_data = torch.sum(probas_selected, dim=1)\n",
    "        \n",
    "        return train_size / batch_size  * log_data\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        '''\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        '''\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     1,
     127,
     272,
     304,
     333,
     354,
     391,
     434,
     440,
     447,
     467,
     485,
     506,
     530,
     553,
     580,
     591,
     598
    ]
   },
   "outputs": [],
   "source": [
    "class DistributionMover():\n",
    "    def __init__(self,\n",
    "                 task='app', \n",
    "                 n_particles=None,\n",
    "                 particles=None,\n",
    "                 target_density=None,\n",
    "                 n_dims=None,\n",
    "                 n_hidden_dims=None,\n",
    "                 use_latent=False,\n",
    "                 net=None,\n",
    "                 precomputed_params=None,\n",
    "                 data_distribution=None):\n",
    "        '''\n",
    "        Args:\n",
    "            task (str):\n",
    "                'app' | 'reg' | 'net_reg' | 'net_class'\n",
    "                - approximate target distribution\n",
    "                - @deprecated solve regression task inplace\n",
    "                - solve regression task using net\n",
    "                - solve classification task using net\n",
    "            n_particles (int): number of particles\n",
    "            particles (2D array_like): array which contains initialized particles\n",
    "            target_density (callable): computes probability density function of target distribution (only for 'app' task)\n",
    "            n_dims (int): dimension of the space where optimization is performed\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_latent (bool): If set to True, Subspace Stein is used\n",
    "            net (nn.Sequential): object which is used to make predictions (for 'net_reg' and' net_class' tasks)\n",
    "            precomputed_params (1D array_like): Precomputed parameters, which will be used for particles initialization\n",
    "            data_distribution (callable): computes probability over data p(D|w) (for 'net_reg' and' net_class' tasks)\n",
    "        '''\n",
    "        \n",
    "        self.task = task\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.particles = particles\n",
    "        self.target_density = target_density\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_latent = use_latent\n",
    "        self.net = net\n",
    "        self.precomputed_params = precomputed_params\n",
    "        self.data_distribution = data_distribution\n",
    "        \n",
    "\n",
    "        if self.task == 'net_reg' or self.task == 'net_class':\n",
    "            self.n_dims = 0\n",
    "            for module in self.modules_net():\n",
    "                if \"numel\" in dir(module):\n",
    "                    self.n_dims += module.numel() // self.n_particles\n",
    "\n",
    "        if not self.use_latent:\n",
    "            self.n_hidden_dims = self.n_dims\n",
    "\n",
    "        ### Learnable samples from the target distribution\n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_hidden_dims, self.n_particles],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., 2.)\n",
    "\n",
    "        ### Class for performing linear transformations\n",
    "        self.lt = None\n",
    "        if self.use_latent:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=False, \n",
    "                normalize=True\n",
    "            )\n",
    "        else:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=True, \n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "        if self.precomputed_params is not None:\n",
    "            self.particles = self.lt.inverse_transform(self.precomputed_params.unsqueeze(1).expand(self.n_dims, self.n_particles))\n",
    "\n",
    "        ### Functions of probability density of target distribution\n",
    "        self.target_density = None\n",
    "        self.real_target_density = None\n",
    "        if self.net is None:\n",
    "            # use unnormed probability density to speedup computations\n",
    "            if target_density is not None:\n",
    "                self.target_density = target_density\n",
    "                self.real_target_density = target_density\n",
    "            else:\n",
    "                self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                                   0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "                self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                        0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "\n",
    "                # self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "                # self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "        ### Number of iterations since begining\n",
    "        self.iter = 0\n",
    "\n",
    "        ### Adagrad parameters\n",
    "        self.fudge_factor = torch.tensor(1e-6, dtype=t_type, device=device)\n",
    "        self.step_size = torch.tensor(1e-2, dtype=t_type, device=device)\n",
    "        self.auto_corr = torch.tensor(0.9, dtype=t_type, device=device)\n",
    "\n",
    "        ### Gradient history term for adagrad optimization\n",
    "        self.historical_grad = None\n",
    "        if self.use_latent:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_hidden_dims, n_particles], dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_dims, n_particles], dtype=t_type, device=device)\n",
    "\n",
    "        ### Factor from kernel\n",
    "        self.med = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.h = torch.tensor(0., dtype=t_type, device=device)\n",
    "\n",
    "        ### Support tensors for computations\n",
    "        self.N = torch.tensor(self.n_particles, dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.three = torch.tensor(3., dtype=t_type, device=device)\n",
    "\n",
    "    @deprecated # use calc_kernel_term_latent / calc_kernel_term_latent_net\n",
    "    def calc_kernel_term(self, h_type):\n",
    "        '''\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4:\n",
    "                    - med(dist(tetta-tetta`)^2) / logN\n",
    "                    - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    - var(tetta) / logN * 2 * n_dims\n",
    "                    - var(diff(tetta-tetta`) / logN * n_dims\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        '''\n",
    "        ### diffs[i, j] = tetta_i - tetta_j\n",
    "        diffs = pairwise_diffs(self.particles, self.particles, n_particles_second=True)\n",
    "        ### dists[i, j] = ||tetta_i - tetta_j||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        ### sq_dists[i, j] = ||tetta_i - tetta_j||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "\n",
    "        ### RBF Kernel\n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 2:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            self.var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            self.var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1) * (self.n_dims)\n",
    "\n",
    "        kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "\n",
    "        grad_kernel = -self.two / self.h * kernel.unsqueeze(dim=0) * diffs\n",
    "\n",
    "        return kernel, grad_kernel\n",
    "\n",
    "    def calc_kernel_term_latent(self, h_type, kernel_type='rbf', p=None):\n",
    "        '''\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(tetta-tetta`)^2) / logN\n",
    "                    1 - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    3 - var(tetta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(tetta-tetta`) / logN * n_dims\n",
    "                    5 - med(dist(tetta-tetta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(tetta-tetta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atetta`_i + tetta_0, Atetta`_j + tetta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        '''\n",
    "        ### power for rational kernel\n",
    "        self.p = torch.tensor(p, dtype=t_type, device=device) if p is not None else None\n",
    "        \n",
    "        ### tetta = Atetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### diffs[i, j] = A(tetta`_i - tetta`_j)\n",
    "        diffs = pairwise_diffs(real_particles, real_particles, n_particles_second=True)\n",
    "        ### dists[i, j] = ||A(tetta`_i - tetta`_j)||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        ### sq_dists[i, j] = ||A(tetta`_i - tetta`_j)||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "\n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 2:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            self.var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            self.var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 5:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  self.med / (torch.pow(self.N, self.two) - self.one)\n",
    "        elif h_type == 6:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  self.med / (torch.pow(self.N, -self.one / self.p) - self.one)\n",
    "        elif h_type == 7:\n",
    "            self.med = torch.median(torch.matmul(real_particles.t(), real_particles)) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        \n",
    "        kernel = None\n",
    "        grad_kernel = None\n",
    "        if kernel_type == 'rbf':\n",
    "            ### RBF Kernel:\n",
    "            ### kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "            kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "            ### grad_kernel[i, j] = -2/h * A(tetta`_i - tetta`_j) * kernel[i, j]\n",
    "            grad_kernel = -self.two / self.h * kernel.unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'imq':\n",
    "            ### IMQ Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, -self.one / self.two)\n",
    "            ### grad_kernel[i, j] = -1/h * A(tetta`_i - tetta`_j) * kernel^(3)[i, j]\n",
    "            grad_kernel = -self.one / self.h * torch.pow(kernel, self.three).unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'exp':\n",
    "            ### Exponential Kernel:\n",
    "            ### kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "            kernel = torch.exp(self.one / self.h * torch.matmul(real_particles.t(), real_particles))\n",
    "            ### grad_kernel[i, j] = 1/h * (Atetta`_j + tetta_0) * kernel[i, j]\n",
    "            grad_kernel = 1. / self.h * kernel.unsqueeze(0) * real_particles.unsqueeze(1)\n",
    "        elif kernel_type == 'rat':\n",
    "            ### RAT Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, self.p)\n",
    "            ### grad_kernel[i, j] = p/h * A(tetta`_i - tetta`_j) * kernel^((p - 1)/p)[i, j]\n",
    "            grad_kernel = self.p / self.h * torch.pow(kernel, (self.p - self.one) / self.p).unsqueeze(0) * diffs\n",
    "            \n",
    "        return kernel, grad_kernel\n",
    "    \n",
    "    def calc_kernel_term_latent_net(self, h_type, kernel_type='rbf', p=None):\n",
    "        '''\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(tetta-tetta`)^2) / logN\n",
    "                    1 - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    3 - var(tetta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(tetta-tetta`) / logN * n_dims\n",
    "                    5 - med(dist(tetta-tetta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(tetta-tetta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atetta`_i + tetta_0, Atetta`_j + tetta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        '''\n",
    "        return self.calc_kernel_term_latent(h_type, p)\n",
    "\n",
    "    @deprecated # use calc_log_term_latent / calc_log_term_latent_regression / calc_log_term_latent_net\n",
    "    def calc_log_term(self):\n",
    "        '''\n",
    "            Calculate grad(log p(tetta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        '''\n",
    "        grad_log_term = torch.zeros(\n",
    "            [self.n_dims, self.n_particles], dtype=t_type, device=device)\n",
    "\n",
    "        for idx in range(self.n_particles):\n",
    "            ### tetta_i\n",
    "            particle = torch.tensor(\n",
    "                self.particles[:, idx:idx+1],\n",
    "                dtype=t_type,\n",
    "                requires_grad=True,\n",
    "                device=device)\n",
    "            ### log_term_i = log p(tetta_i)\n",
    "            log_term = torch.log(self.target_density(particle))\n",
    "            ### grad_log_term_i = dlog_term_i / dtetta_i\n",
    "            grad_log_term[:, idx:idx+1] = torch.autograd.grad(\n",
    "                log_term,\n",
    "                particle,\n",
    "                only_inputs=True,\n",
    "                retain_graph=False,\n",
    "                create_graph=False,\n",
    "                allow_unused=False)[0]\n",
    "\n",
    "        return grad_log_term\n",
    "    \n",
    "    def calc_log_term_latent(self):\n",
    "        '''\n",
    "            Calculate grad(log p(tetta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        '''\n",
    "        \n",
    "        ### tetta = A tetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_term = torch.log(self.target_density(real_particles))\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "    \n",
    "        grad_log_term = real_particles.grad\n",
    "        \n",
    "        return grad_log_term\n",
    "    \n",
    "    @deprecated\n",
    "    def calc_log_term_latent_regression(self, X, y, train_size):\n",
    "        '''\n",
    "            Calculate grad(log p(tetta)) \n",
    "        Args:\n",
    "            X (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input: \n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output: \n",
    "                [n_dims, n_particles]\n",
    "        '''\n",
    "        self.log_prior_distr = lambda x: (normal_density(self.n_dims, 0., 0.01, n_particles_second=True).log_unnormed_density(x))\n",
    "        self.log_data_distr = lambda x: (normal_density(X.shape[0], y, 1e-5, n_particles_second=True).log_unnormed_density(x))\n",
    "\n",
    "        batch_size = torch.tensor(X.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        grad_log_term = torch.zeros([self.n_dims, self.n_particles], dtype=t_type, device=device)\n",
    "        \n",
    "        ### tetta = A tetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        ### get prediction for batch of data\n",
    "        predict_y = self.predict_regression(X, real_particles)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_data = self.log_data_distr(predict_y)\n",
    "        log_prior = self.log_prior_distr(real_particles)\n",
    "        log_term = (log_prior + train_size / batch_size * log_data)\n",
    "        \n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "    \n",
    "        grad_log_term = real_particles.grad\n",
    "        \n",
    "        return grad_log_term\n",
    "    \n",
    "    def calc_log_term_latent_net(self, X, y, train_size):\n",
    "        '''\n",
    "            Calculate grad(log p(tetta)) \n",
    "        Args:\n",
    "            X (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input: \n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output: \n",
    "                [n_dims, n_particles]\n",
    "        '''\n",
    "        self.log_data = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        self.log_prior = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        \n",
    "        ### get real net parameters: tetta_i = A tetta`_i + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### init net with real parameters\n",
    "        self.vector_to_parameters(real_particles.view(-1), self.paramerets_net())\n",
    "        ### compute log prior of all weight in the net\n",
    "        for module in self.modules_net():\n",
    "            if \"calc_log_prior\" in dir(module):\n",
    "                self.log_prior += module.calc_log_prior()\n",
    "        \n",
    "        ### get prediction for the batch of data\n",
    "        y_predict = self.predict_net(X)\n",
    "        ### compute log data term log p(D|w)\n",
    "        self.log_data = self.data_distribution.calc_log_data(X, y, y_predict, train_size)\n",
    "        \n",
    "        ### log_term = log p(tetta) = log p_prior(tetta) + log p_data(D|tetta)\n",
    "        log_term = self.log_prior + self.log_data\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "        \n",
    "        ### collect all gradients into one vector\n",
    "        grad_log_term = self.parameters_grad_to_vector(self.paramerets_net()).view(-1, self.n_particles)\n",
    "            \n",
    "        return grad_log_term\n",
    "    \n",
    "    def paramerets_net(self):\n",
    "        '''\n",
    "            Return all trainable parameters\n",
    "        '''\n",
    "        return chain(self.net.parameters(), self.data_distribution.parameters())\n",
    "    \n",
    "    def modules_net(self):\n",
    "        '''\n",
    "            Return all modules\n",
    "        '''\n",
    "        return chain(self.net.modules(), self.data_distribution.modules())\n",
    "        \n",
    "    @deprecated # use one layer net for linear regression task \n",
    "    def predict_regression(self, X, weight=None):\n",
    "        '''\n",
    "            Predict values for regression task\n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            weight (array_like): \n",
    "                If None - average predictions across all particles\n",
    "                If not None - use weight to make prediction\n",
    "        Shapes:\n",
    "            X.shape = [batch_size, in_features]\n",
    "            weight.shape = [in_features, 1]\n",
    "        '''\n",
    "        X_1 = torch.cat(\n",
    "            (X, torch.tensor(1., dtype=t_type, device=device).expand(X.shape[0]).view(-1, 1)),dim=1)\n",
    "        if weight is not None:\n",
    "            return torch.matmul(X_1, weight)\n",
    "        else:\n",
    "            weight = self.lt.transform(self.particles, n_particles_second=True)\n",
    "            return torch.mean(torch.matmul(X_1, weight), dim=1)\n",
    "\n",
    "    def predict_net(self, X, inference=False):\n",
    "        '''\n",
    "            Use net to make predictions        \n",
    "            Args:\n",
    "                X (array_like): batch of data\n",
    "        '''\n",
    "        if self.task == 'net_reg':\n",
    "            if inference:\n",
    "                return torch.mean(self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape)), dim=0)\n",
    "            else:\n",
    "                return self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape))\n",
    "        elif self.task == 'net_class':\n",
    "            if inference:\n",
    "                return torch.mean(self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape)), dim=0)\n",
    "            else:\n",
    "                return self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape))\n",
    "\n",
    "    @deprecated # use update_latent / update_latent_regression / update_latent_net\n",
    "    def update(self, h_type, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term(h_type)\n",
    "        grad_log_term = self.calc_log_term()\n",
    "\n",
    "        ### Compute value of step in functiuonal space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "    def update_latent(self, h_type, kernel_type='rbf', p=None, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent()\n",
    "        ### Compute value of step in functiuonal space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "        \n",
    "    @deprecated # use one layer net for linear regression task \n",
    "    def update_latent_regression(self, h_type, kernel_type='rbf', p=None, X_batch=None, y_batch=None, train_size=None, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_regression(X_batch, y_batch, train_size)\n",
    "        ### Compute value of step in functiuonal space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "        \n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "    def update_latent_net(self, h_type, kernel_type='rbf', p=None, X_batch=None, y_batch=None, train_size=None, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "        self.net.zero_grad()\n",
    "        self.data_distribution.zero_grad()\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent_net(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_net(X_batch, y_batch, train_size)\n",
    "\n",
    "        ### Compute value of step in functiuonal space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_to_parameters(vec, parameters):\n",
    "        pointer = 0\n",
    "        for param in parameters:\n",
    "            # The length of the parameter\n",
    "            num_param = param.numel()\n",
    "            # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "            param.data = vec[pointer:pointer + num_param].view_as(param).data\n",
    "            # Increment the pointer\n",
    "            pointer += num_param\n",
    "            \n",
    "    @staticmethod \n",
    "    def parameters_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    @staticmethod \n",
    "    def parameters_grad_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.grad.view(-1))\n",
    "        return torch.cat(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.8159, dtype=torch.float64) tensor(20.4882, dtype=torch.float64)\n",
      "tensor(88.1742, dtype=torch.float64) tensor(82.8522, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "### Bostor housing dataset for regression task\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "X_train, X_test, y_train, y_test = (torch.tensor(X_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(X_test, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_test, dtype=t_type, device=device))\n",
    "\n",
    "### Linear Regression baseline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(torch.nn.MSELoss()(torch.tensor(model.predict(X_test), dtype=t_type, device=device), y_test), \n",
    "      torch.nn.MSELoss()(torch.tensor(model.predict(X_train), dtype=t_type, device=device), y_train))\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.mean(y_train).expand(y_test.shape[0]), y_test), \n",
    "      torch.nn.MSELoss()(torch.mean(y_train).expand(y_train.shape[0]), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function calc_log_term.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated function calc_kernel_term.\n",
      "  \"\"\"\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated function update.\n",
      "  \n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:446: DeprecationWarning: Call to deprecated function calc_kernel_term.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:447: DeprecationWarning: Call to deprecated function calc_log_term.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated function calc_log_term_latent_regression.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:346: DeprecationWarning: Call to deprecated function predict_regression.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated function update_latent_regression.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:492: DeprecationWarning: Call to deprecated function calc_log_term_latent_regression.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:346: DeprecationWarning: Call to deprecated function predict_regression.\n"
     ]
    }
   ],
   "source": [
    "### Check all functions\n",
    "\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=13, n_particles=10, use_latent=False)\n",
    "dm.calc_log_term()\n",
    "dm.calc_kernel_term(0)\n",
    "dm.update(0)\n",
    "\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=5, n_particles=10, use_latent=True)\n",
    "dm.calc_log_term_latent()\n",
    "dm.calc_kernel_term_latent(0)\n",
    "dm.update_latent(0)\n",
    "\n",
    "ss = nn.Sequential(SteinLinear(13, 1, 10))#, nn.ReLU(), SteinLinear(5, 1, 10))\n",
    "dd = RegressionDistribution(n_particles=10)\n",
    "dm = DistributionMover(task='net_reg', n_hidden_dims=5, n_particles=10, use_latent=True, net=ss, data_distribution=dd)\n",
    "dm.calc_log_term_latent_net(X_train, y_train, X_train.shape[0])\n",
    "# dm.calc_kernel_term_latent_net(0)\n",
    "# dm.update_latent_net(0, X_train, y_train, X_train.shape[0])\n",
    "\n",
    "dm = DistributionMover(task='reg', n_particles=10, n_dims=X.shape[1] + 1, n_hidden_dims=5, use_latent=True)\n",
    "dm.calc_log_term_latent_regression(X_train, y_train, X_train.shape[0])\n",
    "dm.update_latent_regression(0, 'rbf', X_train, y_train, X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#net = nn.Sequential(SteinLinear(13, 5, 1), nn.Tanh(), SteinLinear(5, 1, 1))\n",
    "net = nn.Sequential(SteinLinear(13, 1, 1))\n",
    "data_distr = RegressionDistribution(1)\n",
    "dm = DistributionMover(task='net_reg', n_particles=1, use_latent=False, net=net, data_distribution=data_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: wrong matrix size, batch1: 32x784, batch2: 13x1 at /opt/conda/conda-bld/pytorch_1532502421238/work/aten/src/TH/generic/THTensorMath.cpp:2312",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-05ac72060ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_latent_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6851fd30943a>\u001b[0m in \u001b[0;36mupdate_latent_net\u001b[0;34m(self, h_type, kernel_type, X_batch, y_batch, train_size, step_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m### Compute additional terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_kernel_term_latent_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mgrad_log_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_log_term_latent_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m### Compute value of step in functiuonal space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6851fd30943a>\u001b[0m in \u001b[0;36mcalc_log_term_latent_net\u001b[0;34m(self, X, y, train_size)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m### get prediction for the batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;31m### compute log data term log p(D|w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_log_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6851fd30943a>\u001b[0m in \u001b[0;36mpredict_net\u001b[0;34m(self, X, inference)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'net_class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f9994610c886>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     68\u001b[0m         '''\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m### NEED SOME OPTIMIZATION TO OMMIT .permute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: wrong matrix size, batch1: 32x784, batch2: 13x1 at /opt/conda/conda-bld/pytorch_1532502421238/work/aten/src/TH/generic/THTensorMath.cpp:2312"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step_size = 0.02\n",
    "    for _ in range(100000):\n",
    "        dm.update_latent_net(h_type=1, X_batch=X_train, y_batch=y_train, train_size=X_train.shape[0], step_size=step_size)\n",
    "        train_loss = torch.nn.MSELoss()(dm.predict_net(X_train, inference=True).view(-1), y_train)\n",
    "        test_loss = torch.nn.MSELoss()(dm.predict_net(X_test, inference=True).view(-1), y_test)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train): {1:.3f}\\t Empirical Loss(Test): {2:.3f}\\t Kernel factor: {3:.3f}'.format(\n",
    "                            _, train_loss, test_loss, dm.h))\n",
    "        if _ % 1200 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import MNIST dataset with class selection\n",
    "\n",
    "sys.path.insert(0, '/home/m.nakhodnov/Samsung-Tasks/Datasets/MyMNIST')\n",
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform)\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform)\n",
    "\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=32, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### MNIST with Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### MAP estimate for MNIST classification task\n",
    "net_map = nn.Sequential(SteinLinear(28 * 28, 200, 10), nn.ReLU(), SteinLinear(200, 200, 10), nn.ReLU(), SteinLinear(200, 10, 10))\n",
    "data_distr_map = ClassificationDistribution(10)\n",
    "dm_map = DistributionMover(task='net_class', n_particles=10, use_latent=False, net=net_map, data_distribution=data_distr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29... Empirical Loss(Train/Test): 9.072/17.357\t Accuracy(Train/Test): 0.971/0.960\t Kernel factor: 2158.1533"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step_size = 0.01\n",
    "    for epoch in range(30):\n",
    "        for train_epoch in range(30):\n",
    "            X, y = next(iter(dataloader_m_train))\n",
    "            X = X.double().view(X.shape[0], -1)\n",
    "            dm_map.update_latent_net(h_type=0, X_batch=X, y_batch=y, train_size=dataloader_m_train.__len__(), step_size=step_size)\n",
    "        \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(dm_map.predict_net(X_train, inference=True)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(dm_map.predict_net(X_train, inference=True), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 15.\n",
    "        train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(dm_map.predict_net(X_test, inference=True)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(dm_map.predict_net(X_test, inference=True), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 15.\n",
    "        test_acc /= (15. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            epoch, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "        if (epoch * 30) % 600 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(28 * 28, 200, 10), nn.ReLU(), SteinLinear(200, 200, 10), nn.ReLU(), SteinLinear(200, 10, 10))\n",
    "data_distr = ClassificationDistribution(10)\n",
    "dm = DistributionMover(task='net_class',\n",
    "                       n_particles=10,\n",
    "                       n_hidden_dims=2000,\n",
    "                       use_latent=True,\n",
    "                       net=net,\n",
    "                       precomputed_params=dm.parameters_to_vector(dm_map.paramerets_net()),\n",
    "                       data_distribution=data_distr\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24... Empirical Loss(Train/Test): 2.340/2.368\t Accuracy(Train/Test): 0.067/0.038\t Kernel factor: 0.000"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step_size = 0.01\n",
    "    for epoch in range(30):\n",
    "        for train_epoch in range(30):\n",
    "            X, y = next(iter(dataloader_m_train))\n",
    "            X = X.double().view(X.shape[0], -1)\n",
    "            dm.update_latent_net(h_type=0, X_batch=X, y_batch=y, train_size=dataloader_m_train.__len__(), step_size=step_size)\n",
    "        \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(dm.predict_net(X_train, inference=True)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(dm.predict_net(X_train, inference=True), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 15.\n",
    "        train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(dm.predict_net(X_test, inference=True)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(dm.predict_net(X_test, inference=True), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 15.\n",
    "        test_acc /= (15. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            epoch, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "        if (epoch * 30) % 600 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X.shape[0], -1)\n",
    "y_pred = dm.predict_net(X_test, inference=True)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_projections(dm, use_real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MNIST with FC neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_nn = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ").double()\n",
    "optim_nn = torch.optim.Adam(net_nn.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for _, (X, y) in enumerate(dataloader_m_train):\n",
    "        X = X.double().view(X.shape[0], -1)\n",
    "        \n",
    "        optim_nn.zero_grad()\n",
    "        train_loss = nn.CrossEntropyLoss()(net_nn(X), y)\n",
    "        train_loss.backward()\n",
    "        optim_nn.step()\n",
    "   \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_train)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(net_nn(X_train), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 30.\n",
    "        train_acc /= (30. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_test)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(net_nn(X_test), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 30.\n",
    "        test_acc /= (30. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            _, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X.shape[0], -1)\n",
    "y_pred = net_nn(X_test)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DistributionMover(n_particles=100, n_dims=2, n_hidden_dims=2, use_latent=False)\n",
    "\n",
    "#marginal_density = lambda x : (normal_density(mu=0., std=2., n=1)(x))\n",
    "marginal_density = lambda x : (0.3 * normal_density(mu=-2., std=1., n=1)(x) + 0.7 * normal_density(mu=2., std=1., n=1)(x))\n",
    "\n",
    "pdf = []\n",
    "for _ in np.linspace(-10, 10, 1000): \n",
    "    _ = torch.tensor(_, dtype=t_type, device=device)\n",
    "    pdf.append(marginal_density(_))\n",
    "    \n",
    "from pynverse import inversefunc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "cum_density = interp1d(np.linspace(-20, 20, 25), [quad(marginal_density, -20, x)[0] for x in np.linspace(-20, 20, 25)])\n",
    "inv_cum_density = inversefunc(cum_density)\n",
    "real_samples = [inv_cum_density(np.random.random()) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = quad(lambda x : marginal_density(x) * x, -20, 20)[0]\n",
    "var = quad(lambda x : marginal_density(x) * (x - mean) ** 2, -20, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7999999999999998, 4.359999999999999)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcedc6dc828>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(-10, 10, 1000), pdf)\n",
    "plt.plot(real_samples, np.zeros_like(real_samples))\n",
    "sns.kdeplot(real_samples, kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot_projections(dm=None, use_real=True, kernel='tri'):\n",
    "    N_plots = None\n",
    "    scale_factor = None\n",
    "    \n",
    "    if use_real:\n",
    "        if not dm.use_latent:\n",
    "            return\n",
    "        N_plots = dm.lt.A.shape[0]\n",
    "    else:\n",
    "        N_plots = dm.particles.shape[0]\n",
    "    if N_plots > 6:\n",
    "        scale_factor = 15\n",
    "    else:\n",
    "        scale_factor = 5\n",
    "        \n",
    "    N_plots = min(N_plots, 1)\n",
    "        \n",
    "    plt.figure(figsize=(3 * scale_factor, (N_plots // 3 + 1) * scale_factor))\n",
    "    \n",
    "    for idx in range(N_plots):\n",
    "        slice_dim = idx\n",
    "        \n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        \n",
    "        particles = None\n",
    "        if use_real:\n",
    "            particles = dm.lt.transform(dm.particles, n_particles_second=True).t()[:, slice_dim]\n",
    "        else:\n",
    "            particles = dm.particles.t()[:, slice_dim]\n",
    "        \n",
    "        plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "        plt.plot(particles.data.cpu().numpy(), torch.zeros_like(particles).data.cpu().numpy(), 'ro')\n",
    "        sns.kdeplot(particles.data.cpu().numpy(), \n",
    "                    kernel=kernel, color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projections(dm, use_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3240x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_projections(dm, use_real=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DistributionMover(n_particles=1000, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_projections(dm, use_real=True, kernel='tri')\n",
    "            plot_projections(dm, use_real=False, kernel='tri')\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.mean(dm.lt.transform(dm.particles, n_particles_second=True)),\n",
    " torch.mean(torch.std(dm.lt.transform(dm.particles, n_particles_second=True), dim=1) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare implementations of Stein Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class SVGD():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def svgd_kernel(self, theta, h = -1):\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = h / np.log(theta.shape[0]+1)\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = 2. * dxkxy / (h)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    " \n",
    "    def update(self, x0, lnprob, n_iter = 1000, stepsize = 1e-3, bandwidth = -1, alpha = 0.9, debug = False):\n",
    "        # Check input\n",
    "        if x0 is None or lnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        theta = np.copy(x0) \n",
    "        \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter+1) % 1000 == 0:\n",
    "                print( 'iter ' + str(iter+1) )\n",
    "            \n",
    "            lnpgrad = lnprob(theta)\n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(theta, h = bandwidth)  \n",
    "            grad_theta = (np.matmul(kxy, lnpgrad) + dxkxy) / x0.shape[0]  \n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + grad_theta ** 2\n",
    "            else:\n",
    "                historical_grad = alpha * historical_grad + (1 - alpha) * (grad_theta ** 2)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            theta = theta + stepsize * adj_grad \n",
    "            \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_ex = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lnprob(particles):\n",
    "    grad_log_term = torch.zeros([particles.shape[0], particles.shape[1]], dtype=t_type)\n",
    "    if use_cuda:\n",
    "        grad_log_term = grad_log_term.cuda()\n",
    "        \n",
    "    for idx in range(100):\n",
    "        particle = torch.tensor(particles[idx:idx+1], dtype=t_type, requires_grad=True)\n",
    "        log_term = torch.log(dm_ex.target_density(particle.t()))\n",
    "        grad_log_term[idx] = torch.autograd.grad(log_term, particle,\n",
    "                                                     only_inputs=True, retain_graph=False, \n",
    "                                                     create_graph=False, allow_unused=False)[0]\n",
    "    return grad_log_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = dm_ex.particles.data.clone().detach().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare kernel implementation\n",
    "\n",
    "ker, grad_ker = dm_ex.calc_kernel_term_latent(10.)\n",
    "print(dm_ex.h)\n",
    "grad_ker_sum = torch.sum(grad_ker, dim=1)\n",
    "\n",
    "ker_l, grad_ker_sum_l = dm_svgd.svgd_kernel(result, h=10.)\n",
    "\n",
    "print((ker - torch.tensor(ker_l)).norm())\n",
    "print((grad_ker_sum - torch.tensor(grad_ker_sum_l).t()).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare log_term implementation\n",
    "\n",
    "(lnprob(result) - dm_ex.calc_log_term_latent().t()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(201): \n",
    "    try:\n",
    "        result = dm_svgd.update(result, lnprob, stepsize=1e-2, bandwidth=-1, n_iter=20)\n",
    "        result = torch.tensor(result, dtype=t_type)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(result.data.cpu().numpy(), torch.zeros_like(result).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(result.data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(2000): \n",
    "    try:\n",
    "        dm_ex.update_latent(h_type=0)\n",
    "\n",
    "        if _ % 30 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm_ex.h))\n",
    "            \n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(dm_ex.particles.t().data.cpu().numpy(), torch.zeros_like(dm_ex.particles.t()).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(dm_ex.particles.t().data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(result - dm_ex.particles.t()).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"Stein-Variational-Gradient-Descent/python/\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.matlib as nm\n",
    "from svgd import SVGD\n",
    "\n",
    "r'''\n",
    "    Example of Bayesian Logistic Regression (the same setting as Gershman et al. 2012):\n",
    "    The observed data D = {X, y} consist of N binary class labels, \n",
    "    y_t \\in {-1,+1}, and d covariates for each datapoint, X_t \\in R^d.\n",
    "    The hidden variables \\theta = {w, \\alpha} consist of d regression coefficients w_k \\in R,\n",
    "    and a precision parameter \\alpha \\in R_+. We assume the following model:\n",
    "        p(\\alpha) = Gamma(\\alpha; a, b)\n",
    "        p(w_k | a) = N(w_k; 0, \\alpha^-1)\n",
    "        p(y_t = 1| x_t, w) = 1 / (1+exp(-w^T x_t))\n",
    "'''\n",
    "class BayesianLR:\n",
    "    def __init__(self, X, Y, batchsize=100, a0=1, b0=0.01):\n",
    "        self.X, self.Y = X, Y\n",
    "        # TODO. Y in \\in{+1, -1}\n",
    "        self.batchsize = min(batchsize, X.shape[0])\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.permutation = np.random.permutation(self.N)\n",
    "        self.iter = 0\n",
    "    \n",
    "        \n",
    "    def dlnprob(self, theta):\n",
    "        \n",
    "        if self.batchsize > 0:\n",
    "            batch = [ i % self.N for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize) ]\n",
    "            ridx = self.permutation[batch]\n",
    "            self.iter += 1\n",
    "        else:\n",
    "            ridx = np.random.permutation(self.X.shape[0])\n",
    "            \n",
    "        Xs = self.X[ridx, :]\n",
    "        Ys = self.Y[ridx]\n",
    "        \n",
    "        w = theta[:, :-1]  # logistic weights\n",
    "        alpha = np.exp(theta[:, -1])  # the last column is logalpha\n",
    "        d = w.shape[1]\n",
    "        \n",
    "        wt = np.multiply((alpha / 2), np.sum(w ** 2, axis=1))\n",
    "        \n",
    "        coff = np.matmul(Xs, w.T)\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-1 * coff))\n",
    "        \n",
    "        dw_data = np.matmul(((nm.repmat(np.vstack(Ys), 1, theta.shape[0]) + 1) / 2.0 - y_hat).T, Xs)  # Y \\in {-1,1}\n",
    "        dw_prior = -np.multiply(nm.repmat(np.vstack(alpha), 1, d) , w)\n",
    "        dw = dw_data * 1.0 * self.X.shape[0] / Xs.shape[0] + dw_prior  # re-scale\n",
    "        \n",
    "        dalpha = d / 2.0 - wt + (self.a0 - 1) - self.b0 * alpha + 1  # the last term is the jacobian term\n",
    "        \n",
    "        return np.hstack([dw, np.vstack(dalpha)])  # % first order derivative \n",
    "    \n",
    "    def evaluation(self, theta, X_test, y_test):\n",
    "        theta = theta[:, :-1]\n",
    "        M, n_test = theta.shape[0], len(y_test)\n",
    "\n",
    "        prob = np.zeros([n_test, M])\n",
    "        for t in range(M):\n",
    "            coff = np.multiply(y_test, np.sum(-1 * np.multiply(nm.repmat(theta[t, :], n_test, 1), X_test), axis=1))\n",
    "            prob[:, t] = np.divide(np.ones(n_test), (1 + np.exp(coff)))\n",
    "        \n",
    "        prob = np.mean(prob, axis=1)\n",
    "        acc = np.mean(prob > 0.5)\n",
    "        llh = np.mean(np.log(prob))\n",
    "        return [acc, llh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = X_test.shape[0] + X_train.shape[0]\n",
    "X_input = np.hstack([X, np.ones([N, 1])])\n",
    "y_input = y\n",
    "d = X_input.shape[1]\n",
    "D = d + 1\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.2, random_state=42)\n",
    "    \n",
    "a0, b0 = 1, 0.01 #hyper-parameters\n",
    "model = BayesianLR(X_train, y_train, 100, a0, b0) # batchsize = 100\n",
    "    \n",
    "# initialization\n",
    "M = 100  # number of particles\n",
    "theta0 = np.zeros([M, D]);\n",
    "alpha0 = np.random.gamma(a0, b0, M); \n",
    "for i in range(M):\n",
    "    theta0[i, :] = np.hstack([np.random.normal(0, np.sqrt(1 / alpha0[i]), d), np.log(alpha0[i])])\n",
    "    \n",
    "theta = SVGD().update(x0=theta0, lnprob=model.dlnprob, bandwidth=-1, n_iter=6000, stepsize=0.05, alpha=0.9, debug=True)\n",
    "    \n",
    "print('[accuracy, log-likelihood]')\n",
    "print(model.evaluation(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis–Hastings algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MHAlgorithm():\n",
    "    def __init__(self, n_dims): \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                           0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "        self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "        \n",
    "#         self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "#         self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "    \n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_dims, 1],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., -1.)\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "    def generate_next(self):\n",
    "        previous = self.particles[:, -1].unsqueeze(1)\n",
    "#         self.additional_distribution = normal_density(self.n_dims, previous, 1, n_particles_second=True)\n",
    "#         candidate = self.additional_distribution.get_sample()\n",
    "        \n",
    "#         alpha = (self.target_density(candidate) * self.additional_distribution.unnormed_density(previous) / \n",
    "#                 (self.target_density(previous) * self.additional_distribution.unnormed_density(candidate)))\n",
    "        candidate = previous + torch.zeros([self.n_dims, 1], device=device, dtype=t_type).uniform_(-1., 1.)\n",
    "        alpha = self.target_density(candidate) / self.target_density(previous)\n",
    "        \n",
    "#         print(self.target_density(candidate)[0].data.numpy(), self.additional_distribution.unnormed_density(previous)[0].data.numpy(),\n",
    "#               self.target_density(previous)[0].data.numpy(), self.additional_distribution.unnormed_density(candidate)[0].data.numpy())\n",
    "        \n",
    "        if alpha > self.one:\n",
    "            self.particles = torch.cat([self.particles, candidate], dim=1)\n",
    "        else:\n",
    "            random = torch.bernoulli(alpha)\n",
    "            if random:\n",
    "                self.particles = torch.cat([self.particles, candidate], dim=1)                \n",
    "            else:\n",
    "                self.particles = torch.cat([self.particles, previous], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = MHAlgorithm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    mh.generate_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(mh.particles.t().data.cpu().numpy()[:,1], \n",
    "            kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot(mh.particles.t().data.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import time\n",
    "\n",
    "mu,sig,N = 0,1,1000000\n",
    "pts = []\n",
    "\n",
    "def q(x):\n",
    "#     return (1/(math.sqrt(2*math.pi*sig**2)))*(math.e**(-((x-mu)**2)/(2*sig**2)))\n",
    "#     return normal_density(1, 0., 2., n_particles_second=True).unnormed_density(x)\n",
    "    return (0.3 * normal_density(1, -2., 1., n_particles_second=True).unnormed_density(x) +\n",
    "            0.7 * normal_density(1, 2., 1., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "def metropolis(N):\n",
    "    r = np.zeros(1)\n",
    "    p = q(r[0])\n",
    "    pts = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        rn = r + np.random.uniform(-1,1)\n",
    "        pn = q(rn[0])\n",
    "        if pn >= p:\n",
    "            p = pn\n",
    "            r = rn\n",
    "        else:\n",
    "            u = np.random.rand()\n",
    "            if u < pn/p:\n",
    "                p = pn\n",
    "                r = rn\n",
    "        pts.append(r)\n",
    "    \n",
    "    pts = np.array(pts)\n",
    "    return pts\n",
    "    \n",
    "def hist_plot(array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1,)\n",
    "    ax.hist(array, bins=1000)    \n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot(metropolis(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg =  lambda : (normal_density(1, -60., 20., n_particles_second=True).get_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot([gg()[0,0] for _ in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

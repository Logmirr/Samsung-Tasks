{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orth\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "device = None\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    use_cuda = True\n",
    "\n",
    "# Ignore cuda\n",
    "use_cuda = False\n",
    "device = None\n",
    "\n",
    "# Set DoubleTensor as a base type for computations\n",
    "t_type = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import functools\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_projections(dm=None, use_real=True, kernel='tri', pdf=None, N_plots_max=10):\n",
    "    \"\"\"\n",
    "        Plot marginal kernel density estimation\n",
    "    Args:\n",
    "        dm (DistributionMover): class containing particles which define distribution\n",
    "        use_real (bool): If set to True then apply transformation dm.lt.transform before creating plot\n",
    "        kernel (str): Kernel type for kernel density estimation\n",
    "        pdf (array_like, None): Samples from target distribution\n",
    "        N_plots_max (int): Maximum number of plots\n",
    "    \"\"\"\n",
    "    N_plots = None\n",
    "    scale_factor = None\n",
    "    \n",
    "    if use_real:\n",
    "        if not dm.use_latent:\n",
    "            return\n",
    "        N_plots = dm.lt.A.shape[0]\n",
    "    else:\n",
    "        N_plots = dm.particles.shape[0]\n",
    "    if N_plots > 6:\n",
    "        scale_factor = 15\n",
    "    else:\n",
    "        scale_factor = 5\n",
    "        \n",
    "    N_plots = min(N_plots, N_plots_max)\n",
    "        \n",
    "    plt.figure(figsize=(3 * scale_factor, (N_plots // 3 + 1) * scale_factor))\n",
    "    \n",
    "    for idx in range(N_plots):\n",
    "        slice_dim = idx\n",
    "        \n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        \n",
    "        particles = None\n",
    "        if use_real:\n",
    "            particles = dm.lt.transform(dm.particles, n_particles_second=True).t()[:, slice_dim]\n",
    "        else:\n",
    "            particles = dm.particles.t()[:, slice_dim]\n",
    "        \n",
    "        if pdf is not None:\n",
    "            plt.plot(np.linspace(-10, 10, len(pdf), dtype=np.float64), pdf)\n",
    "        plt.plot(particles.data.cpu().numpy(), torch.zeros_like(particles).data.cpu().numpy(), 'ro')\n",
    "        sns.kdeplot(particles.data.cpu().numpy(), \n",
    "                    kernel=kernel, color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_diffs(x, y, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: x is a dxN matrix\n",
    "                   y is an optional dxM matrix\n",
    "            Output: diffs is a dxNxM matrix where diffs[i,j] is the subtraction between x[:,i] and y[:,j]\n",
    "            i.e. diffs[i,j] = x[:,i] - y[:,j]\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: x is a Nxd matrix\n",
    "                   y is an optional Mxd matrix\n",
    "            Output: diffs is a NxMxd matrix where diffs[i,j] is the subtraction between x[i,:] and y[j,:]\n",
    "            i.e. diffs[i,j] = x[i,:]-y[j,:]\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return x[:,:,np.newaxis] - y[:,np.newaxis,:]        \n",
    "    return x[:,np.newaxis,:] - y[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_dists(diffs=None, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: diffs is a dxNxM matrix where diffs[i,j] = x[:,i] - y[:,j]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[:,i] - y[:,j]||\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: diffs is a NxMxd matrix where diffs[i,j] = x[i,:] - y[j, :]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[i,:]-y[j,:]||\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return torch.norm(diffs, dim=0)\n",
    "    return torch.norm(diffs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [],
   "source": [
    "class normal_density():\n",
    "    \"\"\"\n",
    "        Multinomial normal density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, mu=0., std=1., n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            mu (float, array_like): mean of distribution\n",
    "                Default: 0.\n",
    "                if mu is float - use same mean across all dimensions\n",
    "                if mu is 1D array_like - use different mean for each dimension but same for each particles dimension\n",
    "                if mu is 2D array_like - use different mean for each dimension\n",
    "            std (float, array_like): std of distribution\n",
    "                Default: 1.\n",
    "                if std is float - use same std across all dimensions\n",
    "                if std is 1D array_like - use different std for each dimension but same for each particles dimension\n",
    "                if std is 2D array_like - use different std for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis\n",
    "                Input will be reduced along all axis exclude particle axis\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        \n",
    "        if type(self.mu) == float:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.mu.shape) == 1:\n",
    "            if self.mu.shape[0] == 1:\n",
    "                self.mu = self.mu.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.mu.shape) == 2:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if type(self.std) == float:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.std.shape) == 1:\n",
    "            if len(self.std) == 1:\n",
    "                self.std = self.std.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.zero = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.pi = torch.tensor(math.pi, dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "    def __call__(self, x, n_axis=None):        \n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.pow(self.two * self.pi, -self.n / self.two) / \n",
    "                torch.prod(self.std, dim=n_axis) * \n",
    "                torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):      \n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std , self.two), dim=n_axis))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):  \n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (-self.n / self.two * torch.log(self.two * self.pi) + \n",
    "                torch.sum(torch.log(self.std), dim=n_axis) -\n",
    "                self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return -self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)\n",
    "    \n",
    "    def get_sample(self):\n",
    "        \"\"\"\n",
    "            Sample from normal distribution\n",
    "        \"\"\"\n",
    "        sample = torch.normal(self.mu, self.std)\n",
    "        if self.n_particles_second:\n",
    "            return sample.view(-1, 1)\n",
    "        else:\n",
    "            return sample.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class gamma_density():\n",
    "    \"\"\"\n",
    "        Multinomial gamma density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, alpha=1, betta=1, n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            alpha (float, array_like): shape of distribution\n",
    "                Default: 1.\n",
    "                if alpha is float - use same shape across all dimensions\n",
    "                if alpha is 1D array_like - use different shape for each dimension but same for each particles dimension\n",
    "                if alpha is 2D array_like - use different shape for each dimension\n",
    "            betta (float, array_like): rate of distribution\n",
    "                Default: 1.\n",
    "                if betta is float - use same rate across all dimensions\n",
    "                if betta is 1D array_like - use different rate for each dimension but same for each particles dimension\n",
    "                if betta is 2D array_like - use different rate for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis  \n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.betta = betta\n",
    "        \n",
    "        if type(self.alpha) == float:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.alpha.shape) == 1:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.alpha.shape) == 2:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if type(self.betta) == float:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.betta.shape) == 1:\n",
    "            if len(self.betta) == 1:\n",
    "                self.betta = self.betta.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "        ### log Г(alpha)\n",
    "        self.lgamma = torch.lgamma(self.alpha)\n",
    "        ### Г(alpha)\n",
    "        self.gamma = torch.exp(self.lgamma)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(self.betta, self.alpha) / self.gamma * torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SteinLinear(nn.Module):\n",
    "    \"\"\"\n",
    "        Custom full connected layer for Stein Gradient Neural Networks\n",
    "        Transformation: y = xA + b\n",
    "        Parameters prior: \n",
    "            1 - p(w, a) = p(w|a)p(a) = П p(w_i|a_i)p(a_i)\n",
    "                p(w_i|a_i) = N(w_i|0, a_i^(-1)); p(a_i) = G(1e-4, 1e-4)\n",
    "                          \n",
    "            2 - p(w) = П p(w_i)\n",
    "                p(w_i) = N(w_i|0, alpha^(-1))\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, n_particles=1, use_bias=True, use_var_prior=True, alpha=1e-2):\n",
    "        super(SteinLinear, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features (int): size of each input sample\n",
    "            out_features (int): size of each output sample\n",
    "            n_particles (int): number of particles\n",
    "            use_bias (bool): If set to False, the layer will not learn an additive bias.\n",
    "                Default: True\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of weight variance\n",
    "                Default: True\n",
    "            alpha (float): If use_var_prior == False - defines weight variance\n",
    "                Default: 1e-2\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_particles = n_particles\n",
    "        self.use_bias = use_bias\n",
    "        self.use_var_prior = use_var_prior\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.zeros([in_features, out_features, n_particles], dtype=t_type, device=device))\n",
    "        if self.use_var_prior:\n",
    "            self.log_weight_alpha = torch.nn.Parameter(torch.zeros([in_features * out_features, n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_weight_alpha = torch.tensor([math.log(self.alpha)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        self.bias = None\n",
    "        self.log_bias_alpha = None\n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros([1, out_features, n_particles], dtype=t_type, device=device))\n",
    "            if self.use_var_prior:\n",
    "                self.log_bias_alpha = torch.nn.Parameter(torch.zeros([out_features, n_particles], dtype=t_type, device=device))\n",
    "            else:\n",
    "                self.log_bias_alpha = torch.tensor([math.log(self.alpha)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        self.weight_alpha_log_prior = None\n",
    "        self.bias_alpha_log_prior = None\n",
    "        if self.use_var_prior:\n",
    "            ### define prior on alpha p(a) = G(1e-4, 1e-4)\n",
    "            self.weight_alpha_log_prior = lambda x: (gamma_density(n=self.log_weight_alpha.shape[0],\n",
    "                                                                   alpha=1e-4,\n",
    "                                                                   betta=1e-4,\n",
    "                                                                   n_particles_second=True\n",
    "                                                                  ).log_unnormed_density_log_x(x))\n",
    "            if self.use_bias:\n",
    "                self.bias_alpha_log_prior = lambda x: (gamma_density(n=self.log_bias_alpha.shape[0],\n",
    "                                                                     alpha=1e-4,\n",
    "                                                                     betta=1e-4,\n",
    "                                                                     n_particles_second=True\n",
    "                                                                    ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    ### useless function - all initialization defined in DistributionMover class\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.log_weight_alpha.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.log_bias_alpha.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Apply transformation: X_out[i, :, :] = X_in[i, :, :] * W + b[i]\n",
    "        Args:\n",
    "            X (torch.tensor): tensor \n",
    "        Shape:\n",
    "            Input: [n_particles, batch_size, in_features]\n",
    "            Output: [n_particles, batch_size, out_features]\n",
    "        \"\"\"\n",
    "        ### NEED SOME OPTIMIZATION TO OMMIT .permute\n",
    "        if self.use_bias:\n",
    "            return torch.bmm(X, self.weight.permute(2, 0, 1)) + self.bias.permute(2, 0, 1)\n",
    "        return torch.bmm(X, self.weight.permute(2, 0, 1))\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())\n",
    "        \n",
    "    def calc_log_prior(self):\n",
    "        \"\"\"\n",
    "            Evaluate log prior of trainable parameters\n",
    "        log p(w,a) = log p(w|a) + log p(a\n",
    "        \"\"\"\n",
    "        ### define prior on weight p(w|a) = N(0, 1 / alpha)\n",
    "        self.weight_log_prior = lambda x: (normal_density(n=self.weight.numel() // self.n_particles,\n",
    "                                                          mu=0.,\n",
    "                                                          std=self.one / torch.exp(self.log_weight_alpha),\n",
    "                                                          n_particles_second=True\n",
    "                                                         ).log_unnormed_density(x))\n",
    "\n",
    "        self.bias_log_prior = lambda x: (normal_density(self.bias.numel() // self.n_particles,\n",
    "                                                        mu=0.,\n",
    "                                                        std=self.one / torch.exp(self.log_bias_alpha),\n",
    "                                                        n_particles_second=True\n",
    "                                                       ).log_unnormed_density(x))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            if self.use_var_prior:\n",
    "                return (self.weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha) +\n",
    "                        self.bias_log_prior(self.bias.view(-1, self.n_particles)) + self.bias_alpha_log_prior(self.log_bias_alpha))   \n",
    "            return (self.weight_log_prior(self.weight.view(-1, self.n_particles)) +\n",
    "                    self.bias_log_prior(self.bias.view(-1, self.n_particles)))        \n",
    "        if self.use_var_prior:\n",
    "            return self.weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha)\n",
    "        return self.weight_log_prior(self.weight.view(-1, self.n_particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LinearTransform():\n",
    "    \"\"\"\n",
    "        Class for various linear transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, n_dims, n_hidden_dims, use_identity=False, normalize=False, A=None, tetta_0=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_dims (int): dimension of the space\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_identity (bool): If set to True, use 'eye' matrix for transformations\n",
    "            normalize (bool): If set to True, columns of the transformation matrix is an orthonormal basis \n",
    "            A (2D array_like, None): Initial value for transformation matrix\n",
    "                If None then matrix will be sampled from uniform distribution and then orthonormate\n",
    "                Default: None\n",
    "            tetta_0 (1D array_like, None): Initial value for bias\n",
    "                If None then matrix will be sampled from uniform distribution\n",
    "                Default: None\n",
    "        \"\"\"\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_identity = use_identity\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if self.use_identity:\n",
    "            return\n",
    "        \n",
    "        self.A = A\n",
    "        self.tetta_0 = tetta_0\n",
    "        \n",
    "        if self.A is None:\n",
    "            self.A = torch.zeros([self.n_dims, self.n_hidden_dims], dtype=t_type, device=device)\n",
    "            self.A.uniform_(-1., 1.)\n",
    "            if self.normalize:\n",
    "                ### normalize columns of matrix A\n",
    "                self.A = torch.tensor(orth(self.A.data.cpu().numpy()), dtype=t_type, device=device)\n",
    "                    \n",
    "        if self.tetta_0 is None:\n",
    "            self.tetta_0 = torch.zeros([self.n_dims, 1], dtype=t_type, device=device)\n",
    "            self.tetta_0.uniform_(-1.,1.)\n",
    "        \n",
    "        ### A^(t)A\n",
    "        self.AtA = torch.matmul(self.A.t(), self.A)\n",
    "        ### (A^(t)A)^(-1)\n",
    "        self.AtA_1 = torch.inverse(self.AtA)\n",
    "        ### (A^(t)A)^(-1)A^(t)\n",
    "        self.inverse_base = torch.matmul(self.AtA_1, self.A.t())\n",
    "        # ### A(A^(t)A)^(-1)A^(t)\n",
    "        # self.projector_base = torch.matmul(self.A, self.inverse_base)\n",
    "        \n",
    "    def transform(self, tetta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Transform tettas as follows: \n",
    "                tetta = Atetta` + tetta_0\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.A, tetta) + self.tetta_0\n",
    "        return (torch.matmul(self.A, tetta.t()) + self.tetta_0).t()\n",
    "    \n",
    "    def inverse_transform(self, tetta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Apply inverse transformation: \n",
    "                tetta` = (A^(t)A)^(-1)A^(t)(tetta - tetta_0)\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.inverse_base, tetta - self.tetta_0)\n",
    "        return torch.matmul(self.inverse_base, tetta.t() - self.tetta_0).t()\n",
    "        \n",
    "    def project(self, tetta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Project tettas onto Linear Space X = {Atetta` + tetta_0 for all tetta` in R^d}:\n",
    "                tetta_projected = A(A^(t)A)^(-1)A^(t)(tetta - tetta_0) + tetta_0\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.projector_base, tetta - self.tetta_0) + self.tetta_0\n",
    "        return (torch.matmul(self.projector_base, tetta.t() - self.tetta_0) + self.tetta_0).t()\n",
    "    \n",
    "    def project_inverse(self, tetta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Project and then apply inverse transform to tetta - tetta_0:\n",
    "                tetta_s_p_i = T^(-1)P(tetta - tetta_0)= (A^(t)A)^(-1)A^(t)tetta\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        ### use solver trick: tetta_s_p_i : A^(t)Atetta_s_p_i = A^(t)tetta\n",
    "        if n_particles_second:\n",
    "            return torch.gesv(torch.matmul(self.A.t(), tetta), self.AtA)[0]\n",
    "        return torch.gesv(torch.matmul(self.A.t(), tetta.t()), self.AtA)[0].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class RegressionDistribution(nn.Module):\n",
    "    \"\"\"\n",
    "        Distribution over data for regression task: p(D|w) = N(y_predicted|y, )\n",
    "    \"\"\"\n",
    "    def __init__(self, n_particles, use_var_prior=True, betta=10-1):\n",
    "        super(RegressionDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of prediction variance\n",
    "                Default: True\n",
    "            betta (float): If use_var_prior == False - defines variance of prediction\n",
    "                Default: 1e-1\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.use_var_prior = use_var_prior\n",
    "        self.betta = betta\n",
    "        \n",
    "        ### define betta - variance of data distribution for regression tasks (betta = 1 / std ** 2)\n",
    "        self.log_betta = None\n",
    "        if self.use_var_prior:\n",
    "            self.log_betta = torch.nn.Parameter(torch.zeros([1, self.n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_betta = torch.tensor([math.log(self.betta)], dtype=t_type, device=device, requires_grad=False)\n",
    "\n",
    "        self.betta_log_prior = None\n",
    "        if self.use_var_prior:\n",
    "            ### define prior on betta p(betta)\n",
    "            self.betta_log_prior = lambda x: (gamma_density(n=1,\n",
    "                                                            alpha=1e-4,\n",
    "                                                            betta=1e-4,\n",
    "                                                            n_particles_second=True\n",
    "                                                           ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        ### Support tensors for computations\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "    \n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(tetta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predicted values\n",
    "            train_size (int) - size of training dataset\n",
    "        Shapes:\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, 1]\n",
    "        \"\"\"\n",
    "        ### squeeze last axis because regression task is being solved\n",
    "        y_predict.squeeze_(2)\n",
    "        \n",
    "        batch_size = torch.tensor(y.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        self.log_data_distr = None\n",
    "        if self.use_var_prior:\n",
    "            self.log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                            mu=y,\n",
    "                                                            std=self.one / torch.sqrt(torch.exp(self.log_betta.expand(X.shape[0], self.n_particles).t())),\n",
    "                                                            n_particles_second=False\n",
    "                                                           ).log_unnormed_density(x))\n",
    "        else:\n",
    "            self.log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                            mu=y,\n",
    "                                                            std=self.one / torch.sqrt(torch.exp(self.log_betta)),\n",
    "                                                            n_particles_second=False\n",
    "                                                           ).log_unnormed_density(x))\n",
    "            \n",
    "        if self.use_var_prior:\n",
    "            return train_size / batch_size * self.log_data_distr(y_predict) + self.betta_log_prior(self.log_betta)\n",
    "        return train_size / batch_size * self.log_data_distr(y_predict)\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ClassificationDistribution(nn.Module):\n",
    "    def __init__(self, n_particles):\n",
    "        super(ClassificationDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        \"\"\"\n",
    "        self.n_particles = n_particles\n",
    "    \n",
    "\n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(tetta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shapes: \n",
    "            X.shape = [batch_size, in_features]\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        batch_size = torch.tensor(X.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        \n",
    "        probas = nn.LogSoftmax(dim=2)(y_predict)\n",
    "        probas_selected = torch.gather(input=probas, dim=2, index=y.view(1, -1, 1).expand(probas.shape[0], probas.shape[1], 1)).squeeze(2)\n",
    "        log_data = torch.sum(probas_selected, dim=1)\n",
    "        \n",
    "        return train_size / batch_size  * log_data\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "code_folding": [
     129,
     226,
     257,
     277,
     320,
     326,
     332,
     414,
     425,
     432
    ]
   },
   "outputs": [],
   "source": [
    "class DistributionMover():\n",
    "    def __init__(self,\n",
    "                 task='app', \n",
    "                 n_particles=None,\n",
    "                 particles=None,\n",
    "                 target_density=None,\n",
    "                 n_dims=None,\n",
    "                 n_hidden_dims=None,\n",
    "                 use_latent=False,\n",
    "                 net=None,\n",
    "                 precomputed_params=None,\n",
    "                 data_distribution=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task (str):\n",
    "                'app' | 'net_reg' | 'net_class'\n",
    "                - approximate target distribution\n",
    "                - solve regression task using net\n",
    "                - solve classification task using net\n",
    "            n_particles (int): number of particles\n",
    "            particles (2D array_like): array which contains initialized particles\n",
    "            target_density (callable): computes probability density function of target distribution (only for 'app' task)\n",
    "            n_dims (int): dimension of the space where optimization is performed\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_latent (bool): If set to True, Subspace Stein is used\n",
    "            net (nn.Sequential): object which is used to make predictions (for 'net_reg' and' net_class' tasks)\n",
    "            precomputed_params (1D array_like): Precomputed parameters, which will be used for particles initialization\n",
    "            data_distribution (callable): computes probability over data p(D|w) (for 'net_reg' and' net_class' tasks)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.task = task\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.particles = particles\n",
    "        self.target_density = target_density\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_latent = use_latent\n",
    "        self.net = net\n",
    "        self.precomputed_params = precomputed_params\n",
    "        self.data_distribution = data_distribution\n",
    "        \n",
    "\n",
    "        if self.task == 'net_reg' or self.task == 'net_class':\n",
    "            self.n_dims = 0\n",
    "            for module in self.modules_net():\n",
    "                if \"numel\" in dir(module):\n",
    "                    self.n_dims += module.numel() // self.n_particles\n",
    "\n",
    "        if not self.use_latent:\n",
    "            self.n_hidden_dims = self.n_dims\n",
    "\n",
    "        ### Learnable samples from the target distribution\n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_hidden_dims, self.n_particles],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., 2.)\n",
    "\n",
    "        ### Class for performing linear transformations\n",
    "        self.lt = None\n",
    "        if self.use_latent:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=False, \n",
    "                normalize=True\n",
    "            )\n",
    "        else:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=True, \n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "        if self.precomputed_params is not None:\n",
    "            self.particles = self.lt.inverse_transform(self.precomputed_params.unsqueeze(1).expand(self.n_dims, self.n_particles))\n",
    "\n",
    "        ### Functions of probability density of target distribution\n",
    "        self.target_density = None\n",
    "        self.real_target_density = None\n",
    "        if self.net is None:\n",
    "            # use unnormed probability density to speedup computations\n",
    "            if target_density is not None:\n",
    "                self.target_density = target_density\n",
    "                self.real_target_density = target_density\n",
    "            else:\n",
    "                self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                                   0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "                self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                        0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "\n",
    "                # self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "                # self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "        ### Number of iterations since beginning\n",
    "        self.iter = 0\n",
    "\n",
    "        ### Adagrad parameters\n",
    "        self.fudge_factor = torch.tensor(1e-6, dtype=t_type, device=device)\n",
    "        self.step_size = torch.tensor(1e-2, dtype=t_type, device=device)\n",
    "        self.auto_corr = torch.tensor(0.9, dtype=t_type, device=device)\n",
    "\n",
    "        ### Gradient history term for adagrad optimization\n",
    "        self.historical_grad = None\n",
    "        self.historical_grad_tetta_0 = None\n",
    "        if self.use_latent:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_hidden_dims, n_particles], dtype=t_type, device=device)\n",
    "            self.historical_grad_tetta_0 = torch.zeros(\n",
    "                [self.n_dims, 1], dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_dims, n_particles], dtype=t_type, device=device)\n",
    "            \n",
    "\n",
    "        ### Factor from kernel\n",
    "        self.med = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.h = torch.tensor(0., dtype=t_type, device=device)\n",
    "\n",
    "        ### Support tensors for computations\n",
    "        self.N = torch.tensor(self.n_particles, dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.three = torch.tensor(3., dtype=t_type, device=device)\n",
    "\n",
    "    def calc_kernel_term_latent(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(tetta-tetta`)^2) / logN\n",
    "                    1 - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    3 - var(tetta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(tetta-tetta`) / logN * n_dims\n",
    "                    5 - med(dist(tetta-tetta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(tetta-tetta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atetta`_i + tetta_0, Atetta`_j + tetta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        ### power for rational kernel\n",
    "        self.p = torch.tensor(p, dtype=t_type, device=device) if p is not None else None\n",
    "        \n",
    "        ### tetta = Atetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### diffs[i, j] = A(tetta`_i - tetta`_j)\n",
    "        diffs = pairwise_diffs(real_particles, real_particles, n_particles_second=True)\n",
    "        ### dists[i, j] = ||A(tetta`_i - tetta`_j)||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        ### sq_dists[i, j] = ||A(tetta`_i - tetta`_j)||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "        \n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 2:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            self.var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            self.var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 5:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  self.med / (torch.pow(self.N, self.two) - self.one)\n",
    "        elif h_type == 6:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  self.med / (torch.pow(self.N, -self.one / self.p) - self.one)\n",
    "        elif h_type == 7:\n",
    "            self.med = torch.median(torch.matmul(real_particles.t(), real_particles)) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        \n",
    "        kernel = None\n",
    "        grad_kernel = None\n",
    "        if kernel_type == 'rbf':\n",
    "            ### RBF Kernel:\n",
    "            ### kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "            kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "            ### grad_kernel[i, j] = -2/h * A(tetta`_i - tetta`_j) * kernel[i, j]\n",
    "            grad_kernel = -self.two / self.h * kernel.unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'imq':\n",
    "            ### IMQ Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, -self.one / self.two)\n",
    "            ### grad_kernel[i, j] = -1/h * A(tetta`_i - tetta`_j) * kernel^(3)[i, j]\n",
    "            grad_kernel = -self.one / self.h * torch.pow(kernel, self.three).unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'exp':\n",
    "            ### Exponential Kernel:\n",
    "            ### kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "            kernel = torch.exp(self.one / self.h * torch.matmul(real_particles.t(), real_particles))\n",
    "            ### grad_kernel[i, j] = 1/h * (Atetta`_j + tetta_0) * kernel[i, j]\n",
    "            grad_kernel = 1. / self.h * kernel.unsqueeze(0) * real_particles.unsqueeze(1)\n",
    "        elif kernel_type == 'rat':\n",
    "            ### RAT Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, self.p)\n",
    "            ### grad_kernel[i, j] = p/h * A(tetta`_i - tetta`_j) * kernel^((p - 1)/p)[i, j]\n",
    "            grad_kernel = self.p / self.h * torch.pow(kernel, (self.p - self.one) / self.p).unsqueeze(0) * diffs\n",
    "            \n",
    "        return kernel, grad_kernel\n",
    "    \n",
    "    def calc_kernel_term_latent_net(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(tetta-tetta`)^2) / logN\n",
    "                    1 - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    3 - var(tetta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(tetta-tetta`) / logN * n_dims\n",
    "                    5 - med(dist(tetta-tetta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(tetta-tetta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atetta`_i + tetta_0, Atetta`_j + tetta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        return self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "\n",
    "    def calc_log_term_latent(self):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(tetta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        \n",
    "        ### tetta = A tetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_term = torch.log(self.target_density(real_particles))\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "    \n",
    "        grad_log_term = real_particles.grad\n",
    "        \n",
    "        return grad_log_term\n",
    "    \n",
    "    def calc_log_term_latent_net(self, X, y, train_size):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(tetta)) \n",
    "        Args:\n",
    "            X (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input: \n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output: \n",
    "                [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        self.log_data = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        self.log_prior = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        \n",
    "        ### get real net parameters: tetta_i = A tetta`_i + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### init net with real parameters\n",
    "        self.vector_to_parameters(real_particles.view(-1), self.parameters_net())\n",
    "        ### compute log prior of all weight in the net\n",
    "        for module in self.modules_net():\n",
    "            if \"calc_log_prior\" in dir(module):\n",
    "                self.log_prior += module.calc_log_prior()\n",
    "        \n",
    "        ### get prediction for the batch of data\n",
    "        y_predict = self.predict_net(X)\n",
    "        ### compute log data term log p(D|w)\n",
    "        self.log_data = self.data_distribution.calc_log_data(X, y, y_predict, train_size)\n",
    "        \n",
    "        ### log_term = log p(tetta) = log p_prior(tetta) + log p_data(D|tetta)\n",
    "        log_term = self.log_prior + self.log_data\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "        \n",
    "        ### collect all gradients into one vector\n",
    "        grad_log_term = self.parameters_grad_to_vector(self.parameters_net()).view(-1, self.n_particles)\n",
    "            \n",
    "        return grad_log_term\n",
    "    \n",
    "    def parameters_net(self):\n",
    "        \"\"\"\n",
    "            Return all trainable parameters\n",
    "        \"\"\"\n",
    "        return chain(self.net.parameters(), self.data_distribution.parameters())\n",
    "    \n",
    "    def modules_net(self):\n",
    "        \"\"\"\n",
    "            Return all modules\n",
    "        \"\"\"\n",
    "        return chain(self.net.modules(), self.data_distribution.modules())\n",
    "\n",
    "    def predict_net(self, X, inference=False):\n",
    "        \"\"\"\n",
    "            Use net to make predictions        \n",
    "            Args:\n",
    "                X (array_like): batch of data\n",
    "        \"\"\"\n",
    "        predictions = self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape))\n",
    "        if self.task == 'net_reg':\n",
    "            if inference:\n",
    "                return torch.mean(predictions, dim=0)\n",
    "            else:\n",
    "                return predictions\n",
    "        elif self.task == 'net_class':\n",
    "            if inference:\n",
    "                return torch.mean(torch.nn.LogSoftmax(dim=2)(predictions), dim=0)\n",
    "            else:\n",
    "                return predictions\n",
    "\n",
    "    def update_latent(self, h_type, kernel_type='rbf', p=None, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent()\n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "        \n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "        \n",
    "#         ### Compute value of step in functional space\n",
    "#         tetta_0_update = -torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "        \n",
    "#         ### Update gradient history\n",
    "#         if self.iter == 1:\n",
    "#             self.historical_grad_tetta_0 = self.historical_grad_tetta_0 + tetta_0_update * tetta_0_update\n",
    "#         else:\n",
    "#             self.historical_grad_tetta_0 = self.auto_corr * self.historical_grad_tetta_0 + (self.one - self.auto_corr) * tetta_0_update * tetta_0_update\n",
    "            \n",
    "#         ### Adjust gradient and make step\n",
    "#         adj_tetta_0_update = tetta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_tetta_0))\n",
    "#         self.lt.tetta_0 = self.lt.tetta_0 + self.step_size * adj_tetta_0_update\n",
    "               \n",
    "    def update_latent_net(self, h_type, kernel_type='rbf', p=None, X_batch=None, y_batch=None, train_size=None, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "        self.net.zero_grad()\n",
    "        self.data_distribution.zero_grad()\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent_net(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_net(X_batch, y_batch, train_size)\n",
    "        \n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_to_parameters(vec, parameters):\n",
    "        pointer = 0\n",
    "        for param in parameters:\n",
    "            # The length of the parameter\n",
    "            num_param = param.numel()\n",
    "            # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "            param.data = vec[pointer:pointer + num_param].view_as(param).data\n",
    "            # Increment the pointer\n",
    "            pointer += num_param\n",
    "            \n",
    "    @staticmethod \n",
    "    def parameters_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    @staticmethod \n",
    "    def parameters_grad_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.grad.view(-1))\n",
    "        return torch.cat(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.7471, dtype=torch.float64) tensor(22.9835, dtype=torch.float64)\n",
      "tensor(77.4397, dtype=torch.float64) tensor(88.1444, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "### Bostor housing dataset for regression task\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = (torch.tensor(X_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(X_test, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_test, dtype=t_type, device=device))\n",
    "\n",
    "### Linear Regression baseline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.tensor(model.predict(X_test), dtype=t_type, device=device), y_test), \n",
    "      torch.nn.MSELoss()(torch.tensor(model.predict(X_train), dtype=t_type, device=device), y_train))\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.mean(y_train).expand(y_test.shape[0]), y_test), \n",
    "      torch.nn.MSELoss()(torch.mean(y_train).expand(y_train.shape[0]), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.12806   0.0378    0.05861   3.24007 -16.22227   3.89352  -0.01279\n",
      "  -1.42327   0.23451  -0.0082   -0.92995   0.01192  -0.54849]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Check all functions\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=5, n_particles=10, use_latent=True)\n",
    "dm.calc_log_term_latent()\n",
    "dm.calc_kernel_term_latent(0)\n",
    "dm.update_latent(0)\n",
    "\n",
    "ss = nn.Sequential(SteinLinear(13, 1, 10))\n",
    "dd = RegressionDistribution(n_particles=10)\n",
    "dm = DistributionMover(task='net_reg', n_hidden_dims=5, n_particles=10, use_latent=True, net=ss, data_distribution=dd)\n",
    "dm.calc_log_term_latent_net(X_train, y_train, X_train.shape[0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(13, 1, 100, use_var_prior=False, alpha=1e-2, use_bias=True))\n",
    "data_distr = RegressionDistribution(100, use_var_prior=False, betta=1e-1)\n",
    "dm = DistributionMover(task='net_reg', n_particles=100, use_latent=False, net=net, data_distribution=data_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alpha = dm.net[0].alpha\n",
    "betta = dm.data_distribution.betta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Append column of ones to data \n",
    "# XX = X_train\n",
    "# XX_test = X_test\n",
    "XX = torch.cat([X_train, torch.ones([X_train.shape[0], 1], dtype=t_type, device=device)], dim=1)\n",
    "XX_test = torch.cat([X_test, torch.ones([X_test.shape[0], 1], dtype=t_type, device=device)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sigma = torch.inverse(betta * XX.t() @ XX + alpha * torch.eye(XX.shape[1], dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu = betta * sigma @ XX.t() @ y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu @ XX.t(), y_train), torch.nn.MSELoss()(mu @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_test = torch.load('particles_12400.txt').cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_mean = torch.mean(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(particles_mean @ XX.t(), y_train), torch.nn.MSELoss()(particles_mean @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.var(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.diag(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import *\n",
    "\n",
    "mod = WLS(y_train.data.cpu().numpy(), XX.data.cpu().numpy())\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stein_mu = (torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu, torch.tensor(results.params, dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu, particles_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(sum(np.logical_and(results.conf_int()[:,0] < stein_mu.data.cpu().numpy(), stein_mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "      sum(np.logical_and(results.conf_int()[:,0] < mu.data.cpu().numpy(), mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "      sum(np.logical_and(results.conf_int()[:,0] < particles_mean.data.cpu().numpy(), particles_mean.data.cpu().numpy() < results.conf_int()[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu, torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    step_size = 0.00075\n",
    "    dm.historical_grad.zero_()\n",
    "    for _ in range(100000):\n",
    "        dm.update_latent_net(h_type=1, kernel_type='rbf', p=-1, X_batch=X_train, y_batch=y_train, train_size=X_train.shape[0], step_size=step_size)\n",
    "        train_loss = torch.nn.MSELoss()(dm.predict_net(X_train, inference=True).view(-1), y_train)\n",
    "        test_loss = torch.nn.MSELoss()(dm.predict_net(X_test, inference=True).view(-1), y_test)\n",
    "        \n",
    "        if _ % 10 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train): {1:.3f}\\t Empirical Loss(Test): {2:.3f}\\t Kernel factor: {3:.3f}'.format(\n",
    "                            _, train_loss, test_loss, dm.h))\n",
    "            \n",
    "            plot_projections(dm, use_real=True)\n",
    "            plot_projections(dm, use_real=False)\n",
    "            plt.pause(1e-300)\n",
    "            \n",
    "        if _ % 3000 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import MNIST dataset with class selection\n",
    "\n",
    "sys.path.insert(0, '/home/m.nakhodnov/Samsung-Tasks/Datasets/MyMNIST')\n",
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform)\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform)\n",
    "\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=32, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MNIST with Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### MAP estimate for MNIST classification task\n",
    "net_map = nn.Sequential(SteinLinear(28 * 28, 200, 1), nn.ReLU(), SteinLinear(200, 200, 1), nn.ReLU(), SteinLinear(200, 10, 1)).to(device=device)\n",
    "data_distr_map = ClassificationDistribution(1)\n",
    "dm_map = DistributionMover(task='net_class', n_particles=1, use_latent=False, net=net_map, data_distribution=data_distr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    step_size = 0.003\n",
    "    for epoch in range(30):\n",
    "        for train_epoch in range(30):\n",
    "            X, y = next(iter(dataloader_m_train))\n",
    "            X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "            y = y.to(device=device)\n",
    "            dm_map.update_latent_net(h_type=0, X_batch=X, y_batch=y, train_size=dataloader_m_train.__len__(), step_size=step_size)\n",
    "        \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().to(device=device).view(X.shape[0], -1)\n",
    "            y_train = y_train.to(device=device)\n",
    "            \n",
    "            net_pred = dm_map.predict_net(X_train, inference=True)\n",
    "            y_pred = torch.argmax(net_pred, dim=1)\n",
    "            train_loss += nn.NLLLoss()(net_pred, y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 15.\n",
    "        train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().to(device=device).view(X.shape[0], -1)\n",
    "            y_test = y_test.to(device=device)\n",
    "\n",
    "            net_pred = dm_map.predict_net(X_test, inference=True)\n",
    "            y_pred = torch.argmax(net_pred, dim=1)\n",
    "            test_loss += nn.NLLLoss()(net_pred, y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 15.\n",
    "        test_acc /= (15. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            epoch, train_loss, test_loss, train_acc, test_acc, dm_map.h))\n",
    "        if (epoch * 30) % 600 == 0 and epoch > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(28 * 28, 200, 10), nn.ReLU(), SteinLinear(200, 200, 10), nn.ReLU(), SteinLinear(200, 10, 10)).to(device=device)\n",
    "data_distr = ClassificationDistribution(10)\n",
    "dm = DistributionMover(task='net_class',\n",
    "                       n_particles=10,\n",
    "                       n_hidden_dims=700,\n",
    "                       use_latent=True,\n",
    "                       net=net,\n",
    "                       data_distribution=data_distr\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.6331, dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1083))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = 0. \n",
    "train_acc = 0.\n",
    "for __ in range(15):\n",
    "    X_train, y_train = next(iter(dataloader_m_train))\n",
    "    X_train = X_train.double().to(device=device).view(X_train.shape[0], -1)\n",
    "    y_train = y_train.to(device=device)\n",
    "            \n",
    "    net_pred = dm.predict_net(X_train, inference=True)\n",
    "    y_pred = torch.argmax(net_pred, dim=1)\n",
    "    train_loss += nn.NLLLoss()(net_pred, y_train)\n",
    "    train_acc += torch.sum(y_pred == y_train).float()\n",
    "    \n",
    "train_loss /= 15.\n",
    "train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dm.lt.tetta_0 = dm_map.parameters_to_vector(dm_map.parameters_net()).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    step_size = 0.01\n",
    "    for epoch in range(30):\n",
    "        for train_epoch in range(30):\n",
    "            X, y = next(iter(dataloader_m_train))\n",
    "            X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "            y = y.to(device=device)\n",
    "            dm.update_latent_net(h_type=0, X_batch=X, y_batch=y, train_size=dataloader_m_train.__len__(), step_size=step_size)\n",
    "        \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().to(device=device).view(X.shape[0], -1)\n",
    "            y_train = y_train.to(device=device)\n",
    "            \n",
    "            net_pred = dm.predict_net(X_train, inference=True)\n",
    "            y_pred = torch.argmax(net_pred, dim=1)\n",
    "            train_loss += nn.NLLLoss()(net_pred, y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 15.\n",
    "        train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().to(device=device).view(X.shape[0], -1)\n",
    "            y_test = y_test.to(device=device)\n",
    "            \n",
    "            net_pred = dm.predict_net(X_test, inference=True)\n",
    "            y_pred = torch.argmax(net_pred, dim=1)\n",
    "            test_loss += nn.NLLLoss()(net_pred, y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 15.\n",
    "        test_acc /= (15. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            epoch, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "        if (epoch * 30) % 600 == 0 and epoch > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5), tensor(3))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X_test.shape[0], -1)\n",
    "y_pred = dm.predict_net(X_test, inference=True)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_projections(dm, use_real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MNIST with FC neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_nn = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ").double()\n",
    "optim_nn = torch.optim.Adam(net_nn.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for _, (X, y) in enumerate(dataloader_m_train):\n",
    "        X = X.double().view(X.shape[0], -1)\n",
    "        \n",
    "        optim_nn.zero_grad()\n",
    "        train_loss = nn.CrossEntropyLoss()(net_nn(X), y)\n",
    "        train_loss.backward()\n",
    "        optim_nn.step()\n",
    "   \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_train)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(net_nn(X_train), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 30.\n",
    "        train_acc /= (30. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_test)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(net_nn(X_test), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 30.\n",
    "        test_acc /= (30. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            _, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X.shape[0], -1)\n",
    "y_pred = net_nn(X_test)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marginal_density = lambda x : (0.3 * normal_density(mu=-2., std=1., n=1)(x) + 0.7 * normal_density(mu=2., std=1., n=1)(x))\n",
    "#marginal_density = lambda x : (normal_density(mu=0., std=2., n=1)(x))\n",
    "\n",
    "pdf = []\n",
    "for _ in np.linspace(-10, 10, 1000): \n",
    "    _ = torch.tensor(_, dtype=t_type, device=device)\n",
    "    pdf.append(marginal_density(_))\n",
    "    \n",
    "from pynverse import inversefunc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "cum_density = interp1d(np.linspace(-20, 20, 250), [quad(marginal_density, -20, x)[0] for x in np.linspace(-20, 20, 250)])\n",
    "inv_cum_density = inversefunc(cum_density)\n",
    "real_samples = [inv_cum_density(np.random.random()) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean = quad(lambda x : marginal_density(x) * x, -20, 20)[0]\n",
    "var = quad(lambda x : marginal_density(x) * (x - mean) ** 2, -20, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7999999999999998, 4.359999999999999)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff6d7673518>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd4XNW59/3vrS5LVrXc5CYbdxsXbEMgmBobSIzhHIoJJ7Sch+QhpAEnIZCXBBJOIAkhz5NDCLxvDAmdQAgGTMC4UIKb3LstC9kqtiSryyojadb7x4yk2Vsz9siaJs39uS5dnr322jNLo/FPS2uvvbYYY1BKKRUdYsLdAKWUUqGjoa+UUlFEQ18ppaKIhr5SSkURDX2llIoiGvpKKRVFNPSVUiqKaOgrpVQU0dBXSqkoEhfuBtgNGTLEjBs3LtzNUEqpfmXLli0njDE5p6sXcaE/btw48vPzw90MpZTqV0TkiD/1dHhHKaWiiIa+UkpFEQ19pZSKIhr6SikVRTT0lVIqimjoK6VUFNHQV0qpKKKhr1Q/oLc1VYGioa9UBNtbVs+iJz9m0k/f52dv76bDqeGv+kZDX6kIVdfUxm3PbaKuuY0rZ4zgL+uP8LtVB8LdLNXPRdwyDEopl6fWFVDZ2Mo7d3+ZGbnpJMbF8KePC1k2fwyjswaFu3mqn9KevlIRqOakg+c/L+LaObnMyE0H4N5Fk4kR+PNnX4S5dao/09BXKgK9vb0UR7uT//zy+K6y4elJLJk1ktc2F9PkaA9j61R/pqGvVAR6Y2sJM3LTmDYyzVJ+3TmjaG7rYM3+ijC1TPV3GvpKRZji6iZ2l9Zzzexc1q07yvnnv8ySJX+npKSBc/OyyRmcyLs7joW7maqf0hO5SkWYtQdcvfhcE8fixW/icHQAUFj4Blu3foPF04fx5pZSWts7SIyLDWdTVT+kPX2lIszqfRWMzUzmpz/8uCvwAfbureIXv9jARZOG0tzWwdYjtWFspeqvNPSViiBtHU42fVFNzI56tm4t77H/scc2klTXTmyM8OmhyjC0UPV3GvpKRZDdpXU0nGjm078d9rq/o8Nwz/fWMHtUOv86XBXi1qmBQENfqQiyuaiapkONdLR3L7eQmhpvqZOfX05eQiJ7SutoaeuwP4VSp6Shr1QE2fRFDQm11jn4Dz54HhdfPNpSFlPpoN1p2FVaF8rmqQFAQ1+pCOF0GvKPVOM80Wopnz9/OBdemGspqzxcD8C2ozUha58aGHTKplIR4kh1EzX1rVSXnLSUz5kzlNZW6zDO9i0VjLlptM7gUb2mPX2lIsTu0jocla10dHSP548dm0ZWVjLnnjvCUnfnzkpmDh3M1qM1uta+6hUNfaUixJ6yejoqrUM7c+YMBSA7O5lJkzK7yp1Ow+BGJxUNrVQ2WI9R6lQ09JWKEHvK6kiutw7jdIY+wHnnWXv7DcWuYaA9x+qD3zg1YGjoKxUBjDHsKaunrcLaa587d1jX4/POG2nZd3Sfazx/b5mGvvKfX6EvIleIyAERKRCR+73sv0dE9orIThFZLSJjPfZ1iMh299eKQDZeqYHiWF0LVQ2tVB1ttJSfqqe/edMxRmUmsU97+qoXThv6IhILPAVcCUwDbhKRabZq24B5xpizgTeAX3vsazbGzHZ/XR2gdis1oOwpq6et2oHDY5ZOTk4yI0emdm3PnJlDcnL3hLvy8ibGxCewV0Nf9YI/Pf0FQIExptAY4wBeBZZ6VjDGrDXGNLk3NwCjAttMpQa2PWV1tJW3WMrmzh2GiHRtx8XFMH/+cEudxJoOvjhxUm+qovzmT+jnAsUe2yXuMl++CbzvsZ0kIvkiskFErvF2gIjc6a6TX1mpi0ip6HPgeANJddbg9hza6TRrVo5lWxraMQb2H28IavvUwOFP6IuXMq8Tg0XkP4B5wG88iscYY+YBXwd+LyITejyZMc8aY+YZY+bl5OTYdys14B2qaKSxwDqef845w3rUmzAhw7LdVOU68VtQ0dijrlLe+BP6JYDnwh+jgDJ7JRG5HHgQuNoY0zUFwRhT5v63EFgHzOlDe5UacBztTg4erKb2WFNXWVxcDJdfPrZHXXvol5c2khAXo6Gv/OZP6G8GJopInogkAMsAyywcEZkDPIMr8Cs8yjNFJNH9eAhwAbA3UI1XaiA4UnWSxoPW4ZmFC0eRkZHUo6499AsP1zJ+SIqGvvLbaUPfGNMO3A18AOwDXjfG7BGRR0SkczbOb4BU4G+2qZlTgXwR2QGsBR4zxmjoK+XhUEUjTYetoX311T1GQQHIy0u3bBcXN5CXNUhDX/nNrwXXjDErgZW2soc8Hl/u47jPgZl9aaBSA92Ogmpai5ssZUuWeA/9pKQ4cnNTKS11hbwxkOmMobimiZa2DpLi9Z656tT0ilylwmztR0csUyOmT89m/PgMn/XtQzwJTU6MgcOV2ttXp6ehr1SY7fr8uGX76qvPOmV9e+h31LYBOoNH+UdDX6kwKiltoHyX9UYovoZ2OtlDv76imRjR0Ff+0dBXKowee2IzxmP9/PHj03usnW83frz1ZG7RF3WMzdYZPMo/GvpKhcnJkw7+uny3peyee+YRE+Pteshu9p7+4cO1TMhJ4YsTJ30coVQ3DX2lwuS553bTUOfo2s7MTOK226af9rgec/UL6xiTNYiiqpM4nXoXLXVqGvpKhUFzcxtPPJFvKbvrrtmkpCSc9tisrCTS0xM9nqudTImlpc11Jy2lTkVDX6kweOihf1FU1L0kckJCLHff7d8KJSLSY1w/psG1WFtRlQ7xqFPT0FcqxDZsKON3v9tiKbvzzrMZPjzF7+ewD/G0VruGiYp0XF+dhoa+UiHU2trO7bf/0zL2npGTxKOPfrlXz2MP/erjTcTHCkVVTT6OUMpFQ1+pEHrmmZ3s319tKbv7Z+eSlpbo4wjvzjrLGvoH9lczOmuQ9vTVaWnoKxUiTU1t/Pd/b7CUpZ6dzjVfO/XFWN7MmDHEsr1jRyV52Sk6pq9OS0NfqRB5+untlJd3D78kJMWSsXAo47L9H8vvNHPmEDzupEhBQQ0jUhI5UtWEMTptU/mmoa9UCDQ2OnjssU2WsnlXjiEjO4ksP6Zp2qWkJHDWWZld28ZAXF0bzW0dOm1TnZKGvlIh8NxzuzlxorlrOzU1npELh5E3JMVy8/PesN8vt/GY68bqOq6vTkVDX6kQ+PTTEsv2979/DsdaHWc0tNPp7LOtoV9Z5Lr71hGdwaNOQUNfqRDYt886Y2fR4nGU1TYzNnvQGT+nvadfsL+a+FjhCz2Zq05BQ1+pIGtvd3LwoHX55IwRg3AaGJ0ZuNDftesEI9OSOFqtPX3lm4a+UkFWVFSHw9HRtT106CAacAIwKjP5jJ93zJg0yxo8DQ0OMk0MJRr66hQ09JUKMvvQztSpWZTUuIJ5dNaZ9/RFpMe4fmxNO8U1zT6OUEpDX6mg27evyrI9dWo2xdWuu10NT0/q03Pbh3haKlqoPungZGt7n55XDVwa+koFmX3ZhSlTXD39EenJxMf27b+gPfSrj7runlWivX3lg4a+UkHmtadf08zorDMfz+9kD/3SQtdyzcU6rq980NBXKoiMMT16+p1j+qP6MHOn+7myLdtlxQ2YdifFNRr6yjsNfaWCqLy8idra7mURUlLiGTJsEOX1rX2artkpNTWBUaMGd213dBhiGzsortbhHeWdhr5SQWQf2pkyJYtjda7lEvoyXdP+nJ4Gtxrt6SufNPSVCqKeQzvZXVMq+zJd05M99OMbOnRMX/mkoa9UEHnr6XfO0Q9WT7+tykFJTbMusay88iv0ReQKETkgIgUicr+X/feIyF4R2Skiq0VkrMe+W0XkkPvr1kA2XqlI5+3CrOLqZuJjhWFpfZuj38ke+g3lzTS2tlPb1BaQ51cDy2lDX0RigaeAK4FpwE0iMs1WbRswzxhzNvAG8Gv3sVnAz4BzgQXAz0QkE6WiREGBdc2dzp7+yIxkYmPObEllu8mTraFfXtyIMTqur7zzp6e/ACgwxhQaYxzAq8BSzwrGmLXGmM5P2AZglPvxYmCVMabaGFMDrAKuCEzTlYpsTqehtLTRUjZ2bJprjn4AZu50ys1NJSUlvmu7qbGNjpPteoGW8sqf0M8Fij22S9xlvnwTeP8Mj1VqwCgvP0lbm7NrOzMziZSUBEprmgI2ng+uNXjsQzztVQ49mau88if0vf0N6vUMkYj8BzAP+E1vjhWRO0UkX0TyKysr/WiSUpGvuLjBsj169GCaHO2caHQEbOZOJ3voxzV06PCO8sqf0C8BRntsjwLK7JVE5HLgQeBqY0xrb441xjxrjJlnjJmXk5Nj361Uv+Qt9EvdQy6B7OmDl2mbje16gZbyyp/Q3wxMFJE8EUkAlgErPCuIyBzgGVyBX+Gx6wNgkYhkuk/gLnKXKTXgeQv94q7pmsHt6bdXO7Snr7yKO10FY0y7iNyNK6xjgeXGmD0i8giQb4xZgWs4JxX4m/smz0eNMVcbY6pF5Be4fnEAPGKMqfbyMkoNOEeP1lu2x4xJ6zq5OjrAPX37DJ7G8hZKappxOg0xAZolpAaG04Y+gDFmJbDSVvaQx+PLT3HscmD5mTZQqf7KW0//SHUTiXEx5AxO9HHUmZk4MRMR6Lweq7aimcFN7ZxobGVogK4HUAODXpGrVJB4C/2SmmZyM5Nx/0UcMElJceTlpVvK2mscehct1YOGvlJB4mtMP5Bz9D3Zl1luq2rtWvJBqU4a+koFQVtbB8eOWS/Mys1NpaSmOeAzdzpNnWpbg+dEq16gpXrQ0FcqCMrKGvFc72zYsEE4jKG2qS3gc/Q7TZli7elLvV6Vq3rS0FcqCHyN50Pg5+h3svf0O6odOryjetDQVyoIeoZ+WteyCKEa0z9Z0ULxCQ19ZaWhr1QQ2Ofoh6Knn5mZxLBh3b9QnB2GoqI6nE5dV19109BXKgh8zdwZlBBLVkpC0F7X3ttvqmjhRGOrj9oqGmnoKxUEvsb0R2cOCvgcfU8976LVqnP1lYWGvlJB4LWnXx3YJZW96TlXX0/mKisNfaWCwB76o0alUlrTHLTpmp16zNWv0rn6ykpDX6kAa25u48SJ7qCNiREGZSTS0Noe8p5+e7XeTEVZaegrFWDHjp20bI8YkcLxBtfJ1EAvqWyXm5vK4MHdJ4qdrU4OFdUG9TVV/6Khr1SAHT/eM/Q7e9vB7ul7u3Xi4QMa+qqbhr5SAWYP/eHDU7rX0Q/ymD7AtGnWIZ7SQp2rr7pp6CsVYN5Cv7imicFJcaQnxwf99efMGWrZbj7WrHP1VRcNfaUCzFdPP1jLL9jZQ99R3qK3TlRdNPSVCrDjx60BO3x4Skjm6HeaPdsa+m3VDg6XNfioraKNhr5SAWbv6Q8bNsjV0w/BeD5AWloiZ52V0V1gYNPW8pC8top8GvpKBZg99JPTE2hu6whZTx9g7txhlu1dOypC9toqsmnoKxVg9tDvSHL9NwvVmD70HNcv3K/TNpWLhr5SAWSM6RH6LXGuf0dlha+nf7yw3kdNFW009JUKoJqaFtranF3bqanxnGhtA4J/Na4ne0//5PFmWlraQ/b6KnJp6CsVQL6ma2YOiic1MS5k7cjJGcSoUYO7tk2H4V/5x0L2+ipyaegrFUC+p2uGrpffyd7b/2R9acjboCKPhr5SAeSrpz8mRNM1PdlDf+t2ncGjNPSVCihvc/RLa5pDehK3k30NnqIv9GSu0tBXKqDsoZ+amYijwxnS6Zqdxo9Pt2wfL9GrcpWfoS8iV4jIAREpEJH7vexfKCJbRaRdRK6z7esQke3urxWBarhSkcge+nEprgXWQnU1rqe8PGvo15Q3Y4yuthntThv6IhILPAVcCUwDbhKRabZqR4HbgJe9PEWzMWa2++vqPrZXqYhmD31ncueFWaEf3snOTrbcUKXD4aSiQhdei3b+9PQXAAXGmEJjjAN4FVjqWcEYU2SM2Qk4vT2BUtHCHvqt8SACuWEIfRHpMcRTUKBX5kY7f0I/Fyj22C5xl/krSUTyRWSDiFzTq9Yp1c/YQ78hxjBscBKJcbFhac/48RmW7R37ToSlHSpy+HO1iHgp683A4BhjTJmIjAfWiMguY8xhywuI3AncCTBmzJhePLVSkaOtrcNyQ3SAamc7o8Mwc6eTvae/a19VmFqiIoU/Pf0SYLTH9iigzN8XMMaUuf8tBNYBc7zUedYYM88YMy8nJ8ffp1YqolRWNuN5nnTIkGTK6lrCMnOnkz30DxbUhKklKlL4E/qbgYkikiciCcAywK9ZOCKSKSKJ7sdDgAuAvWfaWKUimbc5+sfqWxgVhpk7newzeI4W6Vz9aHfa0DfGtAN3Ax8A+4DXjTF7ROQREbkaQETmi0gJcD3wjIjscR8+FcgXkR3AWuAxY4yGvhqQ7KGfMSQZY8Izc6eTfUy/vLQxTC1RkcKvFaCMMSuBlbayhzweb8Y17GM/7nNgZh/bqFS/cOyYNfQHpbumS4Zjjn6nsWPTEKFr2KmhupWWlnaSkkK3+JuKLHpFrlIBUlZm7UUnpofvwqxOSUlx5OZ2r7aJgSNHdIgnmmnoKxUg9tBnUBzxscLwtKTwNMjNfjL38GGdqx/NNPSVChB76LclCiMzkomN8TbrOXTsob9Tp21GNQ19pQLEHvonY01Yp2t2sp/M3b1fQz+aaegrFSBlZdYTudU4wzqe38k+bbNAh3eimoa+UgHQ0eHsMWWzMcYZ1qtxO9mHd4r1RG5U09BXKgDKy5twOrsvx83ITETiYiJieOess6zDO5WljbrEchTT0FcqAOzj+Zk5rh5+JAzv5OQMIi2te4nltlZnz5lGKmpo6CsVAPYQTclIBMJ7NW4nEWHixExL2aFDugZPtNLQVyoA7KEflxZPamIcWSkJPo4ILfsQz4EDGvrRSkNfqQCwh74zKYax2YMQCe8c/U72nr4usRy9NPSVCgD7dM3meBiXnRKm1vRkD/09Olc/amnoKxUA9p5+Q4xhbHb4T+J2sod+oc7Vj1oa+koFQI/ZMCmxjBsSST1965j+saMNlimmKnpo6CsVAPbQj02Ni6jhnezsZDLcM4oA2hxOSksbwtgiFS4a+kr1kcPRQWVl971xRSA2JY5xETS8433apg7xRCMNfdVv7D9ez4Nv7eIbf97Iw+/soejEydMfFAL25RdSMhIZlBhHzuBEH0eEh87VV6Chr/qJlzceZckfPuPvW0upa27jpY1HWfz7T3hnR1m4m9bz5ilp8RE1XbOTfVz/4MHqMLVEhZPeM01FvLe3l/LAW7u4eHIOT94wm8yUBMrrW/juy9v4/qvbGJQQy2VTh4WtfT1vnhJLXgSdxO3Uc9qmhn400p6+imhHq5p44O+7WDAui2e+cQ6Z7itch6Ul8fwd85k6Io0fvradstrm0zxT8PS8eUoMYyPoJG4ne+gf1OGdqKShryKWMYYfv7mTmBjhyWWzKTxUy/XXr2DmzOd57LGNJMTE8Meb5+LocPKLd/eGrZ2lpdbQl5TYiDqJ28m+FEPJkXo6Opxhao0KFw19FbHW7K9gfWEV9142keef2sHs2X/ljTcOsnv3CX7yk0+56KJXobGd7146kfd3H+eTg5VhaWdhYZ1lO3ZwfET29LOyksnK6r5fr2vapq62GW009FVEcjoNj72/n7zsQXz4p7389Kef4XB0WOp8/nkZc+a8wLzUFEZlJvPEhwfCsk78gQPWsfH47ATGDYm8nj7oDB6loa8i1Or9FRyqaGRqXQyvvLzfZ72amhaWLnmL6yYNZ0dJHR+HuLfvdJoewZk6NIlhg5N8HBFe9iGeggKdqx9tNPRVRPrzZ4WkNxmW/3arpTwnJ5lLLhltKSsvb+J/fvQ5ObGx/HHd4VA2k9LSBpqa2ru2EwbFMWF0GjExkTVds1PP0NeefrTR0FcRZ3dpHesPVVHxdgmtrd1DOqmp8Xz22U2sXn0DP/zhOZZjDhfUEr+tnk1fVLP/eOjuAWtflz5xSCJnDRscstfvLR3eURr6KuL8dX0RrXvqOH7UepLxmWcWMWlSFiLCb397MTffPNWyf8fHZcQ54YX1R0LWVvsFTmZwHBNyUkP2+r111lnW0N9/UEM/2mjoq4jS5GjnnS2lNK63rvd+663T+frXu0M+JkZYvvwKRo3q7lU3NbUztS2Ot7aVUt/SFpL22nv6cVkJnDU0kkPfOrxT9EWdrrYZZfwKfRG5QkQOiEiBiNzvZf9CEdkqIu0icp1t360icsj9dWugGq4Gpn/uPs7xDSc4WevoKktOjuNXv7qwR92EhFhuuGGSpaxpfz1Njg5W7jwW9LZCz5k7cVkJTMiJvOmanbKykiyrbba2dOhN0qPMaUNfRGKBp4ArgWnATSIyzVbtKHAb8LLt2CzgZ8C5wALgZyKSiVI+vPxpEY2brL387353DiNGeO8933jjFMv2v9aWMGZwEn/fWhq0Nno6aBseSchOYPyQyO3pi4iezI1y/vT0FwAFxphCY4wDeBVY6lnBGFNkjNkJ2C/vWwysMsZUG2NqgFXAFQFotxqAjtU189Gbh2lv6j55m5aWwI9+tMDnMfPnDycvL71ru7W1g7yTMWwqqqa4uimo7W1paaeoyOPCLIExeekkJ8QG9XX7yn4yV6dtRhd/Qj8XKPbYLnGX+aMvx6oo89amYho2W4dL7r13HtnZyT6PERFuvHGypax0ywkA/rEtuL39w4dr8bwWLDkjgcm5aUF9zUDQufrRzZ/Q9zbh2N8zP34dKyJ3iki+iORXVobnUnoVfs88vQNnS3cvPz09ke9//5xTHOFiH+JZt/oo01KSeWtbaVCv0LWP50tGfETP3Olkn8GjC69FF39CvwTwvBpmFODvIuZ+HWuMedYYM88YMy8nJ8fPp1YDSdHxBnZ/UGIp+9735pCefvobkcyalcP06dld2x0dhsYNVRSeOMmesuDN2bfP3InNjOyZO53sPf19usRyVPEn9DcDE0UkT0QSgGXACj+f/wNgkYhkuk/gLnKXKWXx/zy+AWeT9UIsf3r54BrieeCB8yxl//rnUZy1Dt7fHbxZPPY5+q6ZO/0v9Iu+qA3LmkUqPE4b+saYduBuXGG9D3jdGLNHRB4RkasBRGS+iJQA1wPPiMge97HVwC9w/eLYDDziLlOqS3u7k7+/YF1f5667Zp9yLN/uxhsnM3VqVte202mI29HAyl3HgxZo+2095PgIn6PfKSdnEGlpCV3bLc0dPW75qAYuv+bpG2NWGmMmGWMmGGMedZc9ZIxZ4X682RgzyhiTYozJNsZM9zh2uTHmLPfXc8H5NlR/9vzLe2mqau3aTkyM5Z575vXqOWJjY3joofMtZUWbKjhYUMP+4w0Baaen9nYnO3ZYzz8NHzuYrJQEH0dEDte0TV2OIVrpFbkqrIwxPPb4JkvZLbdMZ9iw3l/gdP31k5g2rXts3zjh5K5a3t8V+CGeffuqaG7uXmgtcXA8MydlneKIyGIf4rFfb6AGLg19FVafflrC4b3WYZJ77vFvLN8uNjamx0Jsjr0NvBeEq3Pz849bX3tYItNGpvuoHXkmT7b29O0zkdTApaGvwuqJJ/It20uWTGDKlGwftU/vxhunkJIS37XdXOtg98ZyDpUHdognP7/csh0/LIkpwyN3dU27yZOtf5XoDJ7ooaGvwqax0cH7//zCUnbvvb0by7cbPDiBG26wXqzVuLOW9wI8xGPv6ScMT2LK8Mi/MKuTPfT37KvyUVMNNBr6KmzWri2mzdG9cseECRksXDiqz8/7zW/OtGw3FzSyYkOxj9q953B09DiJO2hkMhOGRu5Ca3aTJlmHd4qP1NPW1uGjthpINPRV2Lz/fqFl+8or8xDp+x2nzj9/pKUna5yG7WtLKawMzGqSe/acsNzcZVBmApPzMkmMi+w1dzylpSUycmT39NKOdtPjBu9qYNLQV2FhjOHd96yhf8UV4wLy3CLCHXfMsJSd3FvP+7uP+ziid+zj+YnDk5kyov+M53fSk7nRSUNfhcWhQzUUH+0+uZqYGMvFF48+xRG9c/PNU/H8o8FxvIU3Vn/h+4BesI/nO4ckMG1E/xnP72Qf19fQjw4a+ios3n/fGsALF44iJYAXNuXmDuaSS8ZYyratKQnIcss9e/pJzBqd4aN25LKHvv0KYzUwaeirsFhpC/0rrsgL+GvY76F7cl99ny/UqqlpYedO60ncxBFJzMjtP3P0O9mHd3btPRGmlqhQ0tBXIdfc3Ma6ddbZNFdeGfjQ//d/n0RiYvfJ1fbaNl5651CfnvOFF/bS3t494yhtWDKTx2aQmhjXp+cNB3tP/5BelRsVNPRVyK1ffwyHx+yXMWMGM2VK4JcwSE9P5GtfG28p2/pRCcfrWs7o+YwxPPvsDktZ6ox0zh7V/4Z2AMaOTbP8UqytbqWm5szeG9V/aOirkFu37qhl+7LLxgZkqqY3N99svZ1z4+46Xl5d6KP2qa1fX8aePd0XMcXGCjI5lVmj+9/QDriWrbCvwaMncwc+DX0Vcv9cdcSyHchZO3Zf/ep4xo71mFnTYfif3205o+d65hlrL3/BRbnEDY7vtz190Bk80UhDX4VUU1Mb27ZYZ79cdFHfr8L1JSEhloce+pKl7IvPy9m4vdzHEd7V1LTw+usHLWUTLxpBQmwMU/vhHP1OPZZj2KPLMQx0GvoqpDZsOEZ7W/eJ0Ly8dMaODe7wyC23TLcuO2Dgnh9/3Kvn+MMfttLS0r2U8tixadRkxDB7dEa/uhLXbsaMIZbtjfmBuYBNRS4NfRVSq1Zbh3aC2cvvFBcXw8MPX2Ap+3zVUXbtqvRxhFV9fStPPmkdErr9P2ey53gD547vP2voezN7tvWe1Dt2VOitEwc4DX0VUis/tM7PD+Z4vqcbbpjMzJkevVoD9z/wqV/HPvXUdmpru+/slZGRyIKrxtDhNCzI69+hP3lyFsnJ3dNN66pbKSsLzBpFKjJp6KuQaW5uY892a+/6ootCE/oxMcIvf/llS9nKdwvZuPHUF2udPOngd7+zrvn/gx+cw+7KBmJjhLljMn0c2T/ExsZw9tnW3v62bRVhao0KBQ19FTLr15fR0d49dDBuXBrjxoVuuuO60ewGAAAU7UlEQVSSJRM499wRlrIHHvj0lMMZjz66kRMnmru209IS+N735vL54Spm5qaT0g8vyrKbM2eoZVtDf2DT0Fch8/YH1qGdUPXyO4kIjz5q7e2vWXOU3//e+xTO9evLeNx2/97vfncuJiGG7cW1XDw5x+tx/Y099Ddv0ZO5A5mGvgqZ1balF7785dyQt+Gyy8Zy6aXWhdjuu+9jPvrIeoK5srKJW25ZidPZ/VfAyJGp3HvvPD45WIkxcMlka1j2V/bQ37Kld9NZVf+ioa9Cwuk0HNxlXdDrggtCH/oATz99OWlp3St6Op2GG254h/Xryzh4sJpbblnJ6NHPUFBQazlu+fLFZGYmse5ABdkpCczsh4useTNzZg6xsd1XRJeVNOpyDAOYhr4KiU3bymlr7l5vJzMzqceFQaEyaVIWr7zyNct6+zU1LVx88WtMn/48L7yw13JnLIC77prN4sV5tHU4WXewkosm5xATE5ylI0ItKSmOqVOtN6Pfvl3H9QcqDX0VEi+9bb2a9UtfGhHW0LzqqvH80ja+73B0WFbQ7HTOOcP49a8XAvDJwUpqm9r46swRPer1Z3oyN3po6KuQWPOJdTz//PPDM7Tj6Sf3n8uimyf53J+Xl87jjy/kk0+Wdd3g5R/by8gcFM/CSQPjJG4nHdePHv1/vpmKeE6n4fBu65ou558/Mkyt6SYivPj0YmY2nOTEyu7ppGPHpvHYYwu54YbJlr9GGlvbWbX3ONedM4r42IHVX7KH/qf/Kg1TS1SwaeiroPt453Faqxxd27GxwoIFw8PYom45gxO5/dYZvDw8iatT0pkzI4evf30qSUk9/2u8kV9MS5uT684J7VTTUJg/fzjx8TG0uddFKj5ST3FxPaNH9797/6pT86u7IiJXiMgBESkQkfu97E8Ukdfc+zeKyDh3+TgRaRaR7e6vPwW2+ao/eHGF9W5Vs2cPDej9cPvqe5edRWJOEgnnZXPHHTO9Bn57h5PnPi9i7pgMZvfD++GeTkpKQo8L19auLfZRW/Vnpw19EYkFngKuBKYBN4nINFu1bwI1xpizgCeBxz32HTbGzHZ/fTtA7Vb9yMef2sfzwz+042lEejK3nz+ON7eWsLHQ+9LCr+eXcKSqiW9dNCHErQudSy6x/gWzZs1RHzVVf+ZPT38BUGCMKTTGOIBXgaW2OkuBv7gfvwFcJsG6FZLqV0pqmijea53vHgknce2+f/lExmQN4kdv7qS+pc2yr6KhhSc+PMD8cZksmjYsTC0MPvtFa2vWHNUVNwcgf0I/F/DsqpW4y7zWMca0A3VA58TfPBHZJiIfi8iFfWyv6mf+sakEx7FmS1kollPurUEJcfz2+lmU1jRz51/zqWt2BX9DSxvfeWkrJx3t/PKamUG7rWMkOO+8kZZ75hYXN1BYWBfGFqlg8Cf0vX3K7b/+fdU5BowxxswB7gFeFpEeZ4ZE5E4RyReR/MpK/9Y4V/3Da+8csnxapk7NYsSI1PA16BQW5GXx2+tnkV9Uw6InP+bHb+zkqv/7KduO1vKb62YxeXj/vUOWP5KS4noMva1dq0M8A40/oV8CeA72jQLKfNURkTggHag2xrQaY6oAjDFbgMNAj4nRxphnjTHzjDHzcnIG1vznaFbV2MrOTdb53vYhhEhzzZxcXvvWl5gyPI1V+8oZkZbMi/95LktmRdZ5iGCx/3z0ZO7A48+Uzc3ARBHJA0qBZcDXbXVWALcC64HrgDXGGCMiObjCv0NExgMTgcKAtV5FtI/2ldN85KSl7JJLIjv0Ac4Zm8lf7lgQ7maEhevn86+u7dWrj+B0mgGz5ITyo6fvHqO/G/gA2Ae8bozZIyKPiMjV7mp/BrJFpADXME7ntM6FwE4R2YHrBO+3jTHVgf4mVGT6x4Zi2ipaLWWhulOWOjPz5w8nJSW+a7u8vEln8Qwwfl2cZYxZCay0lT3k8bgFuN7LcW8Cb/axjaofamhpY926EkvZ7NlDyc5ODlOLlD8SEmK59tqJvPji3q6y5ct3cfnlY8PYKhVIA+tachUxVu0tp/EL671WL71Ue/n9wR13zLBs//3vh3Sp5QFEQ18FxVvbSmk/2mQpi/STuMrlootGM358970CWls7eOWVfWFskQokDX0VcJUNrazbUEpLVfd4fnx8DBdeGHnz81VPMTHC7bdbe/vLl+8OU2tUoGnoq4BbuesYDfvqLWWXXz6WtLTEMLVI9datt0633GRmy5ZyvbHKAKGhrwLuH9tL6Thsnap53XW+161XkWf06DQWLRpnKXvqqW3haYwKKA19FVBHq5rYtLWcxuPdSy/ExgpLl54VxlapM/Gtb82ybL/00j49oTsAaOirgHo9v5imgw2WsksvHaNTNfuhJUsmMHREStd2c3M7zz+vY/v9nYa+Cpi2Diev5xcTU2SdtaNDO/1TXFwM3/nfsy1lTz21HadTV97szzT0VcCs2V9B8YFaqou7x/NjYoRrrtGhnf7q2986m5i47jO6hw/X8s9/fhHGFqm+0tBXAfPKpqO0bLausnHZZWMYOjTFxxEq0g0dmsKFi61X4/7qVxt1nf1+TENfBcTRqiY+/LSY2v3WqZo/+lF0Llw2kDzywJcs2599VsrHH+vqm/2Vhr4KiD9/VkjDeuutBs89dwSXXaZX4fZ3C8/PZdTMLEvZL3+5IUytUX2loa/6rOakg7++V0DjAWsv/6c/PW9A32kqmtz1w7mW7dWrj7J+vf22Gqo/0NBXffbC+iLK3iu13CFr1qwcvvrV8WFrkwqsH9w8k5Rx1nMzDz74qY7t90Ma+qpPmhzt/Pb/bMFRZr0P7iOPXKC9/AEkOSGWa++YZilbu7aYFSsOh6lF6kxp6Ks+efLtfZR8eMxStnTpWSxZMiFMLVLB8uM7ZpFk6+3fd986HI6OMLVInQkNfXXGjpQ18Mu7P8Y4nF1lqanx/OEPl2ovfwCakZvBeTdNAI8fbUFBLf/zP7omT3+ioa/OyIkTTVz0lddpqbCuxfLooxcyenRamFqlgu2/bppJ6qwMS9nDD3/OsWONPo5QkUZDX/VKQUENt932PrmjnuHI3hrLvq99bTzf+c5sH0eqgWDx9OHMWjqWuKTYrrL6egf33LMufI1SvaKhr/zS0ODg5z//FzNmPM9f/rIHR6t1HPfCC0fx+utLiI3Vj9RAFhsjfO+rUxh8wRBL+auv7mfVqqLwNEr1iv4PVT45HB088cRmFix4kczMP/Dww+tpbe150m7u3GGsWHENycnxYWilCrV/m5vL3CtGkzLSunLq//pfH1JQUOPjKBUpNPSVVzt2VLBgwYvcd9/HbN58nI6OnvOx8/LS+e1vL+Kzz5aRkZEUhlaqcIiLjeFnS6cz+LJhlrtrHTlSz7nnvsQnn+gSDZFMQ1/18Npr+5k//0V27Kj0uj8uNY6nnv0Khw59k3vvna89/Ch04cQcllw+jrRzrMszVFe38JWvvMHrr+8PU8vU6cSFuwEqsqxefYRvfGMlbW3OHvvShicTM24Qr/1xMYvmjAxD61Qk+e9/m0l+YRXHjXBsS/e6Sw5HB8uWvUtRUT233TZdV1mNMBJpl1HPmzfP5Ofnh7sZUWnHjgouvPBVGhoclvJlN00h8fxs1pVU89/XzuTr5+oiasplzf5ybn9uM8MLW9n0hvd19idNyuTaaydy/fWTmDt3mF7DESQissUYM+909XR4RwGwZs1RLr74tR6B//QzXyFl8XDWlVTz069O1cBXFpdOGcYDV02lfEIS1/7wbOLiekbKwYM1PP74JubNe5G5c1/g+ed309LSHobWKtDQV8Dy5btYvPgNamtbLeX3//w83m6uZ83+ch6+ejr/eaEuoKZ6unPheO64II+tCW0s+a+zSUnxfY5n+/YKbr/9n+TkPMWyZe/w7ruHddG2ENPhnShmjOHhhz/n4YfX99h36XUTODopkdTEOJ68cTYXTx4ahhaq/sIYwx/WFPC7VQfJS0hgXLmT3Vsr2L69kvb2nueHPC1aNI6nn76c8eMzTllPnZq/wzt+hb6IXAH8HyAW+P+MMY/Z9icCfwXOAaqAG40xRe59PwG+CXQA3zPGfHCq19LQ958xhpKSBvbtq+bAgWra2pwMGzaIkSNTmTIli+HDU3yOn1ZVNXPffet4/vk9PfaNXjwSmZXG4unDefTameQMTgz2t6IGiA/3HOfHb+6koaWdm88dw01zRrP1szJeeWUf771XiK+4SUqK49//fSK33DKdSy8d43WYSJ1awEJfRGKBg8BXgBJgM3CTMWavR527gLONMd8WkWXAtcaYG0VkGvAKsAAYCXwETDLG+FyWb6CFfkFBDXV1raeveArGQGOjg4qKJoqK6tm7t4q9e6vYv7+6xxi8p4yMRKZOzWbatGwmTswgLy+dpqZ2tm4t57nndtPY2GapL3FC9tdGsnDRWO5dNInzJwzx8cxK+VbZ0MrvPzrIq5uLMcZw6ZShLJo2nHEJCbz217288sp+jh076fP47Oxkli6dwHnnjWTixAwGD07oc5vGjUsnOzv59BX7sUCG/peAnxtjFru3fwJgjPmVR50P3HXWi0gccBzIAe73rOtZz9fr9dfQ93wfOx8aYOnVb/Hee4XhaVQvxCbHcsvP5nHvN85m+sj0cDdHDQAlNU28sukob24p5Xi9a2G+EelJTBk2mPiqNj555RAHtp0ISVuef/4Kbr11RkheK1z8DX1/5unnAp6X2JUA5/qqY4xpF5E6INtdvsF2bK4fr9lr1ScdfPnxNbja4CozeA/i7kL/6nUGurXMv3ZV7K/wr2IY5WXUsPLmV5jSXAXPhrs1aqAYBfyX+4vOC7ZbgaOuh2YJ5Az/I7WfVdLRENzZPPe/uYtfFRQj+J4ueqqZpKGaZHr2qAxeufO8oL6GP6Hv7fu1R56vOv4ci4jcCdzp3mwUkQN+tMuXIUBoug+9E7Ht+qKWE1OfCnczeojY9wttV2+cpl3/EZJGHH/H9eUhIt+vvTDk1W+dcbvG+lPJn9AvAUZ7bI8C7HdE7qxT4h7eSQeq/TwWY8yzBKiPKSL5/vyJE2rart7RdvWOtqt3orld/pwi3wxMFJE8EUkAlgErbHVWALe6H18HrDGuMZEVwDIRSRSRPGAisCkwTVdKKdVbp+3pu8fo7wY+wDVlc7kxZo+IPALkG2NWAH8GXhCRAlw9/GXuY/eIyOvAXqAd+M6pZu4opZQKLr8WXDPGrARW2soe8njcAlzv49hHgUf70MbeitRTkdqu3tF29Y62q3eitl0Rd0WuUkqp4NHL3pRSKor0y9AXketFZI+IOEVknm3fT0SkQEQOiMhiH8fnichGETkkIq+5T1AHuo2vich291eRiGz3Ua9IRHa56wX9qjQR+bmIlHq07Sof9a5wv4cFInJ/CNr1GxHZLyI7ReQtEfG6EEuo3q/Tff/uyQmvufdvFJFxwWqLx2uOFpG1IrLP/fn/vpc6F4tIncfP9yFvzxWEtp3y5yIu/9f9fu0UkbkhaNNkj/dhu4jUi8gPbHVC8n6JyHIRqRCR3R5lWSKyyp1Dq0Qk08ext7rrHBKRW73V6RVjTL/7AqYCk4F1wDyP8mnADiARyAMOA7Fejn8dWOZ+/Cfgfwe5vU8AD/nYVwQMCeF793PgvtPUiXW/d+OBBPd7Oi3I7VoExLkfPw48Hq73y5/vH7gL+JP78TLgtRD87EYAc92PB+NaHsXerouBd0P1efL35wJcBbyP69qd84CNIW5fLK6VAsaG4/0CFgJzgd0eZb8G7nc/vt/bZx7IAgrd/2a6H2f2pS39sqdvjNlnjPF2AddS4FVjTKsx5gugANe6P11ERIBLgTfcRX8BrglWW92vdwOuNYj6iwVAgTGm0BjjAF7F9d4GjTHmQ2NM52WZG3Bd0xEu/nz/S3F9dsD1WbrM/bMOGmPMMWPMVvfjBmAfQbrCPQiWAn81LhuADBEZEcLXvww4bIw5EsLX7GKM+QTXzEZPnp8hXzm0GFhljKk2xtQAq4Ar+tKWfhn6p+BtyQj7f4psoNYjYIK2NITbhUC5MeaQj/0G+FBEtrivTA6Fu91/Yi/38SelP+9jMN2Bq1foTSjeL3++f8vSI0Dn0iMh4R5OmgNs9LL7SyKyQ0TeF5HpIWrS6X4u4f5MLcN3xysc7xfAMGPMMXD9Qge8rV8e8PctYu+RKyIfAcO97HrQGPO2r8O8lPm7ZESv+dnGmzh1L/8CY0yZiAwFVonIfnev4Iydql3A08AvcH3Pv8A19HSH/Sm8HNvnaV7+vF8i8iCuazpe8vE0AX+/vDXVS1nQPke9JSKpwJvAD4wx9bbdW3ENYTS6z9f8A9dFkcF2up9LON+vBOBq4Cdedofr/fJXwN+3iA19Y8zlZ3CYP8s+nMD1p2Wcu4fmdWmIQLRRXEtS/Buu+wz4eo4y978VIvIWrqGFPoWYv++diPy/wLtedvm1fEag2+U+SfU14DLjHtD08hwBf7+86MvSI0ElIvG4Av8lY8zf7fs9fwkYY1aKyB9FZIgxJqjrzPjxcwnKZ8pPVwJbjTHl9h3her/cykVkhDHmmHuoy9vqjCW4zjt0GoXrXOYZG2jDO6dd9sEdJmtxLRcBruUjfP3l0FeXA/uNMSXedopIiogM7nyM62Tmbm91A8U2jnqtj9fzZ+mNQLfrCuDHwNXGmCYfdUL1fvVl6ZGgcZ8z+DOwzxjzOx91hneeWxCRBbj+j1cFuV3+/FxWALe4Z/GcB9R1Dm2EgM+/tsPxfnnw/Az5yqEPgEUikukeil3kLjtzwT5rHYwvXGFVgmuh1nLgA499D+KaeXEAuNKjfCUw0v14PK5fBgXA34DEILXzeeDbtrKRwEqPduxwf+3BNcwR7PfuBWAXsNP9oRthb5d7+ypcs0MOh6hdBbjGLre7v/5kb1co3y9v3z/wCK5fSuBaLPhv7nZvAsaH4D36Mq4/7Xd6vE9XAd/u/JwBd7vfmx24ToifH4J2ef252NolwFPu93MXHrPugty2QbhCPN2jLOTvF65fOseANnd2fRPXOaDVwCH3v1nuuvNw3aGw89g73J+zAuD2vrZFr8hVSqkoMtCGd5RSSp2Chr5SSkURDX2llIoiGvpKKRVFNPSVUiqKaOgrpVQU0dBXSqkooqGvlFJR5P8HrDSn8axmrloAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(-10, 10, 1000), pdf)\n",
    "plt.plot(real_samples, np.zeros_like(real_samples))\n",
    "sns.kdeplot(real_samples, kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=20, n_hidden_dims=5, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot_projections(dm, use_real=True, N_plots_max=1, pdf=pdf)\n",
    "# plot_projections(dm, use_real=False, N_plots_max=1, pdf=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_projections(dm, use_real=True, kernel='tri', pdf=pdf)\n",
    "            plot_projections(dm, use_real=False, kernel='tri', pdf=pdf)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(torch.mean(dm.lt.transform(dm.particles, n_particles_second=True)),\n",
    " torch.mean(torch.std(dm.lt.transform(dm.particles, n_particles_second=True), dim=1) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_condition_distribution(dm, n_samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dm (DistributionMover): object contains unconditioned density, linear manifold and particles\n",
    "        n_samples (int): number of samples\n",
    "    Return:\n",
    "        (points, weight)\n",
    "    \"\"\"\n",
    "    points = torch.zeros([dm.n_hidden_dims, n_samples], dtype=t_type, device=device).uniform_(-6, 5)\n",
    "    weight = dm.real_target_density(dm.lt.transform(points, n_particles_second=True))\n",
    "    \n",
    "    sns.distplot(points.data.cpu().numpy(), hist_kws={'weights': weight.data.cpu().numpy()},\n",
    "             color='darkblue', kde=True)\n",
    "    plt.plot(dm.particles.data.cpu().numpy(), torch.zeros_like(dm.particles).data.cpu().numpy(), 'ro')\n",
    "    sns.kdeplot(dm.particles[0, :].data.cpu().numpy(), \n",
    "                kernel='gau', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=2, n_hidden_dims=1, use_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_condition_distribution(dm, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1200...\n",
      "Kernel factor 0.47011352887414687"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_condition_distribution(dm, 100000)\n",
    "            \n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd4VFXi//HPSYEAoUooApEmID0QQlG/ulYURL8qKkoPIraFr2113fW3ukV22WWxLYpUgVVBbNiRVdcChAnSe++Q0FtCyvn9EUaGZGZuO7fMnc/refI8QmbuPTOY95ycuXOvkFKCiIj8I8HtARARkVoMOxGRzzDsREQ+w7ATEfkMw05E5DMMOxGRzzDsREQ+w7ATEfkMw05E5DNJbuy0bt26smnTpm7smogoZuXm5uZLKdO0budK2Js2bYpAIODGromIYpYQYoee23EphojIZxh2IiKfYdiJiHyGYSci8hmGnYjIZxh2IiKfYdiJiHyGYSci8hmGnYjIZxh2IiKfYdiJiHyGYSci8hmGnYjIZxh2IiKfYdiJiHyGYSci8hllYRdCJAohfhZCfKJqm0REZJzKGftoAOsUbo+IiExQcmk8IURjAH0A/BnAYyq2SUTOuOKW70zd74f5VykeCami6pqnEwA8BaC6ou0RkQJmo61q24y/OyyHXQjRF8BBKWWuEOLqKLcbCWAkAKSnp1vdLRGFsDPgVkQal1PBLywuQWkpUKVSoiP78wohpbS2ASFeBDAIQDGAFAA1ALwvpRwY6T6ZmZkyEAhY2i9RPPPi8omVFxc7xlVSKvHIv5ch/2Qh3r6/B5ISY/8gQCFErpQyU/N2VsNebqdXA3hCStk32u0YdiLjjITTa0sgTo9dSomn563Cu4Fd+H3ftsi+opnlbXqB3rCrWmMnIsX0xtBrEQ8n3BgjPb7Qvzf72MZ+sR7vBnbh0Wta+ibqRiidsevFGTtRZFpBj4WQG6XyjdjXv9uCsZ+vx6Ael+CFW9tBCGF1eJ7BGTtRDInHmIcq//jCPR96ZvLv5OzE2M/Xo1+ni/F8P39F3QiGnchF0YLu95hHE3zsWss1oc/RZ6v24bcfrMLVrdPwj7s6ISEhPqMOMOxErnAj6GNeOWnLdkNNeDRV6fZCn4tos/gXx7fF6Hd+Rpf02ph4X1ck++AIGCu4xk7kIKeO63Yi4kaoDH7557CwSgnyLzmDpKIEBF68DjWrJCvbl9e4crijXgw7xRu7g24l5Gaj68Y+Q11xy3coqlyCg03PIKFEoN72KkgsLpup+3UZi2En8ohwUVcRHiNhVb1EosWJse06fBp3vv4T8g6dRb1tVZBUdOHyix/jzrATucyOWbqeYDodcb1Ujj3vRCH6v/4TjpwuwpwHeqJ1g+qun77ACQw7kYtUztK1gujVkEdjJfLHzhThnkmLsePQKcwa0R1d0mtf8H27fkPyAoadyAVOBd1szOcuLjV1P7369zB+NIqRF64zZ0sweOoSLN91FFOHdsOVl6ZFvJ8fA8+wEzlMVUhUBd3uiOthNPTRHvu4h6pi5FsBfLsxD68O6II+HRtqbs9vcWfYiRxUPiAqg253zM3Msu3eV/nnQkqJ3CPrsfvMQbx4ewcMyNJ/6m8/xZ1hJ3KIXVHXG3S9gTUbcKNUjmfMKychpcTKY5ux7dRetK3RDK2qp5tailLx7+Q2hp3IZipmgmaDrieeToVci9Wxjl+wES8v3ISWqY3RrkbzC87/YjTwsR53hp3IRm5FPVokjYR865Ytum+rR/MWLXTdTivy5R/DtB+34fn5a3F3ZhOMvaMD/u/VUxXuE09xZ9iJbGI16k4HXXXE9dKKvdZjen/Zbjw2ZwV6t2uAV+/NuOAKSFaWroDYjTvDTmQTK1EwGqRI8bMj5npn3aq3He4xrt13AG8t/Rk9mtfB1KHdUDmp4jVL4zHuDDuRDVRG3cwsPVrQ9QRXb7zN0Bv8SGMIPt4t+Ycw+acAGtaojpGXZ2HQlZUibive4s6wEynmVNSNztK1gmpnzKMxM67Ve47hjomLULNKCh66sgeqVSqLutZvKEZfNEPFUtwZdiKF3Iy6maBrxbzgyylRv29Uyo3ZUb+vZ6xb806i/+uLkJKciPce7Ikf1lWucNtogY+HuDPsRIq4FXWjQY8Wc9UhjyZa5CON/eDJIjz++V4UFJVg7qieaJ5W9jwZXY5SFXeG3QSGnWKJ2R94p6IeKehGYq414za7bT2RP1ZQjNHzd+LQqWK8O6oX2jeqWeG2ep8fwN9xZ9iJFDA7W1cddSOzdK3oGo24Xmb2u3rdRjz+6S5sO1KIv93UBB0bVtV8czXIjrh7fUlGb9i98dE0ohjgpag3b9GiQgALvpwSMa4pN2b/8mUXrX2UH1tBUQn+8sNhbD5ciP93bSN0bFgVQNnjDfeYyz830Y6DN3v2S6+F3CzO2IkiULEEY1fUy4sW9Gj2Pzkw6ve1NBg3K+r3I40r6bpheGj2Mny19gAm3N0Zt2U00v1Yzc7c/bAko3fGnuTEYIjihd5LwpmJuqqgW415pG2Fi3xwHKHjlBJ46pV5+Gp/Vfzhlra4LaMRgJCjY0Ie99YtWyo87v49Ei54/uYuLtV9QrFYvCiJGZyxE4Vh52zdzqhHCrremGvNwK1u58wXU/C3LdUxbVc1PNz0JB5tdjLsmPU8/tDnMV5m7ZyxEznMTEBURd1M0LUiruc+kba//8mBYbc/tfJVmLZrA+5rdAqPNC17vgq+nFJh/M1btND8gFPozD3arH3Co6m//NvEy6ydb54SlWPnTM3oxSnsiHqDcbNMRT3atsJtb/+TAy8Yw+wlOzDuyw24rfPF+OPD/RFy9t2wv32EPnY9pytQfcWo0H/7SBfK9iqGnUgBO2brZqNePqhBKoMeTrTAf7JyL3734Wpc26YexvXvhIQEUWHsWodL6jlSJpLQfxO974PEMi7FEEVg52zdyLHqQXqjXp6ZmH+a3PqCP/cp2qD7vsH9BceyGA3wxOwAujVLw2v3dUFyyOl3U27Mjhp0PUsyofS+kep3fAaIQpj5ldvsm3PRaJ3rRe/x6Hqi/mly6wpfWrfRu++VqIunE65EcxzHX7ZMQkpyxdPvhj4WM0sydobcK2+aGsWwEzlA9Ww9nPKzda2oG4l0pPtGu//6/cfxRLWbUBdn8M/Sb5GKIl1H1dh1XhuryzGxtM7OsBN5jJnZupmoh9OnaEPUL73b2nnoNAZNyUHVSkl4+zf9cBEKIo410mMKZWTWrvpN1FjEsBOFYeZXcLsOowudwepZgjET9Wjh1nO70G0ePF6AgVOWoKikFDOzs9CkTlVb37Slihh2IguM/kqvtR5s5qIYWp/+DBXuTVEjb4xGu9+nya1x7HQRBk/NQf7JQkwfloVL61fXHHOQ1lq7CmZefGNxnd1y2IUQTYQQ3wgh1gkh1gghRqsYGFG8cOti02aCHm0bhUmVMHzGUmzNO4U3B2eic5NaF9zW6qxd60WPR8Ocp+KZKAbwuJTyMgA9ADwshGirYLtEjrLrzTFVa75Wz8wYOltXEfXQbRUnJGJyvyFYtu0QXh7QGZe3rKts+2Sc5bBLKfdJKZed++8TANYBaGR1u0RkjFvr2CWlEm/ddC/WNrsM9y6Yi97tG7oyDqP8/EElpb+7CCGaAsgAsCTM90YKIQJCiEBeXp7K3RKRS6SUeO6j1chtk4HbvpuPXqsr/OhfgG+iOkNZ2IUQqQDmARgjpTxe/vtSyklSykwpZWZaWpqq3RKRi/7x1UbMXrIT1+csxPWBbzRvr/KUwVb5+WRgSsIuhEhGWdRnSynfV7FNIr9Q9aae1pEiRqJp9oNJoSZ/vxWvfrMZl69chFu//xSA2rV7Mk/FUTECwBQA66SU460Picgdbh3WZuYQRzNURve93N3406frkLFhOe75+j0I7bvYjh9MOk/FVOJyAIMAXCOEWH7u62YF2yXyPKO/zmvFx+qhj0Zn7WZm7l+t2Y/fzFuJNts3YMjns5Fw7mI9Wi8cRo6313PxDTP8fiqBIBVHxfwgpRRSyo5Sys7nvj5TMTgit1g9GZhKWh/cMfKGZKRPjeoN/E9b8vHQtMVI370N9388DcklJRG3G8rNc8TEI562l8hnIl29KCgY4fIx14r7jvqN8dJdDyHt+BE8+MFkpBSdvWB7eoUbm5FzsfODStr4DBA5QOskVVonuTI6a9czQzYS5P116uFft49E6plTeOS9N1Ct4LTu0xEYPRLGzAexuL5+IYadyCI3rs6jMu5agT5cvRZeufMBCFmKeX+8A/cdXao76HrOOmnmyklmmDlvvhcvaK0Hw04UwqnrXFqdtQP64653xhzuNL09jqzEtGf/ipK0NLz7TB80rVtN17b0XslJ7/Vbg8Itw2id6z4e8VkgcojR6KiKO2As8EEnCoowZFoO9h49g6lDu6HtxTU07xPteqvl6Ym6kbX1aOy4ypWXMexECphZjtGatQPW4m4l8AVFJbj/rQDW7zuBiQO7olvTOlFvb/QC2kajHomds/VYXYYBACHPHYPqpMzMTBkIBBzfL5FeZn6oywc90sywfND1XCov3GxV7zKGnpCHxre4pBSjZi3DwvUHMOHuzri1c8Vz+mltM9pROWairrUEA0QOu9nZuhfDLoTIlVJmat2OhzsSKTLh0VRds/X+PRI0j+Jo3qLFBXHbumVLhbil3Jh9QSSD/10+lKGRjRTk4N+XAviT6IGvE5rhidKl6D77beyfrfmQwu6rvHC/WdgddbO8GHUjuBRDFIaKN1GjRV7PNTrNLMsAZQGNdKRJpKURAJAAXhYZ+CyhGUaWrsSdcnOk4YfdZqTtRhqPE1GPt7X1IC7FEEVQPuhuLMkA+mIHRD5sUO9x4a8s3IR/LNiIu0s3YIxcFvb8L0Y+5WpkPOFetFRGHfDHIY56l2IYdqIozP6Q2x13QN+6e3mRIj9z8Q78/sPVuD2jEf7evxMSEsyd1iva/iPt22zUAX3r6kDsr60HMexECpidtQP6lwH0BivSUSJmAh+UcmM2Plq+B2PeXY5r29THxIFdkJyof4VWz36MBB1g1KNh2IkUsXtJBjAWLr0z3KBo8f3uUCU8vKo2MmoWYXLHw6icGPGmhqkIOuDM8gtg7UXcKQw7kUJ2L8kA1uMepDfyuUeTkb2iDppXLcZbGYeRmmStBVpr+UbHbOT5APwfdYBhJ1JK1ZJMkNuBX7v3OO6etAhpqZUxZ1RPpP5o7Fqket+Q1fqQkd5ZOmBf1AHvL8EEMexEirkdd8Bc4IOCEd2efwp3vr4IyYkC7z3YC41qVdG8rxFGxlKe0ccNxE/UAYadyBZOxh0wF7poYc0/VYRff7wTp4tK8f7Dl6NlvepR96/FyJkXo/0GYeZxAmqjDjDsljDsFMusxkDV7B3QDl9oeI8XlGDMJztw4GQxxvdpgtZpamfq4WiduEtV0AH/Rx1g2Ils5XTcAWuBP322GPdNXoLVe45hbO/GyLhY3+l3jdJ7BkYrjyVeow4w7ES2syPugLXAAxXDWFhcghEzAvhxcz4mDuyKG9s1qHAfVUsq0ViJOWBP0IHYiTrAsBM5QsXMz2zgAe3Il0qJ/25dgU9X7cO4Ozuif2YTw+MzS8/l6swGHYi/qAMMO5FjVAQj2gnDzAZeSol5y9dgyY5d6Nu+Da5q2eyX76k+G6KRa47q3beqoAP+iDrAsBM5SlU4rAY+aO7iUny2ZgO+2bQV17RqgZvatjI8FpWMvJCoeg6CYnU9PRyGncgFqiJiNW6T/rsFf/lsPe7tno4/39Ye7y1x9ufczG8EdgcdiO2oAww7kWtUBkXrwh3hgjdn6S48NW8l+nZsiJfuyUCixpkajSyjhFKxnKM65kF+jDrAsBO5KtLFOazERU/kv1i9Dw/NXoYrL03Dm4MzUSnJW9fS0XOFKQY9MoadyAPsCE2kOOYVHMGiQ6tQq1J19LqoI14dXdPSflTQe2Fvq1c3suOF1IsYdiKPsDM6wXAeOXscP+SvQLXEKrgirRMqJSRHvI9dl4jTG3GV44iXoAcx7EQeY1eENh04gf5vLEKNlGS0rdQRKYmVLW3PTqpeVKJdh9avUQf0hz3JicEQ0fnglI+SlbML7j5yGoOm5CA5MQGzsrsj/aKqFW5jdCatih2/GcRr0I3ijJ3IJVYjlXeiEHe9sQiHThZizqieaNOghqlxmA2/XUs64TDoZbgUQxQjokULCB+u4wVFuOeNxdiafxKzR3RH10vq2DU81zDmFXEphihGhEYqXMzKL9UUFJVgxPQANh08gclDuvkq6mZe5Kgihp3IQyKtwwddfsu3OJRegILUErxybwauapXm5PCU0wo5wJiboWQpRgjRG8BLABIBTJZSjo12ey7FEOkXjJ+ExJFGhThdqxi19lZG6pGKhzR6PYJ6Qh7k9cfiBsfW2IUQiQA2ArgewG4ASwEMkFKujXQfhp3IGCklnp+/FtN/2o4aByqhRn4l3fd1I5BGAh7EkGtzco09C8BmKeXWczt+B8CtACKGnYiMeWnhJkz/aTtGXNEMz/a5DEKUnf9FT0D1RlZPWM0E28r+yBwVYW8EYFfIn3cD6K5gu0QEYPqP2zDh6024s2vjC6IOhI+j2fiqjHY4DLlzVIQ93KnjKqzvCCFGAhgJAOnp6Qp2S+R/H/68B3+YvxY3tK2Psbd3uCDqkUQLKOMdH1SEfTeA0OttNQawt/yNpJSTAEwCytbYFeyXyNf+s/4AHp+7Aj2bX4SXB2QgKdH6mRoZ3vig4pyeSwFcKoRoJoSoBOAeAB8r2C5R3MrZdhgPzlqGdhfXwJtDMpGSnOj2kCiGWJ6xSymLhRCPAPgSZYc7TpVSrrE8MqI4tXrPMWRPX4rGtatg+rAspFbmx03IGCX/x0gpPwPwmYptEcWzbfmnMHRaDqqnJGFmdnfUqab/sEaiIG9dXoUoju07dgYDJy+BlMDMEd1xca0qbg+JYhR/xyPygCOnzmLwlBwcO1OEd0b2QIs0586cSP7DsBO57GRhMYZOX4odh0/jreFZaN/I/UvaUWzjUgyRiwqLS/DAzABW7zmG1+7tgh7NL3J7SOQDnLH7HD8C7l3FJaUY/fZy/Lj5EMbf1QnXt63v9pDIJxh2n7D7E4Va+2D0jZFS4tkPVuOLNfvxXN+2uL1LY7eHRD7CsMcoVSHXusiDXuXvy9BHN/bz9Xg3sAu/vqYlhl/RzO3hkM8w7DHCaHTNhNXIfbTGY+UCzX438dsteOO/WzG45yX4v+tbuT0c8iGG3cNUnm5VNSNnFWTkz3s7Zyf++sV69Ot0Mf5wSztdJ/UiMoph96BYve6jnmWdeI78Z6v24dkPVuHq1mn4x12dkJDAqJM9GHYP8dObk0YiH2uPzYzvN+Vh9Ds/o0t6bUy8ryuSFZypkSgSht0DIoXPL8HTirzfA79s5xE8MDMXLetVx5Sh3VClEs/USPZi2F3k96CHE3xs8RL4DftPYNi0pUirXhkzhndDzSoVL0BNpBrD7oJ4DHp5WoH3w3Ox6/BpDJqyBCnJCZiV3R31qqe4PSSKEwy7w8KFzA8RMytS4GN99n7wRAEGTlmCwuJSzHmgJ5rUqer2kCiOMOwOYdCjixb4WHuejp0pwpCpS5F3ohCzRnRH6wbV3R4SxRmG3QFeifqYV04avs+ER509fWy4wMdS3M+cLUH29KXYfPAEpg7thi7ptd0eEsUhIaXz15XOzMyUgUDA8f26wa2P2puJuBFOBN8rL4h6FZWUYuRbAXy3MQ+v3tsFN3do6PaQyGeEELlSykzN2zHs9nEy6naHPBo7Ix8rcS8tlRjz7nJ8vGIvXry9AwZkpbs9JPIhht1lTkRdb8xVhNfJfZXn9bhLKfHcR2swc/EO/KZ3Gzx4dQu3h0Q+xbC7yO6oa0XWiWUSN8bg1TNIjv9qA17+z2Y88D/N8czNl7k9HPIxht0ldsYnWkydfpMzlJPj8lrcp/6wDS98shZ3ZzbB2Ds68KReZCuG3QVuRN3NoIfjxDi9ciKxebm78fjcFejdrgFevTcDSTz/C9mMYXeYXVGPlaCXF27cqsbshVn7grUHMGpWLno0r4OpQ7uhchLP/0L20xt2TjFsYGfUJzya6vmoA+EjrurInfLPrxOXBQy1eOshPPzvZWjfqCYmDcpk1MlzGHYF7FgaKB/BWAl6qHBjtivuTlm95xhGzAjgkjpVMX1oN1SrzM/4kfcw7BbZMVsMF/VY5kTcnZi1b8k7icFTc1CzSjJmZndH7WqVbN8nkRkMu0IqZpF+i3qQXXF3yt6jZzBo8hIkCGDWiO5oUJNnaiTvYtgtUL0E49eoB9kRdydm7YdPncWgKUtwoqAY04dloVndarbsh0gVht2j/Bb1oFibuZ8sLMbQaTnYfeQMpgzthvaNaro9JCJNfOfHI0IDZ2fU5y4uNXT7/j3Uv/ZPeDRVadB/mH/VL7N1lWeCLCgqwf0zAli79zgmDe6KrGZ1lGyXyG6csZukchnG7lnr3MWlv3yZva9qoS9eXpy1F5eU4tdv/4xFWw/h7/074Zo29d0eEpFunLF7jOrZerQoR5qNh7vP3MWltszevai0VOLp91fhq7UH8Hy/drgto5HbQyIyJD5+UuNUuED375Hwy1ckkW6jevauctau6k1UKSX+8tk6vJe7G2OuuxRDejW1NC4iNzDsFqlchlE5Wy8fYK2YRxIvs/Sgf327BZN/2IahvZpi9LWXuj0cIlMs/dQKIcYJIdYLIVYKIT4QQtRSNTAvc/oj7EaFi7oV4WbufjRr8Q6M+3ID/jejEZ7r25ZnaqSYZXU6tgBAeyllRwAbATxjfUikkqoZt10zd68c1jl/xV78/qPVuLZNPfztzo5ISGDUKXZZ+mmVUn4lpSw+98fFABpbHxJ5lV+XZb7bmIfH5ixHt6Z18Np9XZDM0+9SjFP5f/BwAJ9H+qYQYqQQIiCECOTl5SncLUXi1xCrlLvjMEbNzEWr+tUxeUgmUpJ5pkaKfZo/+UKIr4UQq8N83Rpym2cBFAOYHWk7UspJUspMKWVmWlqamtFTVH5dC1dl3b7jGDZtKRrUTMGM4VmokZLs9pCIlNA8jl1KeV207wshhgDoC+Ba6cZVO8gxdrxQuPXhpB2HTmHw1BxUrZSEmdlZqJta2ZVxENnB6lExvQH8BkA/KeVpNUMilVTF2E+z/4PHCzBoSg6KSkoxMzsLjWtXdXtIREpZXYR9FUB1AAuEEMuFEK8rGJPnuX0BZS2qD09Uffikm46dLsKgKTnIP1mI6cOycGn96m4PiUg5q0fFtJRSNpFSdj73NUrVwGKF1WPa7TpniopPjYa7TyxEPdIL7+mzxRg2PQfb8k/hzcGZ6NwkLj52QXGI54rxsf49EiqEOVqotcKvOuoqP3Wr9QJ7trgUo2Ytw/JdR/Gv+7rg8pZ1Le2PyMu8P/2KA3ae6VDrVAJ6z/xoZ9TtVlIq8dic5fjvxjyMvb0jerdv6Ni+idzAsJtk55V77IiemXPF6DlhmAp2fvpUSonnPlqNT1buw29vboO7ujWxbV9EXsGlGI9QffGJSLywRq76cUY7N/7fv9qA2Ut24sGrW2Dk/7RQul8ir3L/p9wnYmHW7gVOXtd18vdb8do3WzAgKx1P3djatv0QeQ3DboHqwx5j7XqgRtkR9Uiz9TmBXfjTp+vQp0ND/Om29jxTI8UVhl0hFbN2v8bd7qiH+nLNfjw9byWuvLQuxt/dCYk8UyPFGYbdIjs+rBQu7rEceCeWX4L/Dj9tycej//4ZnZrUwusDu6JyEk/qRfGHYVfAjiNkwsUv1uIe7gVJVdTDPc8rdx/F/TMCaFa3GqYN7YZqlXlsAMUn4cZ5uzIzM2UgEHB8v3YqHxqVM/lwQffKBSrCifQCZFfUf5h/FTYfPIH+ry9CakoS3hvVC/VrpCjZF5GXCCFypZSZmrdj2NVxOu6AtwLvxBjDPcd7jp7BnRN/QlGJxHujeqJp3WrK9kfkJQy7S+yMO+DNwEdbIrI76vknC3HX64uQd7IQcx7oicsa1lC2PyKvYdhdZHfcAe31drtD7/T+wz2nJwqKMODNxdh88CRmZXdHZtM6SvdJ5DUMu8uciDug/w1VK6F1Yh/RhHuj9Ov3r8CQqTnI3XEEbw7JxK9a17Nl30RewrB7gFNxD3LrqBk7fzsI9xwWl5SdqXHh+gOYcHdn3Nq5kW37J/ISht0jws02nbhQh52Rd2o9P1zUS0slnnhvBd5ftgd/vK09BvW4xJGxEHkBw+4hbsU9Ej3Rd/PN2EjPl5QSL3yyFtN+3I7Hr2+FR6+91IXREbmHYfeYSB9c8vpl9pwW7UXw5YWbMH7BRgy/vBl+3/cynv+F4o7esPOjeQ4Jxql8uK645TvGHdq/1cxctB3jF2zEHV0a43d9GHWiaHhKAYeFi/gVt3yn/LS/sSLSYw99nj5avgfPfbwG111WH3+9owMSeFIvoqg4Y3dBtNl76Pf9TO/S1DfrD+LxOSuQ1bQOXr03A0mJnIsQaWHYXaQV+NDb+IWR9xqWbj+MB2fnok3D6pg8JBMpyTxTI5EeDLsHRAp86N/FcuCjLTNFelxr9x7H8OlLcXGtKpgxLAvVU5LtGh6R7zDsHqIn8OVv61Va7xlEG//2/FMYPDUHqZWTMDO7Oy5Krax6eES+xrB7kJ7zu3st9Hre/NUzxv3HCjBwyhKUSomZ2T3QqFYVFcMjiisMu8fpvYiHUx+CMnP0jt5xHD19FoOnLsGRU2fx9sgeaFnPO6ckJoolDHsMMXqlJrcOoTTzgnKqsBhDpy3F9kOnMX1YN3RsXMuGkRHFB4Y9RkU6Ht4NVn8zKCwuwahZuVi5+ygmDuyKXi3qKhoZUXxi2H0kWmCtRN/O9fuSUonH3l2B7zflY9ydHXFjuwa27YsoXjDsccLtN1fDkVLidx+uwqer9uF3fS5D/8wmbg+JyBf4MT5yzd++3IC3c3bhkV+1xIgrm7s9HCLfYNjJFW98twUTv92C+7qn4/EbWrk9HCJfYdjJce8u3YkXP1+Pvh0b4oVb2/NMjUSKMezkqC9W78Mz76/CVa3SMP6uzkjkmRqJlFMSdiGopHPoAAAFlUlEQVTEE0IIKYTgcWoU0Q+b8vHrt5cjI702Jg7sgkpJnFcQ2cHyT5YQogmA6wHstD4c8qvlu45i5MwAmqdVw9Qh3VC1Eg/IIrKLiinTPwE8BcD5a+xRTNh04ASGTstB3dTKeGt4FmpW5ZkaiexkKexCiH4A9kgpVygaD/nMrsOnMXDKElRKTMCs7O6oVyPF7SER+Z7m78NCiK8BhPs44LMAfgvgBj07EkKMBDASANLT0w0MkWJV3olCDJqyBGfOlmDOqJ5Iv6iq20MiigtCSnMrKEKIDgAWAjh97q8aA9gLIEtKuT/afTMzM2UgEDC1X4oNxwuKcM8bi7Et/xRmjeiOrpfUdntIRDFPCJErpczUup3pd7CklKsA1AvZ4XYAmVLKfLPbJH84c7YEI6YHsOngCUwe0o1RJ3IYD00gpYpKSvHwv5dh6Y7DePmeDFzVKs3tIRHFHWVhl1I2VbUtik2lpRJPzl2B/6w/iD//b3vc0ulit4dEFJf4CRFSQkqJFz5Ziw+X78WTN7bGfd0vcXtIRHGLYSclJny9CdN/2o77r2yGh65u4fZwiOIaw06WTftxG15auAn9uzbGb2++jCf1InIZw06WfPDzbjw/fy1uaFsfL97egVEn8gCGnUxbuO4Anpi7Er1aXISXB2QgKZH/OxF5AX8SyZQlWw/hodnL0O7iGpg0OBMpyYluD4mIzmHYybDVe45hxIwAGteugunDspBamR+HIPIShp0M2Zp3EkOm5qBGlWTMGtEddapVcntIRFQOw0667Tt2BoOm5AAAZmZnoWHNKi6PiIjCYdhJt235p3C2pBQzhmeheVqq28Mhogi4OEq69WpRF98/9Su+UUrkcZyxkyGMOpH3MexERD7DsBMR+QzDTkTkMww7EZHPMOxERD5j+mLWlnYqRB6AHY7vuKK6AHiN1jJ8Ls7jc3Een4vzvPBcXCKl1LzepCth9wohREDPFb/jAZ+L8/hcnMfn4rxYei64FENE5DMMOxGRz8R72Ce5PQAP4XNxHp+L8/hcnBczz0Vcr7ETEflRvM/YiYh8h2EHIIR4QgghhRB13R6LW4QQ44QQ64UQK4UQHwghark9JqcJIXoLITYIITYLIZ52ezxuEUI0EUJ8I4RYJ4RYI4QY7faY3CaESBRC/CyE+MTtsegR92EXQjQBcD2AnW6PxWULALSXUnYEsBHAMy6Px1FCiEQArwG4CUBbAAOEEG3dHZVrigE8LqW8DEAPAA/H8XMRNBrAOrcHoVfchx3APwE8BSCu32yQUn4lpSw+98fFABq7OR4XZAHYLKXcKqU8C+AdALe6PCZXSCn3SSmXnfvvEygLWiN3R+UeIURjAH0ATHZ7LHrFddiFEP0A7JFSrnB7LB4zHMDnbg/CYY0A7Ar5827EccyChBBNAWQAWOLuSFw1AWWTv1K3B6KX76+gJIT4GkCDMN96FsBvAdzg7IjcE+25kFJ+dO42z6LsV/HZTo7NA0SYv4vr3+KEEKkA5gEYI6U87vZ43CCE6AvgoJQyVwhxtdvj0cv3YZdSXhfu74UQHQA0A7BCCAGULT0sE0JkSSn3OzhEx0R6LoKEEEMA9AVwrYy/42B3A2gS8ufGAPa6NBbXCSGSURb12VLK990ej4suB9BPCHEzgBQANYQQs6SUA10eV1Q8jv0cIcR2AJlSSrdP8uMKIURvAOMBXCWlzHN7PE4TQiSh7E3jawHsAbAUwL1SyjWuDswFomymMwPAYSnlGLfH4xXnZuxPSCn7uj0WLXG9xk4XeBVAdQALhBDLhRCvuz0gJ5174/gRAF+i7M3COfEY9XMuBzAIwDXn/l9Yfm7GSjGCM3YiIp/hjJ2IyGcYdiIin2HYiYh8hmEnIvIZhp2IyGcYdiIin2HYiYh8hmEnIvKZ/w8knfmKb5SUQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-5, 5, 0.025)\n",
    "Y = np.arange(-5, 5, 0.025)\n",
    "XX, YY = np.meshgrid(X, Y)\n",
    "XY = torch.stack([torch.tensor(XX, dtype=t_type), torch.tensor(YY, dtype=t_type)], dim=2)\n",
    "Z = torch.zeros([XY.shape[0], XY.shape[1]])\n",
    "\n",
    "Z = dm.real_target_density(XY.permute(2, 0, 1).view(2, -1)).view(Z.shape)\n",
    "Z = Z.cpu().data.numpy()\n",
    "\n",
    "# Plot the surface.\n",
    "plt.contour(X, Y, Z, cmap=cm.coolwarm,\n",
    "                    linewidth=0, antialiased=False)\n",
    "# Plot conditioning line\n",
    "XXX, YYY = dm.lt.transform(torch.tensor(X, dtype=t_type, device=device).view(1, -1), n_particles_second=True).data.cpu().numpy()\n",
    "plt.plot(XXX, YYY)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare implementations of Stein Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class SVGD():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def svgd_kernel(self, theta, h = -1):\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = h / np.log(theta.shape[0]+1)\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = 2. * dxkxy / (h)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    " \n",
    "    def update(self, x0, lnprob, n_iter = 1000, stepsize = 1e-3, bandwidth = -1, alpha = 0.9, debug = False):\n",
    "        # Check input\n",
    "        if x0 is None or lnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        theta = np.copy(x0) \n",
    "        \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter+1) % 1000 == 0:\n",
    "                print( 'iter ' + str(iter+1) )\n",
    "            \n",
    "            lnpgrad = lnprob(theta)\n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(theta, h = bandwidth)  \n",
    "            grad_theta = (np.matmul(kxy, lnpgrad) + dxkxy) / x0.shape[0]  \n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + grad_theta ** 2\n",
    "            else:\n",
    "                historical_grad = alpha * historical_grad + (1 - alpha) * (grad_theta ** 2)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            theta = theta + stepsize * adj_grad \n",
    "            \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_ex = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lnprob(particles):\n",
    "    grad_log_term = torch.zeros([particles.shape[0], particles.shape[1]], dtype=t_type, device=device)\n",
    "        \n",
    "    for idx in range(100):\n",
    "        particle = torch.tensor(particles[idx:idx+1], dtype=t_type, device=device, requires_grad=True)\n",
    "        log_term = torch.log(dm_ex.target_density(particle.t()))\n",
    "        grad_log_term[idx] = torch.autograd.grad(log_term, particle,\n",
    "                                                     only_inputs=True, retain_graph=False, \n",
    "                                                     create_graph=False, allow_unused=False)[0]\n",
    "    return grad_log_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = dm_ex.particles.data.clone().detach().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare kernel implementation\n",
    "\n",
    "ker, grad_ker = dm_ex.calc_kernel_term_latent(10.)\n",
    "print(dm_ex.h)\n",
    "grad_ker_sum = torch.sum(grad_ker, dim=1)\n",
    "\n",
    "ker_l, grad_ker_sum_l = dm_svgd.svgd_kernel(result, h=10.)\n",
    "\n",
    "print((ker - torch.tensor(ker_l, dtype=t_type, device=device)).norm())\n",
    "print((grad_ker_sum - torch.tensor(grad_ker_sum_l, dtype=t_type, device=device).t()).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare log_term implementation\n",
    "(lnprob(result) - dm_ex.calc_log_term_latent().t()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(201): \n",
    "    try:\n",
    "        result = dm_svgd.update(result, lnprob, stepsize=1e-2, bandwidth=-1, n_iter=20)\n",
    "        result = torch.tensor(result, dtype=t_type, device=device)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(result.data.cpu().numpy(), torch.zeros_like(result).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(result.data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(2000): \n",
    "    try:\n",
    "        dm_ex.update_latent(h_type=0)\n",
    "\n",
    "        if _ % 30 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm_ex.h))\n",
    "            \n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(dm_ex.particles.t().data.cpu().numpy(), torch.zeros_like(dm_ex.particles.t()).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(dm_ex.particles.t().data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(result - dm_ex.particles.t()).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"Stein-Variational-Gradient-Descent/python/\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.matlib as nm\n",
    "from svgd import SVGD\n",
    "\n",
    "r\"\"\"\n",
    "    Example of Bayesian Logistic Regression (the same setting as Gershman et al. 2012):\n",
    "    The observed data D = {X, y} consist of N binary class labels, \n",
    "    y_t \\in {-1,+1}, and d covariates for each datapoint, X_t \\in R^d.\n",
    "    The hidden variables \\theta = {w, \\alpha} consist of d regression coefficients w_k \\in R,\n",
    "    and a precision parameter \\alpha \\in R_+. We assume the following model:\n",
    "        p(\\alpha) = Gamma(\\alpha; a, b)\n",
    "        p(w_k | a) = N(w_k; 0, \\alpha^-1)\n",
    "        p(y_t = 1| x_t, w) = 1 / (1+exp(-w^T x_t))\n",
    "\"\"\"\n",
    "class BayesianLR:\n",
    "    def __init__(self, X, Y, batchsize=100, a0=1, b0=0.01):\n",
    "        self.X, self.Y = X, Y\n",
    "        # TODO. Y in \\in{+1, -1}\n",
    "        self.batchsize = min(batchsize, X.shape[0])\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.permutation = np.random.permutation(self.N)\n",
    "        self.iter = 0\n",
    "    \n",
    "        \n",
    "    def dlnprob(self, theta):\n",
    "        \n",
    "        if self.batchsize > 0:\n",
    "            batch = [ i % self.N for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize) ]\n",
    "            ridx = self.permutation[batch]\n",
    "            self.iter += 1\n",
    "        else:\n",
    "            ridx = np.random.permutation(self.X.shape[0])\n",
    "            \n",
    "        Xs = self.X[ridx, :]\n",
    "        Ys = self.Y[ridx]\n",
    "        \n",
    "        w = theta[:, :-1]  # logistic weights\n",
    "        alpha = np.exp(theta[:, -1])  # the last column is logalpha\n",
    "        d = w.shape[1]\n",
    "        \n",
    "        wt = np.multiply((alpha / 2), np.sum(w ** 2, axis=1))\n",
    "        \n",
    "        coff = np.matmul(Xs, w.T)\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-1 * coff))\n",
    "        \n",
    "        dw_data = np.matmul(((nm.repmat(np.vstack(Ys), 1, theta.shape[0]) + 1) / 2.0 - y_hat).T, Xs)  # Y \\in {-1,1}\n",
    "        dw_prior = -np.multiply(nm.repmat(np.vstack(alpha), 1, d) , w)\n",
    "        dw = dw_data * 1.0 * self.X.shape[0] / Xs.shape[0] + dw_prior  # re-scale\n",
    "        \n",
    "        dalpha = d / 2.0 - wt + (self.a0 - 1) - self.b0 * alpha + 1  # the last term is the jacobian term\n",
    "        \n",
    "        return np.hstack([dw, np.vstack(dalpha)])  # % first order derivative \n",
    "    \n",
    "    def evaluation(self, theta, X_test, y_test):\n",
    "        theta = theta[:, :-1]\n",
    "        M, n_test = theta.shape[0], len(y_test)\n",
    "\n",
    "        prob = np.zeros([n_test, M])\n",
    "        for t in range(M):\n",
    "            coff = np.multiply(y_test, np.sum(-1 * np.multiply(nm.repmat(theta[t, :], n_test, 1), X_test), axis=1))\n",
    "            prob[:, t] = np.divide(np.ones(n_test), (1 + np.exp(coff)))\n",
    "        \n",
    "        prob = np.mean(prob, axis=1)\n",
    "        acc = np.mean(prob > 0.5)\n",
    "        llh = np.mean(np.log(prob))\n",
    "        return [acc, llh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = X_test.shape[0] + X_train.shape[0]\n",
    "X_input = np.hstack([X, np.ones([N, 1])])\n",
    "y_input = y\n",
    "d = X_input.shape[1]\n",
    "D = d + 1\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.2, random_state=42)\n",
    "    \n",
    "a0, b0 = 1, 0.01 #hyper-parameters\n",
    "model = BayesianLR(X_train, y_train, 100, a0, b0) # batchsize = 100\n",
    "    \n",
    "# initialization\n",
    "M = 100  # number of particles\n",
    "theta0 = np.zeros([M, D]);\n",
    "alpha0 = np.random.gamma(a0, b0, M); \n",
    "for i in range(M):\n",
    "    theta0[i, :] = np.hstack([np.random.normal(0, np.sqrt(1 / alpha0[i]), d), np.log(alpha0[i])])\n",
    "    \n",
    "theta = SVGD().update(x0=theta0, lnprob=model.dlnprob, bandwidth=-1, n_iter=6000, stepsize=0.05, alpha=0.9, debug=True)\n",
    "    \n",
    "print('[accuracy, log-likelihood]')\n",
    "print(model.evaluation(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Metropolis–Hastings algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MHAlgorithm():\n",
    "    def __init__(self, n_dims): \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                           0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "        self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "        \n",
    "#         self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "#         self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "    \n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_dims, 1],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., -1.)\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "    def generate_next(self):\n",
    "        previous = self.particles[:, -1].unsqueeze(1)\n",
    "#         self.additional_distribution = normal_density(self.n_dims, previous, 1, n_particles_second=True)\n",
    "#         candidate = self.additional_distribution.get_sample()\n",
    "        \n",
    "#         alpha = (self.target_density(candidate) * self.additional_distribution.unnormed_density(previous) / \n",
    "#                 (self.target_density(previous) * self.additional_distribution.unnormed_density(candidate)))\n",
    "        candidate = previous + torch.zeros([self.n_dims, 1], device=device, dtype=t_type).uniform_(-1., 1.)\n",
    "        alpha = self.target_density(candidate) / self.target_density(previous)\n",
    "        \n",
    "#         print(self.target_density(candidate)[0].data.numpy(), self.additional_distribution.unnormed_density(previous)[0].data.numpy(),\n",
    "#               self.target_density(previous)[0].data.numpy(), self.additional_distribution.unnormed_density(candidate)[0].data.numpy())\n",
    "        \n",
    "        if alpha > self.one:\n",
    "            self.particles = torch.cat([self.particles, candidate], dim=1)\n",
    "        else:\n",
    "            random = torch.bernoulli(alpha)\n",
    "            if random:\n",
    "                self.particles = torch.cat([self.particles, candidate], dim=1)                \n",
    "            else:\n",
    "                self.particles = torch.cat([self.particles, previous], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mh = MHAlgorithm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    mh.generate_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(mh.particles.t().data.cpu().numpy()[:,1], \n",
    "            kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(mh.particles.t().data.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import time\n",
    "\n",
    "mu,sig,N = 0,1,1000000\n",
    "pts = []\n",
    "\n",
    "def q(x):\n",
    "#     return (1/(math.sqrt(2*math.pi*sig**2)))*(math.e**(-((x-mu)**2)/(2*sig**2)))\n",
    "#     return normal_density(1, 0., 2., n_particles_second=True).unnormed_density(x)\n",
    "    return (0.3 * normal_density(1, -2., 1., n_particles_second=True).unnormed_density(x) +\n",
    "            0.7 * normal_density(1, 2., 1., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "def metropolis(N):\n",
    "    r = np.zeros(1)\n",
    "    p = q(r[0])\n",
    "    pts = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        rn = r + np.random.uniform(-1,1)\n",
    "        pn = q(rn[0])\n",
    "        if pn >= p:\n",
    "            p = pn\n",
    "            r = rn\n",
    "        else:\n",
    "            u = np.random.rand()\n",
    "            if u < pn/p:\n",
    "                p = pn\n",
    "                r = rn\n",
    "        pts.append(r)\n",
    "    \n",
    "    pts = np.array(pts)\n",
    "    return pts\n",
    "    \n",
    "def hist_plot(array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1,)\n",
    "    ax.hist(array, bins=1000)    \n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(metropolis(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gg =  lambda : (normal_density(1, -60., 20., n_particles_second=True).get_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot([gg()[0,0] for _ in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

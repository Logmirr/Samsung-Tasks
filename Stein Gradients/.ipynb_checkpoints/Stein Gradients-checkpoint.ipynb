{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orth\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "device = None\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    use_cuda = True\n",
    "\n",
    "# Ignore cuda\n",
    "# use_cuda = False\n",
    "# device = None\n",
    "\n",
    "# Set DoubleTensor as a base type for computations\n",
    "t_type = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import functools\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Support classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_plots(data, axis, labels, file_name=None):\n",
    "    N_plots = len(data)\n",
    "    plt.figure(figsize=(30, (N_plots // 3 + 1) * 10))\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        for jdx in range(len(data[idx])):\n",
    "            plt.plot(data[idx][jdx], label=labels[idx][jdx])\n",
    "        plt.xlabel(axis[idx][0], fontsize=16)\n",
    "        plt.ylabel(axis[idx][1], fontsize=16)\n",
    "        plt.legend(loc=0, fontsize=16)\n",
    "    if file_name is not None:\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_projections(dm=None, use_real=True, kernel='tri', pdf=None, N_plots_max=10):\n",
    "    \"\"\"\n",
    "        Plot marginal kernel density estimation\n",
    "    Args:\n",
    "        dm (DistributionMover): class containing particles which define distribution\n",
    "        use_real (bool): If set to True then apply transformation dm.lt.transform before creating plot\n",
    "        kernel (str): Kernel type for kernel density estimation\n",
    "        pdf (array_like, None): Samples from target distribution\n",
    "        N_plots_max (int): Maximum number of plots\n",
    "    \"\"\"\n",
    "    N_plots = None\n",
    "    scale_factor = None\n",
    "    \n",
    "    if use_real:\n",
    "        if not dm.use_latent:\n",
    "            return\n",
    "        N_plots = dm.lt.A.shape[0]\n",
    "    else:\n",
    "        N_plots = dm.particles.shape[0]\n",
    "    if N_plots > 6:\n",
    "        scale_factor = 15\n",
    "    else:\n",
    "        scale_factor = 5\n",
    "        \n",
    "    N_plots = min(N_plots, N_plots_max)\n",
    "        \n",
    "    plt.figure(figsize=(3 * scale_factor, (N_plots // 3 + 1) * scale_factor))\n",
    "    \n",
    "    for idx in range(N_plots):\n",
    "        slice_dim = idx\n",
    "        \n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        \n",
    "        particles = None\n",
    "        if use_real:\n",
    "            particles = dm.lt.transform(dm.particles, n_particles_second=True).t()[:, slice_dim]\n",
    "        else:\n",
    "            particles = dm.particles.t()[:, slice_dim]\n",
    "        \n",
    "        if pdf is not None:\n",
    "            plt.plot(np.linspace(-10, 10, len(pdf), dtype=np.float64), pdf)\n",
    "        plt.plot(particles.data.cpu().numpy(), torch.zeros_like(particles).data.cpu().numpy(), 'ro')\n",
    "        sns.kdeplot(particles.data.cpu().numpy(), \n",
    "                    kernel=kernel, color='darkblue', linewidth=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_condition_distribution(dm, n_samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dm (DistributionMover): object contains unconditioned density, linear manifold and particles\n",
    "        n_samples (int): number of samples\n",
    "    Return:\n",
    "        (points, weight)\n",
    "    \"\"\"\n",
    "    if not dm.use_latent:\n",
    "        return\n",
    "    \n",
    "    points = torch.zeros([dm.n_hidden_dims, n_samples], dtype=t_type, device=device).uniform_(-10, 10)\n",
    "    weight = dm.real_target_density(dm.lt.transform(points, n_particles_second=True))\n",
    "    points = points.view(-1)\n",
    "    \n",
    "    plt.hist(points.data.cpu().numpy(), weights=weight.data.cpu().numpy(), density=True, bins=100, alpha=0.5, label='True conditional density')\n",
    "    plt.plot(dm.particles.data.cpu().numpy(), torch.zeros_like(dm.particles).data.cpu().numpy(), 'ro')\n",
    "    sns.kdeplot(dm.particles[0, :].data.cpu().numpy(), \n",
    "                kernel='tri', color='darkblue', linewidth=4, label='Approximated conditional density')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pairwise_diffs(x, y, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: x is a dxN matrix\n",
    "                   y is an optional dxM matrix\n",
    "            Output: diffs is a dxNxM matrix where diffs[i,j] is the subtraction between x[:,i] and y[:,j]\n",
    "            i.e. diffs[i,j] = x[:,i] - y[:,j]\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: x is a Nxd matrix\n",
    "                   y is an optional Mxd matrix\n",
    "            Output: diffs is a NxMxd matrix where diffs[i,j] is the subtraction between x[i,:] and y[j,:]\n",
    "            i.e. diffs[i,j] = x[i,:]-y[j,:]\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return x[:,:,np.newaxis] - y[:,np.newaxis,:]        \n",
    "    return x[:,np.newaxis,:] - y[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pairwise_dists(diffs=None, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: diffs is a dxNxM matrix where diffs[i,j] = x[:,i] - y[:,j]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[:,i] - y[:,j]||\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: diffs is a NxMxd matrix where diffs[i,j] = x[i,:] - y[j, :]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[i,:]-y[j,:]||\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return torch.norm(diffs, dim=0)\n",
    "    return torch.norm(diffs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class normal_density():\n",
    "    \"\"\"\n",
    "        Multinomial normal density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, mu=0., std=1., n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            mu (float, array_like): mean of distribution\n",
    "                Default: 0.\n",
    "                if mu is float - use same mean across all dimensions\n",
    "                if mu is 1D array_like - use different mean for each dimension but same for each particles dimension\n",
    "                if mu is 2D array_like - use different mean for each dimension\n",
    "            std (float, array_like): std of distribution\n",
    "                Default: 1.\n",
    "                if std is float - use same std across all dimensions\n",
    "                if std is 1D array_like - use different std for each dimension but same for each particles dimension\n",
    "                if std is 2D array_like - use different std for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis\n",
    "                Input will be reduced along all axis exclude particle axis\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        \n",
    "        if isinstance(self.mu, float):\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.mu.shape) == 1:\n",
    "            if self.mu.shape[0] == 1:\n",
    "                self.mu = self.mu.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.mu.shape) == 2:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if isinstance(self.std, float):\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.std.shape) == 1:\n",
    "            if len(self.std) == 1:\n",
    "                self.std = self.std.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.zero = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.pi = torch.tensor(math.pi, dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "    def __call__(self, x, n_axis=None):        \n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.pow(self.two * self.pi, -self.n / self.two) / \n",
    "                torch.prod(self.std, dim=n_axis) * \n",
    "                torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):      \n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std , self.two), dim=n_axis))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):  \n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (-self.n / self.two * torch.log(self.two * self.pi) + \n",
    "                torch.sum(torch.log(self.std), dim=n_axis) -\n",
    "                self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return -self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)\n",
    "    \n",
    "    def get_sample(self):\n",
    "        \"\"\"\n",
    "            Sample from normal distribution\n",
    "        \"\"\"\n",
    "        sample = torch.normal(self.mu, self.std)\n",
    "        if self.n_particles_second:\n",
    "            return sample.view(-1, 1)\n",
    "        else:\n",
    "            return sample.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class gamma_density():\n",
    "    \"\"\"\n",
    "        Multinomial gamma density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, alpha=1, betta=1, n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            alpha (float, array_like): shape of distribution\n",
    "                Default: 1.\n",
    "                if alpha is float - use same shape across all dimensions\n",
    "                if alpha is 1D array_like - use different shape for each dimension but same for each particles dimension\n",
    "                if alpha is 2D array_like - use different shape for each dimension\n",
    "            betta (float, array_like): rate of distribution\n",
    "                Default: 1.\n",
    "                if betta is float - use same rate across all dimensions\n",
    "                if betta is 1D array_like - use different rate for each dimension but same for each particles dimension\n",
    "                if betta is 2D array_like - use different rate for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis  \n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.betta = betta\n",
    "        \n",
    "        if isinstance(self.alpha, float):\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.alpha.shape) == 1:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.alpha.shape) == 2:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if isinstance(self.betta, float):\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.betta.shape) == 1:\n",
    "            if len(self.betta) == 1:\n",
    "                self.betta = self.betta.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "        ### log Г(alpha)\n",
    "        self.lgamma = torch.lgamma(self.alpha)\n",
    "        ### Г(alpha)\n",
    "        self.gamma = torch.exp(self.lgamma)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(self.betta, self.alpha) / self.gamma * torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     11,
     33,
     36,
     47,
     54,
     73,
     107
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SteinLinear(nn.Module):\n",
    "    \"\"\"\n",
    "        Custom full connected layer for Stein Gradient Neural Networks\n",
    "        Transformation: y = xA + b\n",
    "        Parameters prior: \n",
    "            1 - p(w, a) = p(w|a)p(a) = П p(w_i|a_i)p(a_i)\n",
    "                p(w_i|a_i) = N(w_i|0, a_i^(-1)); p(a_i) = G(1e-4, 1e-4)\n",
    "                          \n",
    "            2 - p(w) = П p(w_i)\n",
    "                p(w_i) = N(w_i|0, alpha^(-1))\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, n_particles=1, use_bias=True, use_var_prior=True, alpha=1e-2):\n",
    "        super(SteinLinear, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features (int): size of each input sample\n",
    "            out_features (int): size of each output sample\n",
    "            n_particles (int): number of particles\n",
    "            use_bias (bool): If set to False, the layer will not learn an additive bias.\n",
    "                Default: True\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of weight variance\n",
    "                Default: True\n",
    "            alpha (float): If use_var_prior == False - defines weight variance\n",
    "                Default: 1e-2\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_particles = n_particles\n",
    "        self.use_bias = use_bias\n",
    "        self.use_var_prior = use_var_prior\n",
    "\n",
    "        ### if alpha is None use GLOROT prior\n",
    "        if alpha is None:\n",
    "            self.alpha_weight = (self.in_features + self.out_features) / 2.\n",
    "            self.alpha_bias = (self.out_features) / 2.\n",
    "        else:\n",
    "            self.alpha_weight = alpha\n",
    "            self.alpha_bias = alpha\n",
    "    \n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.zeros([in_features, out_features, n_particles], dtype=t_type, device=device))\n",
    "        if self.use_var_prior:\n",
    "            self.log_weight_alpha = torch.nn.Parameter(torch.zeros([in_features * out_features, n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_weight_alpha = torch.tensor([math.log(self.alpha_weight)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros([1, out_features, n_particles], dtype=t_type, device=device))\n",
    "            if self.use_var_prior:\n",
    "                self.log_bias_alpha = torch.nn.Parameter(torch.zeros([out_features, n_particles], dtype=t_type, device=device))\n",
    "            else:\n",
    "                self.log_bias_alpha = torch.tensor([math.log(self.alpha_bias)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        if self.use_var_prior:\n",
    "            ### define prior on alpha p(a) = G(1e-4, 1e-4)\n",
    "            self.weight_alpha_log_prior = lambda x: (gamma_density(n=self.log_weight_alpha.shape[0],\n",
    "                                                                   alpha=1e-4,\n",
    "                                                                   betta=1e-4,\n",
    "                                                                   n_particles_second=True\n",
    "                                                                  ).log_unnormed_density_log_x(x))\n",
    "            if self.use_bias:\n",
    "                self.bias_alpha_log_prior = lambda x: (gamma_density(n=self.log_bias_alpha.shape[0],\n",
    "                                                                     alpha=1e-4,\n",
    "                                                                     betta=1e-4,\n",
    "                                                                     n_particles_second=True\n",
    "                                                                    ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    ### useless function - all initialization defined in DistributionMover class\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.log_weight_alpha.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.log_bias_alpha.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Apply transformation: X_out[i, :, :] = X_in[i, :, :] * W + b[i]\n",
    "        Args:\n",
    "            X (torch.tensor): tensor \n",
    "        Shape:\n",
    "            Input: [n_particles, batch_size, in_features]\n",
    "            Output: [n_particles, batch_size, out_features]\n",
    "        \"\"\"\n",
    "        ### NEED SOME OPTIMIZATION TO OMMIT .permute\n",
    "        if self.use_bias:\n",
    "            return torch.bmm(X, self.weight.permute(2, 0, 1)) + self.bias.permute(2, 0, 1)\n",
    "        return torch.bmm(X, self.weight.permute(2, 0, 1))\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())\n",
    "        \n",
    "    def calc_log_prior(self):\n",
    "        \"\"\"\n",
    "            Evaluate log prior of trainable parameters\n",
    "        log p(w,a) = log p(w|a) + log p(a)\n",
    "        \"\"\"\n",
    "        ### define prior on weight p(w|a) = N(0, 1 / alpha)\n",
    "        weight_log_prior = lambda x: (normal_density(n=self.weight.numel() // self.n_particles,\n",
    "                                                     mu=0.,\n",
    "                                                     std=self.one / torch.exp(self.log_weight_alpha),\n",
    "                                                     n_particles_second=True\n",
    "                                                    ).log_unnormed_density(x))\n",
    "\n",
    "        bias_log_prior = lambda x: (normal_density(self.bias.numel() // self.n_particles,\n",
    "                                                   mu=0.,\n",
    "                                                   std=self.one / torch.exp(self.log_bias_alpha),\n",
    "                                                   n_particles_second=True\n",
    "                                                  ).log_unnormed_density(x))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            if self.use_var_prior:\n",
    "                return (weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha) +\n",
    "                        bias_log_prior(self.bias.view(-1, self.n_particles)) + self.bias_alpha_log_prior(self.log_bias_alpha))   \n",
    "            return (weight_log_prior(self.weight.view(-1, self.n_particles)) +\n",
    "                    bias_log_prior(self.bias.view(-1, self.n_particles)))     \n",
    "        \n",
    "        if self.use_var_prior:\n",
    "            return weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha)\n",
    "        return weight_log_prior(self.weight.view(-1, self.n_particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     4,
     47,
     58,
     69,
     84,
     109
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LinearTransform():\n",
    "    \"\"\"\n",
    "        Class for various linear transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, n_dims, n_hidden_dims, use_identity=False, normalize=False, A=None, theta_0=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_dims (int): dimension of the space\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_identity (bool): If set to True, use 'eye' matrix for transformations\n",
    "            normalize (bool): If set to True, columns of the transformation matrix is an orthonormal basis \n",
    "            A (2D array_like, None): Initial value for transformation matrix\n",
    "                If None then matrix will be sampled from uniform distribution and then orthonormate\n",
    "                Default: None\n",
    "            theta_0 (1D array_like, None): Initial value for bias\n",
    "                If None then matrix will be sampled from uniform distribution\n",
    "                Default: None\n",
    "        \"\"\"\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_identity = use_identity\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if self.use_identity:\n",
    "            return\n",
    "        \n",
    "        self.A = A\n",
    "        self.theta_0 = theta_0\n",
    "        \n",
    "        if self.A is None:\n",
    "            self.A = torch.zeros([self.n_dims, self.n_hidden_dims], dtype=t_type, device=device)\n",
    "            self.A.uniform_(-1., 1.)\n",
    "            if self.normalize:\n",
    "                ### normalize columns of matrix A\n",
    "                self.A = torch.tensor(orth(self.A.data.cpu().numpy()), dtype=t_type, device=device)\n",
    "                    \n",
    "        if self.theta_0 is None:\n",
    "            self.theta_0 = torch.zeros([self.n_dims, 1], dtype=t_type, device=device)\n",
    "            self.theta_0.uniform_(-1.,1.)\n",
    "        \n",
    "        ### A^(t)A\n",
    "        self.AtA = torch.matmul(self.A.t(), self.A)\n",
    "        ### (A^(t)A)^(-1)\n",
    "        self.AtA_1 = torch.inverse(self.AtA)\n",
    "        ### (A^(t)A)^(-1)A^(t)\n",
    "        self.inverse_base = torch.matmul(self.AtA_1, self.A.t())\n",
    "        \n",
    "    def transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Transform thetas as follows: \n",
    "                theta = Atheta` + theta_0\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.A, theta) + self.theta_0\n",
    "        return (torch.matmul(self.A, theta.t()) + self.theta_0).t()\n",
    "    \n",
    "    def inverse_transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Apply inverse transformation: \n",
    "                theta` = (A^(t)A)^(-1)A^(t)(theta - theta_0)\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.inverse_base, theta - self.theta_0)\n",
    "        return torch.matmul(self.inverse_base, theta.t() - self.theta_0).t()\n",
    "    \n",
    "    def project_inverse(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Project and then apply inverse transform to theta - theta_0:\n",
    "                theta_s_p_i = T^(-1)P(theta - theta_0)= (A^(t)A)^(-1)A^(t)theta\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        ### This optimization severely reduces performance!!!!\n",
    "        ### use solver trick: theta_s_p_i : A^(t)Atheta_s_p_i = A^(t)theta\n",
    "        if n_particles_second:\n",
    "            # return torch.gesv(torch.matmul(self.A.t(), theta), self.AtA)[0]\n",
    "            return torch.matmul(self.inverse_base, theta)\n",
    "        # return torch.gesv(torch.matmul(self.A.t(), theta.t()), self.AtA)[0].t()\n",
    "        return torch.matmul(theta, self.inverse_base.t())\n",
    "        \n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        Example::\n",
    "            >>> module.state_dict().keys()\n",
    "            ['bias', 'weight']\n",
    "        \"\"\"\n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=1)\n",
    "        \n",
    "        destination[prefix + 'A'] = self.A if keep_vars else self.A.data\n",
    "        destination[prefix + 'theta_0'] = self.theta_0 if keep_vars else self.theta_0.data\n",
    "        destination[prefix + 'n_dims'] = self.n_dims\n",
    "        destination[prefix + 'n_hidden_dims'] = self.n_hidden_dims\n",
    "        destination[prefix + 'use_identity'] = self.use_identity\n",
    "        destination[prefix + 'normalize'] = self.normalize\n",
    "        \n",
    "        return destination\n",
    "    \n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        self.A.copy_(state_dict[prefix + 'A'])\n",
    "        self.theta_0.copy_(state_dict[prefix + 'theta_0'])\n",
    "        self.__init__(state_dict[prefix + 'n_dims'],\n",
    "                      state_dict[prefix + 'n_hidden_dims'],\n",
    "                      state_dict[prefix + 'use_identity'],\n",
    "                      state_dict[prefix + 'normalize'],\n",
    "                      self.A, self.theta_0\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0,
     4,
     20,
     22,
     25,
     36,
     74,
     77
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RegressionDistribution(nn.Module):\n",
    "    \"\"\"\n",
    "        Distribution over data for regression task: p(D|w) = N(y_predicted|y, )\n",
    "    \"\"\"\n",
    "    def __init__(self, n_particles, use_var_prior=True, betta=1e-1):\n",
    "        super(RegressionDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of prediction variance\n",
    "                Default: True\n",
    "            betta (float): If use_var_prior == False - defines variance of prediction\n",
    "                Default: 1e-1\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.use_var_prior = use_var_prior\n",
    "        self.betta = betta\n",
    "        \n",
    "        ### define betta - variance of data distribution for regression tasks (betta = 1 / std ** 2)\n",
    "        if self.use_var_prior:\n",
    "            self.log_betta = torch.nn.Parameter(torch.zeros([1, self.n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_betta = torch.tensor([math.log(self.betta)], dtype=t_type, device=device, requires_grad=False)\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            ### define prior on betta p(betta)\n",
    "            self.betta_log_prior = lambda x: (gamma_density(n=1,\n",
    "                                                            alpha=1e-4,\n",
    "                                                            betta=1e-4,\n",
    "                                                            n_particles_second=True\n",
    "                                                           ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        ### Support tensors for computations\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "    \n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(theta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predicted values\n",
    "            train_size (int) - size of training dataset\n",
    "        Shapes:\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, 1]\n",
    "        \"\"\"\n",
    "        ### squeeze last axis because regression task is being solved\n",
    "        y_predict.squeeze_(2)\n",
    "        \n",
    "        batch_size = torch.tensor(y.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        log_data_distr = None\n",
    "        if self.use_var_prior:\n",
    "            log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                       mu=y,\n",
    "                                                       std=self.one / torch.sqrt(torch.exp(self.log_betta.expand(X.shape[0], self.n_particles).t())),\n",
    "                                                       n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "        else:\n",
    "            log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                       mu=y,\n",
    "                                                       std=self.one / torch.sqrt(torch.exp(self.log_betta)),\n",
    "                                                       n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "            \n",
    "        if self.use_var_prior:\n",
    "            return train_size / batch_size * log_data_distr(y_predict) + self.betta_log_prior(self.log_betta)\n",
    "        return train_size / batch_size * log_data_distr(y_predict)\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ClassificationDistribution(nn.Module):\n",
    "    def __init__(self, n_particles):\n",
    "        super(ClassificationDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        \"\"\"\n",
    "        self.n_particles = n_particles \n",
    "\n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(theta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shapes: \n",
    "            X.shape = [batch_size, in_features]\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        batch_size = torch.tensor(X.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        \n",
    "        probas = nn.LogSoftmax(dim=2)(y_predict)\n",
    "        probas_selected = torch.gather(input=probas, dim=2, index=y.view(1, -1, 1).expand(probas.shape[0], probas.shape[1], 1)).squeeze(2)\n",
    "        log_data = torch.sum(probas_selected, dim=1)\n",
    "        \n",
    "        return train_size / batch_size  * log_data\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     1,
     45,
     47,
     51,
     58,
     65,
     73,
     77,
     79,
     82,
     98,
     103,
     116,
     129,
     226,
     257,
     267,
     287,
     328,
     352,
     407,
     466,
     477,
     484,
     490,
     516
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DistributionMover(nn.Module):\n",
    "    def __init__(self,\n",
    "                 task='app', \n",
    "                 n_particles=None,\n",
    "                 particles=None,\n",
    "                 target_density=None,\n",
    "                 n_dims=None,\n",
    "                 n_hidden_dims=None,\n",
    "                 use_latent=False,\n",
    "                 net=None,\n",
    "                 precomputed_params=None,\n",
    "                 data_distribution=None\n",
    "                ):\n",
    "        super(DistributionMover, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task (str):\n",
    "                'app' | 'net_reg' | 'net_class'\n",
    "                - approximate target distribution\n",
    "                - solve regression task using net\n",
    "                - solve classification task using net\n",
    "            n_particles (int): number of particles\n",
    "            particles (2D array_like): array which contains initialized particles\n",
    "            target_density (callable): computes probability density function of target distribution (only for 'app' task)\n",
    "            n_dims (int): dimension of the space where optimization is performed\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_latent (bool): If set to True, Subspace Stein is used\n",
    "            acr (list): List contains arcitecture of object which is used to make predictions (for 'net_reg' and' net_class' tasks)\n",
    "            precomputed_params (1D array_like): Precomputed parameters, which will be used for particles initialization\n",
    "            data_distribution (callable): computes probability over data p(D|w) (for 'net_reg' and' net_class' tasks)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.task = task\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.particles = particles\n",
    "        self.target_density = target_density\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_latent = use_latent\n",
    "        self.net = net\n",
    "        self.precomputed_params = precomputed_params\n",
    "        self.data_distribution = data_distribution\n",
    "\n",
    "        \n",
    "        if self.task == 'net_reg' or self.task == 'net_class':\n",
    "            self.n_dims = self.numel() // self.n_particles\n",
    "        if not self.use_latent:\n",
    "            self.n_hidden_dims = self.n_dims\n",
    "\n",
    "        ### Learnable samples from the target distribution\n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_hidden_dims, self.n_particles],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., 2.)\n",
    "\n",
    "        ### Class for performing linear transformations\n",
    "        if self.use_latent:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=False, \n",
    "                normalize=True\n",
    "            )\n",
    "        else:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=True, \n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "        if self.precomputed_params is not None:\n",
    "            self.particles = self.lt.inverse_transform(self.precomputed_params.unsqueeze(1).expand(self.n_dims, self.n_particles))\n",
    "\n",
    "        ### Functions of probability density of target distribution\n",
    "        if self.net is None:\n",
    "            # use unnormed probability density to speedup computations\n",
    "            if target_density is not None:\n",
    "                self.target_density = target_density\n",
    "                self.real_target_density = target_density\n",
    "            else:\n",
    "                self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                                   0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "                self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                        0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "\n",
    "        ### Number of iterations since beginning\n",
    "        self.iter = 0\n",
    "\n",
    "        ### Adagrad parameters\n",
    "        self.fudge_factor = torch.tensor(1e-6, dtype=t_type, device=device)\n",
    "        self.step_size = torch.tensor(1e-2, dtype=t_type, device=device)\n",
    "        self.auto_corr = torch.tensor(0.9, dtype=t_type, device=device)\n",
    "\n",
    "        ### Gradient history term for adagrad optimization\n",
    "        if self.use_latent:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_hidden_dims, n_particles], dtype=t_type, device=device)\n",
    "            self.historical_grad_theta_0 = torch.zeros(\n",
    "                [self.n_dims, 1], dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_dims, n_particles], dtype=t_type, device=device)\n",
    "\n",
    "        ### Factor from kernel\n",
    "        self.h = torch.tensor(0., dtype=t_type, device=device)\n",
    "\n",
    "        ### Support tensors for computations\n",
    "        self.N = torch.tensor(self.n_particles, dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.three = torch.tensor(3., dtype=t_type, device=device)\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"   \n",
    "        cnt = 0\n",
    "        for module in self.children():\n",
    "            if 'numel' in dir(module):\n",
    "                cnt += module.numel(trainable)\n",
    "        return cnt\n",
    "    \n",
    "    def calc_kernel_term_latent(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        ### power for rational kernel\n",
    "        p = torch.tensor(p, dtype=t_type, device=device) if p is not None else None\n",
    "        \n",
    "        ### theta = Atheta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### diffs[i, j] = A(theta`_i - theta`_j)\n",
    "        diffs = pairwise_diffs(real_particles, real_particles, n_particles_second=True)\n",
    "        ### dists[i, j] = ||A(theta`_i - theta`_j)||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        ### sq_dists[i, j] = ||A(theta`_i - theta`_j)||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "        \n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 2:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 5:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  med / (torch.pow(self.N, self.two) - self.one)\n",
    "        elif h_type == 6:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  med / (torch.pow(self.N, -self.one / p) - self.one)\n",
    "        elif h_type == 7:\n",
    "            med = torch.median(torch.matmul(real_particles.t(), real_particles)) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "        \n",
    "        kernel = None\n",
    "        grad_kernel = None\n",
    "        if kernel_type == 'rbf':\n",
    "            ### RBF Kernel:\n",
    "            ### kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "            kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "            ### grad_kernel[i, j] = -2/h * A(theta`_i - theta`_j) * kernel[i, j]\n",
    "            grad_kernel = -self.two / self.h * kernel.unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'imq':\n",
    "            ### IMQ Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, -self.one / self.two)\n",
    "            ### grad_kernel[i, j] = -1/h * A(theta`_i - theta`_j) * kernel^(3)[i, j]\n",
    "            grad_kernel = -self.one / self.h * torch.pow(kernel, self.three).unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'exp':\n",
    "            ### Exponential Kernel:\n",
    "            ### kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "            kernel = torch.exp(self.one / self.h * torch.matmul(real_particles.t(), real_particles))\n",
    "            ### grad_kernel[i, j] = 1/h * (Atheta`_j + theta_0) * kernel[i, j]\n",
    "            grad_kernel = 1. / self.h * kernel.unsqueeze(0) * real_particles.unsqueeze(1)\n",
    "        elif kernel_type == 'rat':\n",
    "            ### RAT Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, p)\n",
    "            ### grad_kernel[i, j] = p/h * A(theta`_i - theta`_j) * kernel^((p - 1)/p)[i, j]\n",
    "            grad_kernel = p / self.h * torch.pow(kernel, (self.p - self.one) / p).unsqueeze(0) * diffs\n",
    "            \n",
    "        return kernel, grad_kernel\n",
    "    \n",
    "    def calc_kernel_term_latent_net(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        return self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "    \n",
    "    def calc_log_prior_net(self):\n",
    "        \"\"\"\n",
    "            Traverse all modules and evaluate weights log prior\n",
    "        \"\"\"\n",
    "        log_prior = 0\n",
    "        for module in self.children():\n",
    "            if 'calc_log_prior' in dir(module):\n",
    "                log_prior += module.calc_log_prior()\n",
    "        return log_prior\n",
    "\n",
    "    def calc_log_term_latent(self):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        \n",
    "        ### theta = A theta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_term = torch.log(self.target_density(real_particles))\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "    \n",
    "        grad_log_term = real_particles.grad\n",
    "        \n",
    "        return grad_log_term\n",
    "    \n",
    "    def calc_log_term_latent_net(self, X, y, train_size):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta)) \n",
    "        Args:\n",
    "            X (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input: \n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output: \n",
    "                [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        log_data = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        log_prior = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        \n",
    "        ### get real net parameters: theta_i = A theta`_i + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### init net with real parameters\n",
    "        self.vector_to_parameters(real_particles.view(-1), self.parameters_net())\n",
    "        ### compute log prior of all weight in the net\n",
    "        log_prior = self.calc_log_prior_net()\n",
    "        \n",
    "        ### get prediction for the batch of data\n",
    "        y_predict = self.predict_net(X)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_data = self.data_distribution.calc_log_data(X, y, y_predict, train_size)\n",
    "        \n",
    "        ### log_term = log p(theta) = log p_prior(theta) + log p_data(D|theta)\n",
    "        log_term = log_prior + log_data\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "        \n",
    "        ### collect all gradients into one vector\n",
    "        grad_log_term = self.parameters_grad_to_vector(self.parameters_net()).view(-1, self.n_particles)\n",
    "            \n",
    "        return grad_log_term\n",
    "    \n",
    "    def parameters_net(self):\n",
    "        \"\"\"\n",
    "            Return all trainable parameters\n",
    "        \"\"\"\n",
    "        return chain(self.net.parameters(), self.data_distribution.parameters())\n",
    "    \n",
    "    def predict_net(self, X, inference=False):\n",
    "        \"\"\"\n",
    "            Use net to make predictions        \n",
    "            Args:\n",
    "                X (array_like): batch of data\n",
    "        \"\"\"\n",
    "        predictions = self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape))\n",
    "        if self.task == 'net_reg':\n",
    "            if inference:\n",
    "                return torch.mean(predictions, dim=0)\n",
    "            else:\n",
    "                return predictions\n",
    "        elif self.task == 'net_class':\n",
    "            if inference:\n",
    "                return torch.log(torch.mean(torch.nn.Softmax(dim=2)(predictions), dim=0))\n",
    "            else:\n",
    "                return predictions\n",
    "\n",
    "    def update_latent(self, \n",
    "                      h_type, kernel_type='rbf', p=None, \n",
    "                      step_size=None, \n",
    "                      move_theta_0=False, \n",
    "                      burn_in=False, burn_in_coeff=None,\n",
    "                      epoch=None\n",
    "                     ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "            \n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent()\n",
    "        \n",
    "        ### Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "        \n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "        \n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "        \n",
    "        ### Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            ### Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "            \n",
    "            ### Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            ### Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "               \n",
    "    def update_latent_net(self,\n",
    "                          h_type, kernel_type='rbf', p=None,\n",
    "                          X_batch=None, y_batch=None, train_size=None,\n",
    "                          step_size=None,\n",
    "                          move_theta_0=False, \n",
    "                          burn_in=False, burn_in_coeff=None, \n",
    "                          epoch=None\n",
    "                         ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "            \n",
    "        self.iter += 1\n",
    "        self.net.zero_grad()\n",
    "        self.data_distribution.zero_grad()\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent_net(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_net(X_batch, y_batch, train_size)\n",
    "        \n",
    "        ### Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "        \n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "                \n",
    "        ### Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            ### Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "            \n",
    "            ### Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            ### Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_to_parameters(vec, parameters):\n",
    "        pointer = 0\n",
    "        for param in parameters:\n",
    "            # The length of the parameter\n",
    "            num_param = param.numel()\n",
    "            # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "            param.data = vec[pointer:pointer + num_param].view_as(param).data\n",
    "            # Increment the pointer\n",
    "            pointer += num_param\n",
    "            \n",
    "    @staticmethod \n",
    "    def parameters_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    @staticmethod \n",
    "    def parameters_grad_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.grad.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        \"\"\"\n",
    "        destination = super(DistributionMover, self).state_dict(destination, prefix, keep_vars)\n",
    "        \n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=self._version)\n",
    "        \n",
    "        destination[prefix + 'particles'] = self.particles if keep_vars else self.particles.data\n",
    "        destination[prefix + 'historical_grad'] = self.historical_grad if keep_vars else self.historical_grad.data\n",
    "        if self.use_latent:\n",
    "            destination[prefix + 'historical_grad_theta_0'] = self.historical_grad_theta_0 if keep_vars else self.historical_grad_theta_0.data\n",
    "            self.lt.state_dict(destination, prefix + 'lt' + '.', keep_vars=keep_vars)\n",
    "        destination[prefix + 'step_size'] = self.step_size\n",
    "        destination[prefix + 'iter'] = self.iter\n",
    "        destination[prefix + 'epoch'] = self.epoch\n",
    "                \n",
    "        return destination\n",
    "    \n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        super(DistributionMover, self).load_state_dict(state_dict, prefix)\n",
    "        \n",
    "        self.particles.copy_(state_dict[prefix + 'particles'])\n",
    "        self.historical_grad.copy_(state_dict[prefix + 'historical_grad'])\n",
    "        self.step_size = state_dict[prefix + 'step_size']\n",
    "        self.iter = state_dict[prefix + 'iter']\n",
    "        self.epoch = state_dict[prefix + 'epoch']\n",
    "        \n",
    "        if self.use_latent:\n",
    "            self.historical_grad_theta_0.copy_(state_dict[prefix + 'historical_grad_theta_0'])\n",
    "            self.lt.load_state_dict(state_dict, prefix + 'lt' + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LRStrategy:\n",
    "    def __init__(self, step_size, factor=0.1, n_epochs=1, patience=10):\n",
    "        \"\"\"\n",
    "            Multiply @step_size by factor each @n_epochs epochs\n",
    "            Freeze @step_size after @patience epochs\n",
    "        \"\"\"\n",
    "        self.step_size = step_size\n",
    "        self.factor = factor\n",
    "        self.n_epochs = n_epochs\n",
    "        self.patience = patience\n",
    "        self.iter = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.iter += 1\n",
    "        if self.iter < self.patience and self.iter % self.n_epochs == 0:\n",
    "            self.step_size *= self.factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Add some methods to nn.Sequential to make code clear \n",
    "\n",
    "setattr(nn.Sequential, \"numel\", DistributionMover.numel)\n",
    "setattr(nn.Sequential, \"calc_log_prior\", DistributionMover.calc_log_prior_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Bostor housing dataset for regression task\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = (torch.tensor(X_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(X_test, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_test, dtype=t_type, device=device))\n",
    "\n",
    "### Linear Regression baseline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.tensor(model.predict(X_test), dtype=t_type, device=device), y_test), \n",
    "      torch.nn.MSELoss()(torch.tensor(model.predict(X_train), dtype=t_type, device=device), y_train))\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.mean(y_train).expand(y_test.shape[0]), y_test), \n",
    "      torch.nn.MSELoss()(torch.mean(y_train).expand(y_train.shape[0]), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Check all functions\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=5, n_particles=10, use_latent=True)\n",
    "dm.calc_log_term_latent()\n",
    "dm.calc_kernel_term_latent(0)\n",
    "dm.update_latent(0)\n",
    "\n",
    "net = nn.Sequential(SteinLinear(13, 1, 10, use_var_prior=True))\n",
    "dd = RegressionDistribution(n_particles=10, use_var_prior=False)\n",
    "dm = DistributionMover(task='net_reg', n_hidden_dims=5, n_particles=10, use_latent=True, net=net, data_distribution=dd)\n",
    "dm.calc_log_term_latent_net(X_train, y_train, X_train.shape[0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(13, 1, 100, use_var_prior=False, alpha=1e-2, use_bias=True))\n",
    "data_distr = RegressionDistribution(100, use_var_prior=False, betta=1e-1)\n",
    "dm = DistributionMover(task='net_reg', n_particles=100, use_latent=False, net=net, data_distribution=data_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# alpha = dm.net[0].alpha\n",
    "# betta = dm.data_distribution.betta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Append column of ones to data \n",
    "# XX = X_train\n",
    "# XX_test = X_test\n",
    "XX = torch.cat([X_train, torch.ones([X_train.shape[0], 1], dtype=t_type, device=device)], dim=1)\n",
    "XX_test = torch.cat([X_test, torch.ones([X_test.shape[0], 1], dtype=t_type, device=device)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sigma = torch.inverse(betta * XX.t() @ XX + alpha * torch.eye(XX.shape[1], dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mu = betta * sigma @ XX.t() @ y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu @ XX.t(), y_train), torch.nn.MSELoss()(mu @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_test = torch.load('particles_12400.txt').cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_mean = torch.mean(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(particles_mean @ XX.t(), y_train), torch.nn.MSELoss()(particles_mean @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.var(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.diag(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import *\n",
    "\n",
    "mod = WLS(y_train.data.cpu().numpy(), XX.data.cpu().numpy())\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stein_mu = (torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, torch.tensor(results.params, dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, particles_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(sum(np.logical_and(results.conf_int()[:,0] < stein_mu.data.cpu().numpy(), stein_mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < mu.data.cpu().numpy(), mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < particles_mean.data.cpu().numpy(), particles_mean.data.cpu().numpy() < results.conf_int()[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    step_size = 0.00075\n",
    "    dm.historical_grad.zero_()\n",
    "    for _ in range(100000):\n",
    "        dm.update_latent_net(h_type=1, kernel_type='rbf', p=-1, X_batch=X_train, y_batch=y_train, train_size=X_train.shape[0], step_size=step_size)\n",
    "        train_loss = torch.nn.MSELoss()(dm.predict_net(X_train, inference=True).view(-1), y_train)\n",
    "        test_loss = torch.nn.MSELoss()(dm.predict_net(X_test, inference=True).view(-1), y_test)\n",
    "        \n",
    "        if _ % 10 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train): {1:.3f}\\t Empirical Loss(Test): {2:.3f}\\t Kernel factor: {3:.3f}'.format(\n",
    "                            _, train_loss, test_loss, dm.h))\n",
    "            \n",
    "            plot_projections(dm, use_real=True)\n",
    "            plot_projections(dm, use_real=False)\n",
    "            plt.pause(1e-300)\n",
    "            \n",
    "        if _ % 3000 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset with class selection\n",
    "\n",
    "sys.path.insert(0, '/home/m.nakhodnov/Samsung-Tasks/Datasets/MyMNIST')\n",
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform)\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform)\n",
    "\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=100, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(dm,\n",
    "          dataloader_train, dataloader_test,\n",
    "          lr_str, start_epoch, end_epoch, n_epochs_save=20, n_epochs_log=1,\n",
    "          move_theta_0=False, plot_graphs=True, verbose=False,\n",
    "          checkpoint_file_name=None, plots_file_name=None, log_file_name=None,\n",
    "          n_warmup_epochs=16, n_previous=10\n",
    "          ):\n",
    "    ### Get all y_test in one tensor\n",
    "    y_test_all = torch.tensor([], dtype=torch.int64, device=device)\n",
    "    for _, y_test in dataloader_test:\n",
    "        y_test = y_test.to(device=device)\n",
    "        y_test_all = torch.cat([y_test_all, y_test.data.detach().clone()], dim=0)\n",
    "    ### WARNING: May be incorrect if output features of dm.net != n_classes\n",
    "    n_classes = len(dataloader_train.dataset.class_nums)\n",
    "\n",
    "    ### Train loss/accuracy\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    ### Test loss/accuracy\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    ### Mean loss/accuracy from @n_warmup_epochs to current epoch\n",
    "    test_losses_mean = []\n",
    "    test_accs_mean = []\n",
    "    predictions_test_cummulative = torch.zeros([1, 1], dtype=t_type, device=device)\n",
    "    ### Mean loss/accuracy from (current epoch - n_previous)  to current epoch\n",
    "    test_losses_mean_previous = []\n",
    "    test_accs_mean_previous = []\n",
    "    predictions_test_previous = torch.zeros([n_previous, y_test_all.shape[0], n_classes], dtype=t_type, device=device)\n",
    "    ### Index of 'oldest' element in predictions_test_previous\n",
    "    pointer_to_the_back = 0\n",
    "\n",
    "    if log_file_name is not None:\n",
    "        log_file = open(log_file_name, 'a')\n",
    "        log_file.write('\\rNew run of training.\\r')\n",
    "        log_file.close()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, end_epoch):\n",
    "            epoch_since_start = epoch - start_epoch\n",
    "\n",
    "            ### One update of particles via all dataloader_train\n",
    "            for X, y in dataloader_train:\n",
    "                X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "                y = y.to(device=device)\n",
    "                burn_in_coeff = max(1. - (1. - 1.) / 20. * epoch, 1.)\n",
    "                dm.update_latent_net(h_type=0, kernel_type='rbf', p=None,\n",
    "                                     X_batch=X, y_batch=y,\n",
    "                                     train_size=len(dataloader_train.dataset),\n",
    "                                     step_size=lr_str.step_size,\n",
    "                                     move_theta_0=move_theta_0,\n",
    "                                     burn_in=True, burn_in_coeff=burn_in_coeff,\n",
    "                                     epoch=epoch\n",
    "                                     )\n",
    "\n",
    "            ### Evaluate cross entropy and accuracy over dataloader_train\n",
    "            train_loss = 0.\n",
    "            train_acc = 0.\n",
    "            for X_train, y_train in dataloader_train:\n",
    "                X_train = X_train.double().to(device=device).view(X.shape[0], -1)\n",
    "                y_train = y_train.to(device=device)\n",
    "\n",
    "                net_pred = dm.predict_net(X_train, inference=True)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                train_loss -= torch.sum(torch.gather(net_pred, 1, y_train.view(-1, 1)))\n",
    "                train_acc += torch.sum(y_pred == y_train).float()\n",
    "            train_loss /= (len(dataloader_train.dataset) + 0.)\n",
    "            train_acc /= (len(dataloader_train.dataset) + 0.)\n",
    "\n",
    "            ### Evaluate cross entropy and accuracy over dataloader_test\n",
    "            test_loss = 0.\n",
    "            test_acc = 0.\n",
    "            predictions_test_current = torch.tensor([], dtype=t_type, device=device)\n",
    "            for X_test, y_test in dataloader_test:\n",
    "                X_test = X_test.double().to(device=device).view(X.shape[0], -1)\n",
    "                y_test = y_test.to(device=device)\n",
    "\n",
    "                ### Get output of net before Softmax, mean and log, Shape = [n_particles, batch_size, output_features]\n",
    "                net_pred_pure = dm.predict_net(X_test, inference=False)\n",
    "                net_pred_pure = torch.mean(torch.nn.Softmax(dim=2)(net_pred_pure), dim=0)\n",
    "                predictions_test_current = torch.cat([predictions_test_current, net_pred_pure.data.detach().clone()],\n",
    "                                                     dim=0)\n",
    "\n",
    "                net_pred = torch.log(net_pred_pure)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                test_loss -= torch.sum(torch.gather(net_pred, 1, y_test.view(-1, 1)))\n",
    "                test_acc += torch.sum(y_pred == y_test).float()\n",
    "            test_loss /= (len(dataloader_test.dataset) + 0.)\n",
    "            test_acc /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            ### Evaluate cross entropy and accuracy over dataloader_test using\n",
    "            ### all predictions from previous (@epoch_since_start - @n_warmup_epochs) epochs\n",
    "            test_loss_mean = 0.\n",
    "            test_acc_mean = 0.\n",
    "            if epoch_since_start >= n_warmup_epochs:\n",
    "                predictions_test_cummulative = (\n",
    "                        predictions_test_cummulative * (epoch_since_start - n_warmup_epochs) / (\n",
    "                            epoch_since_start - n_warmup_epochs + 1.) +\n",
    "                        predictions_test_current / (epoch_since_start - n_warmup_epochs + 1.))\n",
    "                log_predictions_test = torch.log(predictions_test_cummulative)\n",
    "                y_pred_all = torch.argmax(log_predictions_test, dim=1)\n",
    "\n",
    "                test_loss_mean = -torch.sum(torch.gather(log_predictions_test, 1, y_test_all.view(-1, 1)))\n",
    "                test_acc_mean = torch.sum(y_pred_all == y_test_all).float()\n",
    "                test_loss_mean /= (len(dataloader_test.dataset) + 0.)\n",
    "                test_acc_mean /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            ### Evaluate cross entropy and accuracy over dataloader_test using\n",
    "            ### all predictions from previous @n_previous epochs\n",
    "            test_loss_mean_previous = 0.\n",
    "            test_acc_mean_previous = 0.\n",
    "            predictions_test_previous[pointer_to_the_back] = predictions_test_current\n",
    "            if pointer_to_the_back + 1 == n_previous:\n",
    "                pointer_to_the_back = 0\n",
    "            else:\n",
    "                pointer_to_the_back += 1\n",
    "            if epoch_since_start + 1 >= n_previous:\n",
    "                log_predictions_test = torch.log(torch.mean(predictions_test_previous, dim=0))\n",
    "                y_pred_all = torch.argmax(log_predictions_test, dim=1)\n",
    "                test_loss_mean_previous = -torch.sum(torch.gather(log_predictions_test, 1, y_test_all.view(-1, 1)))\n",
    "                test_acc_mean_previous = torch.sum(y_pred_all == y_test_all).float()\n",
    "                test_loss_mean_previous /= (len(dataloader_test.dataset) + 0.)\n",
    "                test_acc_mean_previous /= (len(dataloader_test.dataset) + 0.)\n",
    "\n",
    "            ### Append evaluated losses and accuracies\n",
    "            train_losses.append(train_loss.data[0].cpu().numpy())\n",
    "            train_accs.append(train_acc.data[0].cpu().numpy())\n",
    "            test_losses.append(test_loss.data[0].cpu().numpy())\n",
    "            test_accs.append(test_acc.data[0].cpu().numpy())\n",
    "            if epoch_since_start >= n_warmup_epochs:\n",
    "                test_losses_mean.append(test_loss_mean.data[0].cpu().numpy())\n",
    "                test_accs_mean.append(test_acc_mean.data[0].cpu().numpy())\n",
    "            else:\n",
    "                test_losses_mean.append(None)\n",
    "                test_accs_mean.append(None)\n",
    "            if epoch_since_start + 1 >= n_previous:\n",
    "                test_losses_mean_previous.append(test_loss_mean_previous.data[0].cpu().numpy())\n",
    "                test_accs_mean_previous.append(test_acc_mean_previous.data[0].cpu().numpy())\n",
    "            else:\n",
    "                test_losses_mean_previous.append(None)\n",
    "                test_accs_mean_previous.append(None)\n",
    "\n",
    "            ### Print log into console and file\n",
    "            if epoch % n_epochs_log == 0:\n",
    "                sys.stdout.write(\n",
    "                    ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                     '\\nEmpirical Loss (Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {4:.3f}/{5:.3f}/{6:.3f}/{7:.3f}' +\n",
    "                     '\\nAccuracy (Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {8:.3f}/{9:.3f}/{10:.3f}/{11:.3f}\\t'\n",
    "                     ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                              train_loss, test_loss, test_loss_mean, test_loss_mean_previous,\n",
    "                              train_acc, test_acc, test_acc_mean, test_acc_mean_previous\n",
    "                              )\n",
    "                )\n",
    "                if log_file_name is not None:\n",
    "                    log_file = open(log_file_name, 'a')\n",
    "                    log_file.write(\n",
    "                        ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                         '\\nEmpirical Loss(Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {4:.3f}/{5:.3f}/{6:.3f}/{7:.3f}' +\n",
    "                         '\\nAccuracy(Train/Test/Test (Mean (All))/Test (Mean (n_prev))): {8:.3f}/{9:.3f}/{10:.3f}/{11:.3f}\\t'\n",
    "                         ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                                  train_loss, test_loss, test_loss_mean, test_loss_mean_previous,\n",
    "                                  train_acc, test_acc, test_acc_mean, test_acc_mean_previous\n",
    "                                  )\n",
    "                    )\n",
    "                    log_file.close()\n",
    "\n",
    "            if epoch % n_epochs_save == 0 and epoch > start_epoch and checkpoint_file_name is not None:\n",
    "                torch.save(dm.state_dict(), checkpoint_file_name.format(epoch))\n",
    "\n",
    "            ### Update step_size\n",
    "            lr_str.step()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    if plot_graphs:\n",
    "        print_plots([[train_losses, test_losses, test_losses_mean, test_losses_mean_previous],\n",
    "                     [train_accs, test_accs, test_accs_mean, test_accs_mean_previous]],\n",
    "                    [['Epochs', ''],\n",
    "                     ['Epochs', '% * 1e-2']],\n",
    "                    [['Cross Entropy Loss (Train)', 'Cross Entropy Loss (Test)', 'Cross Entropy Loss (Test (Mean))',\n",
    "                      'Cross Entropy Loss (Test (Mean (n_prev)))'],\n",
    "                     ['Accuracy (Train)', 'Accuracy (Test)', 'Accuracy (Mean)', 'Accuracy (Mean (n_prev))']\n",
    "                     ],\n",
    "                    plots_file_name\n",
    "                    )\n",
    "    if checkpoint_file_name is not None:\n",
    "        torch.save(dm.state_dict(), checkpoint_file_name.format(epoch))\n",
    "\n",
    "    if verbose:\n",
    "        return (train_losses, test_losses, test_losses_mean, test_losses_mean_previous,\n",
    "                train_accs, test_accs, test_accs_mean, test_accs_mean_previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 1. particles - 1 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### MAP estimate for MNIST classification task\n",
    "net_1 = nn.Sequential(SteinLinear(28 * 28, 18, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 1, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_1 = ClassificationDistribution(1)\n",
    "dm_1 = DistributionMover(task='net_class', n_particles=1, use_latent=False, net=net_1, data_distribution=data_distr_1)\n",
    "lr_str_1 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_1 = 'model_1'\n",
    "version_1 = 19\n",
    "checkpoint_file_name_1 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_1 + '.pth'\n",
    "plots_file_name_1 = './Experiments/Plots/' + own_name_1 + '.png'\n",
    "log_file_name_1 = './Experiments/Logs/' + own_name_1 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_1.format(version_1)):\n",
    "    dm_1.load_state_dict(torch.load(checkpoint_file_name_1.format(version_1)))\n",
    "    lr_str_1.step_size = dm_1.step_size\n",
    "    lr_str_1.iter = dm_1.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_1,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_1, start_epoch=lr_str_1.iter, end_epoch=26,\n",
    "      checkpoint_file_name=checkpoint_file_name_1, plots_file_name=plots_file_name_1, log_file_name=log_file_name_1,\n",
    "      n_warmup_epochs=5\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X, y = next(iter(dataloader_m_train))\n",
    "# X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_1.update_latent_net(h_type=0, kernel_type='rbf', p=None, X_batch=X, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_1.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2. particles - 5 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_2 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_2 = ClassificationDistribution(5)\n",
    "dm_2 = DistributionMover(task='net_class',\n",
    "                         n_particles=5,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_2,\n",
    "                         data_distribution=data_distr_2\n",
    "                        )\n",
    "lr_str_2 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_2 = 'model_2'\n",
    "version_2 = 0\n",
    "checkpoint_file_name_2 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_2 + '.pth'\n",
    "plots_file_name_2 = './Experiments/Plots/' + own_name_2 + '.png'\n",
    "log_file_name_2 = './Experiments/Logs/' + own_name_2 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_2.format(version_2)):\n",
    "    dm_2.load_state_dict(torch.load(checkpoint_file_name_2.format(version_2)))\n",
    "    lr_str_2.step_size = dm_2.step_size\n",
    "    lr_str_2.iter = dm_2.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_2,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_2, start_epoch=lr_str_2.iter, end_epoch=lr_str_2.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_2, plots_file_name=plots_file_name_2, log_file_name=log_file_name_2,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X, y = next(iter(dataloader_m_train))\n",
    "# X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_2.update_latent_net(h_type=0, kernel_type='rbf', p=None, X_batch=X, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_2.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3. particles - 5 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_3 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_3 = ClassificationDistribution(5)\n",
    "dm_3 = DistributionMover(task='net_class',\n",
    "                       n_particles=5,\n",
    "                       use_latent=False,\n",
    "                       net=net_3,\n",
    "                       data_distribution=data_distr_3\n",
    "                      )\n",
    "lr_str_3 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_3 = 'model_3'\n",
    "version_3 = 0\n",
    "checkpoint_file_name_3 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_3 + '.pth'\n",
    "plots_file_name_3 = './Experiments/Plots/' + own_name_3 + '.png'\n",
    "log_file_name_3 = './Experiments/Logs/' + own_name_3 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_3.format(version_3)):\n",
    "    dm_3.load_state_dict(torch.load(checkpoint_file_name_3.format(version_3)))\n",
    "    lr_str_3.step_size = dm_3.step_size\n",
    "    lr_str_3.iter = dm_3.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_3,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_3, start_epoch=lr_str_3.iter, end_epoch=lr_str_3.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_3, plots_file_name=plots_file_name_3, log_file_name=log_file_name_3,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X, y = next(iter(dataloader_m_train))\n",
    "# X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_3.update_latent_net(h_type=0, kernel_type='rbf', p=None, X_batch=X, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_3.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 4. particles - 20 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_4 = nn.Sequential(SteinLinear(28 * 28, 18, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 20, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_4 = ClassificationDistribution(20)\n",
    "dm_4 = DistributionMover(task='net_class',\n",
    "                         n_particles=20,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_4,\n",
    "                         data_distribution=data_distr_4\n",
    "                        )\n",
    "lr_str_4 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_4 = 'model_4'\n",
    "version_4 = 0\n",
    "checkpoint_file_name_4 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_4 + '.pth'\n",
    "plots_file_name_4 = './Experiments/Plots/' + own_name_4 + '.png'\n",
    "log_file_name_4 = './Experiments/Logs/' + own_name_4 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_4.format(version_4)):\n",
    "    dm_4.load_state_dict(torch.load(checkpoint_file_name_4.format(version_4)))\n",
    "    lr_str_4.step_size = dm_4.step_size\n",
    "    lr_str_4.iter = dm_4.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_4,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_4, start_epoch=lr_str_4.iter, end_epoch=lr_str_4.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_4, plots_file_name=plots_file_name_4, log_file_name=log_file_name_4,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "config_name = 'config_model_23'\n",
    "config = importlib.import_module('Experiments.Configs.' + config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dm = nn.Sequential(SteinLinear(28 * 28, 300, config.n_particles, use_var_prior=config.use_var_prior, alpha=config.alpha),\n",
    "                       nn.Tanh(),\n",
    "                       SteinLinear(300, 100, config.n_particles, use_var_prior=config.use_var_prior, alpha=config.alpha),\n",
    "                       nn.Tanh(),\n",
    "                       SteinLinear(100, 10, config.n_particles, use_var_prior=config.use_var_prior, alpha=config.alpha)\n",
    "                      ).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distr = ClassificationDistribution(config.n_particles)\n",
    "dm = DistributionMover(task='net_class',\n",
    "                       n_particles=config.n_particles,\n",
    "                       n_hidden_dims=config.n_hidden_dims,\n",
    "                       use_latent=config.use_latent,\n",
    "                       net=net_dm,\n",
    "                       data_distribution=data_distr)\n",
    "lr_str = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file_name = ('./Experiments/Checkpoints/' + 'e{0}-{1}_' + config.experiment_name + '.pth').format(0, 199)\n",
    "dm.load_state_dict(torch.load(checkpoint_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 300),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(300, 100),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(100, 10)\n",
    ").to(device=device).double()\n",
    "\n",
    "\n",
    "checkpoint_file_name = ('./Experiments/Checkpoints/' + 'e{0}-{1}_' + 'ml_est' + '.pth').format(0,199)\n",
    "net.load_state_dict(torch.load(checkpoint_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modify_sample_net(net, samples, real_labels, eps=1e-2, iters=1):\n",
    "    for idx in range(iters):\n",
    "        samples = samples.detach().clone().requires_grad_(True)\n",
    "        \n",
    "        predictions = net(samples)\n",
    "        predictions = torch.log(torch.nn.Softmax()(predictions))\n",
    "        \n",
    "        train_loss = -torch.sum(torch.gather(predictions, 1, real_labels.view(-1, 1)))\n",
    "    \n",
    "        train_loss.backward()\n",
    "        perturbation = torch.sign(samples.grad)    \n",
    "        samples = torch.clamp(samples + eps * perturbation, -0.42, 2.8)\n",
    "        \n",
    "    return samples.detach(), perturbation.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def modify_sample(dm, samples, real_labels, eps=1e-2, iters=1):\n",
    "    for idx in range(iters):\n",
    "        samples = samples.detach().clone().requires_grad_(True)\n",
    "        \n",
    "        predictions = dm.predict_net(samples, inference=False)\n",
    "        predictions_one_particle = predictions[2:3]\n",
    "        predictions_one_particle = torch.log(torch.mean(torch.nn.Softmax(dim=2)(predictions_one_particle), dim=0))\n",
    "\n",
    "        train_loss = -torch.sum(torch.gather(predictions_one_particle, 1, real_labels.view(-1, 1)))\n",
    "\n",
    "        train_loss.backward()\n",
    "        perturbation = torch.sign(samples.grad)        \n",
    "        samples = torch.clamp(samples + eps * perturbation, -0.42, 2.8)\n",
    "\n",
    "    return samples.detach(), perturbation.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, real_labels = next(iter(dataloader_m_test))\n",
    "samples = samples.double().to(device=device).view(samples.shape[0], -1)\n",
    "real_labels = real_labels.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_samples, perturbation = modify_sample(dm, samples, real_labels, 0.01, 20)\n",
    "modified_samples_net, perturbation_net = modify_sample_net(net, samples, real_labels, 0.01, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADLhJREFUeJzt3U+InPUdx/HPp1Ev6iGSNYSYNFZCqRQau0MopJQUUaKX6MFiDpKCsB4UFDxUvOilEErV9lCEWIMp+AdBrTmE1hCEVCjirAQTm7YRSTUmZDfkYDxJ9NvDPpE17s5M5vm7+32/YJiZZ/4833lmP/vMzPd5np8jQgDy+V7bBQBoB+EHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5DUFU3ObNWqVbFhw4axHz89Pb3obZOTk2M/L9ox6P2U6n1Ph817mGG1tfXaTpw4obNnz3qU+7rM5r22t0n6o6QVkv4cEbsG3b/X60W/3y8zv0VvYzPlpWfQ+ynV+54Om/cww2pr67X1ej31+/2RXtzYH/ttr5D0J0l3SLpZ0g7bN4/7fACaVeY7/2ZJH0XExxHxpaRXJG2vpiwAdSsT/rWSPp13/WQx7VtsT9nu2+7Pzs6WmB2AKpUJ/0LfK77zRSYidkdELyJ6ExMTJWYHoEplwn9S0rp512+QdKpcOQCaUib870naaPtG21dJulfSvmrKAlC3sfv8EXHB9kOS/q65Vt+eiPiwssoWnmdtz91m2ymrNpdp3fMu2UKv7bnnK7WRT0Tsl7S/kkoANIrNe4GkCD+QFOEHkiL8QFKEH0iK8ANJNbo/f5e12XMuu3vpIGVf13Ld/qHLu/Q2tUxZ8wNJEX4gKcIPJEX4gaQIP5AU4QeSotXXAV1ul3W5tjbV2Z5tCmt+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iKPv8yV/euq3XOv85tDNh+gTU/kBbhB5Ii/EBShB9IivADSRF+ICnCDyRVqs9v+4Sk85K+knQhInpVFLXcLOV9v8seunspDFVdhy4fjv2iKjby+WVEnK3geQA0iI/9QFJlwx+S3rI9bXuqioIANKPsx/4tEXHK9vWSDtj+d0Qcmn+H4p/ClCStX7++5OwAVKXUmj8iThXnM5LekLR5gfvsjoheRPQmJibKzA5AhcYOv+2rbV978bKk2yUdraowAPUq87F/taQ3ipbGFZJeioi/VVIVgNqNHf6I+FjSTyqspZS2h1xeqrrcK+9ybcMshdpp9QFJEX4gKcIPJEX4gaQIP5AU4QeS4tDdhS638sq0IZdCywntYM0PJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0l1qs/f5V57ncr24rvcy+/qEN1gzQ+kRfiBpAg/kBThB5Ii/EBShB9IivADSXWqz19nX7fNbQiWc7+6zuXa5fes7kPFN4E1P5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kNTT8tvfYnrF9dN6062wfsH28OF9ZRTG2xz4tZcv5tS1VGd6TUdb8L0jadsm0xyQdjIiNkg4W1wEsIUPDHxGHJJ27ZPJ2SXuLy3sl3VVxXQBqNu53/tURcVqSivPrqysJQBNq/8HP9pTtvu3+7Oxs3bMDMKJxw3/G9hpJKs5nFrtjROyOiF5E9CYmJsacHYCqjRv+fZJ2Fpd3SnqzmnIANGWUVt/Lkv4p6Ye2T9q+X9IuSbfZPi7ptuI6gCVk6P78EbFjkZturbiWWvdxLvvcZXq7ZfvCXT72fZv7pS+Xfvvlqup1s4UfkBThB5Ii/EBShB9IivADSRF+IKlGD909PT09sE1B22hhXTjMM6pVJgeDbu/1eiPXwJofSIrwA0kRfiApwg8kRfiBpAg/kBThB5JqNPyTk5OKiEVPWBiHNF9+r60LWPMDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKN7s/fpqx94WGvm2MotKML27Ww5geSIvxAUoQfSIrwA0kRfiApwg8kRfiBpIaG3/Ye2zO2j86b9qTtz2wfLk531lvmcMP2/R50HIEu9FzbUuZYAWVPdRv2nmf/exhlzf+CpG0LTH8mIjYVp/3VlgWgbkPDHxGHJJ1roBYADSrznf8h2x8UXwtWVlYRgEaMG/5nJd0kaZOk05KeWuyOtqds9233Z2dnx5wdgKqNFf6IOBMRX0XE15Kek7R5wH13R0QvInoTExPj1gmgYmOF3/aaeVfvlnR0sfsC6Kahu/TaflnSVkmrbJ+U9ISkrbY3SQpJJyQ9UGONAGrgJnuatgfOjH3LMV+X++1dPU5Cr9dTv98f6Y+ZLfyApAg/kBThB5Ii/EBShB9IivADSTFE94iWat11K7NrbJu71Zbd3bhs7V0Yepw1P5AU4QeSIvxAUoQfSIrwA0kRfiApwg8klWaI7mHK9JXL9qS7vDtx2dfW1e0g6n7PRtnGoa7nHhVrfiApwg8kRfiBpAg/kBThB5Ii/EBShB9Iij5/B3S1F76cld22os73rKm/B9b8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5DU0PDbXmf7bdvHbH9o++Fi+nW2D9g+XpyvrL9cZFL22PoYbJQ1/wVJj0bEjyT9TNKDtm+W9JikgxGxUdLB4jqAJWJo+CPidES8X1w+L+mYpLWStkvaW9xtr6S76ioSQPUu6zu/7Q2SbpH0rqTVEXFamvsHIen6qosDUJ+Rw2/7GkmvSXokIj6/jMdN2e7b7s/Ozo5TI4AajBR+21dqLvgvRsTrxeQzttcUt6+RNLPQYyNid0T0IqI3MTFRRc0AKjDKr/2W9LykYxHx9Lyb9knaWVzeKenN6ssDUJdRdundIuk+SUdsHy6mPS5pl6RXbd8v6RNJ99RT4mi6vItmZrTkumto+CPiHUmLvYO3VlsOgKawhR+QFOEHkiL8QFKEH0iK8ANJEX4gKQ7dXWizHz1sG4M6a1vKw4uzDUE5rPmBpAg/kBThB5Ii/EBShB9IivADSRF+IKll0+eve3/8Lm8HMEjZuoc9vsvbKNQ577LLpc55j4o1P5AU4QeSIvxAUoQfSIrwA0kRfiApwg8ktWz6/HVbqsf1r7sXvlT351+q72eVWPMDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFJDw297ne23bR+z/aHth4vpT9r+zPbh4nTnsOeanp6W7UVPI9Qy9mOXszLLJSJKnepUtra26q7boNc1OTk58vOMspHPBUmPRsT7tq+VNG37QHHbMxHx+zHqB9CyoeGPiNOSTheXz9s+Jmlt3YUBqNdlfee3vUHSLZLeLSY9ZPsD23tsr1zkMVO2+7b7pSoFUKmRw2/7GkmvSXokIj6X9KykmyRt0twng6cWelxE7I6IXkT0KqgXQEVGCr/tKzUX/Bcj4nVJiogzEfFVRHwt6TlJm+srE0DVRvm135Kel3QsIp6eN33NvLvdLelo9eUBqMsov/ZvkXSfpCO2DxfTHpe0w/YmSSHphKQHhj3R5OSk+v3xv/p39XDIZXebLfu6Bj2+zUNMN/H8GN8ov/a/I2mhv6D91ZcDoCls4QckRfiBpAg/kBThB5Ii/EBShB9IqlOH7m6zJ13nMNht9rrp47ejy38TF7HmB5Ii/EBShB9IivADSRF+ICnCDyRF+IGk3GS/0faspP/Nm7RK0tnGCrg8Xa2tq3VJ1DauKmv7fkRMjHLHRsP/nZnb/a4e26+rtXW1LonaxtVWbXzsB5Ii/EBSbYd/d8vzH6SrtXW1LonaxtVKba1+5wfQnrbX/ABa0kr4bW+z/R/bH9l+rI0aFmP7hO0jxcjDrQ4xVgyDNmP76Lxp19k+YPt4cb7gMGkt1XbZIzfXVNtiI0u3uuyqHPG6knqa/thve4Wk/0q6TdJJSe9J2hER/2q0kEXYPiGpFxGt94Rt/0LSF5L+EhE/Lqb9TtK5iNhV/ONcGRG/6UhtT0r6ou2Rm4sBZdbMH1la0l2Sfq0Wl92Aun6lFpZbG2v+zZI+ioiPI+JLSa9I2t5CHZ0XEYcknbtk8nZJe4vLezX3x9O4RWrrhIg4HRHvF5fPS7o4snSry25AXa1oI/xrJX067/pJdWvI75D0lu1p21NtF7OA1cWw6ReHT7++5XouNXTk5iZdMrJ0Z5bdOCNeV62N8C90fKMutRy2RMRPJd0h6cHi4y1GM9LIzU1ZYGTpThh3xOuqtRH+k5LWzbt+g6RTLdSxoIg4VZzPSHpD3Rt9+MzFQVKL85mW6/lGl0ZuXmhkaXVg2XVpxOs2wv+epI22b7R9laR7Je1roY7vsH118UOMbF8t6XZ1b/ThfZJ2Fpd3SnqzxVq+pSsjNy82srRaXnZdG/G6lY18ilbGHyStkLQnIn7beBELsP0Dza3tpbkjG7/UZm22X5a0VXN7fZ2R9ISkv0p6VdJ6SZ9IuiciGv/hbZHatmruo+s3Izdf/I7dcG0/l/QPSUckfV1Mflxz369bW3YD6tqhFpYbW/gBSbGFH5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpP4PnxVz6hCxquIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_idx = 1\n",
    "plt.imshow(samples[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(modified_samples[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(modified_samples_net[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(perturbation[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(perturbation_net[plot_idx].view(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(dm.predict_net(samples, inference=True), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5820e-11, 1.3675e-08, 1.0000e+00, 5.9114e-10, 2.9623e-15, 1.5808e-12,\n",
       "        4.9553e-11, 1.1388e-13, 4.7097e-10, 1.9104e-14],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(dm.predict_net(samples, inference=True)[plot_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_net = torch.argmax(net(samples), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4230e-11, 1.2675e-05, 9.9999e-01, 7.5929e-07, 5.3609e-13, 1.2665e-08,\n",
       "        6.7657e-11, 5.1346e-16, 1.6951e-10, 2.0722e-17],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax()(net(samples))[plot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_modified = torch.argmax(dm.predict_net(modified_samples, inference=True), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2769e-09, 6.9147e-05, 9.9993e-01, 1.3869e-06, 5.0071e-14, 8.2623e-08,\n",
       "        5.1771e-09, 2.5483e-11, 1.1067e-07, 1.0522e-11],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(dm.predict_net(modified_samples, inference=True)[plot_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_modified_net = torch.argmax(net(modified_samples_net), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.3577e-11, 1.1250e-03, 9.9887e-01, 3.9611e-07, 2.2810e-11, 6.2506e-07,\n",
       "        5.5752e-10, 3.8357e-15, 2.5324e-10, 2.6056e-16],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax()(net(modified_samples_net))[plot_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(predictions - predictions_modified != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(predictions_net - predictions_modified_net != 0).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0.\n",
    "cnt_net = 0.\n",
    "cross_entr = 0.\n",
    "cross_entr_net = 0.\n",
    "cross_entr_modified = 0.\n",
    "cross_entr_modified_net = 0.\n",
    "for samples, real_labels in dataloader_m_test:\n",
    "    samples = samples.double().to(device=device).view(samples.shape[0], -1)\n",
    "    real_labels = real_labels.to(device=device)\n",
    "    \n",
    "    ### modify samples\n",
    "    modified_samples, _ = modify_sample(dm, samples, real_labels, 0.01, 20)\n",
    "    modified_samples_net, _ = modify_sample_net(net, samples, real_labels, 0.01, 20)\n",
    "        \n",
    "    ### evaluate crossentropy on real samples\n",
    "    pred = dm.predict_net(samples, inference=False)\n",
    "    pred_slice = pred[2:3]\n",
    "    log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "    cross_entr -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "        \n",
    "    pred_net = net(samples)\n",
    "    log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "    cross_entr_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "        \n",
    "    ### evaluate crossentropy on modified samples\n",
    "    pred = dm.predict_net(modified_samples, inference=False)\n",
    "    pred_slice = pred[2:3]\n",
    "    log_P = torch.log(torch.mean(torch.nn.Softmax(dim=2)(pred_slice), dim=0))\n",
    "    cross_entr_modified -= torch.sum(torch.gather(log_P, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "        \n",
    "    pred_net = net(modified_samples_net)\n",
    "    log_P_net = torch.log(torch.nn.Softmax()(pred_net))\n",
    "    cross_entr_modified_net -= torch.sum(torch.gather(log_P_net, 1, real_labels.view(-1, 1))).data[0].cpu().numpy()\n",
    "    \n",
    "    ### evaluate error rate\n",
    "    pred = torch.argmax(dm.predict_net(samples, inference=True), dim=1)\n",
    "    pred_net = torch.argmax(net(samples), dim=1)\n",
    "    \n",
    "    pred_modified = torch.argmax(dm.predict_net(modified_samples, inference=True), dim=1)\n",
    "    pred_modified_net = torch.argmax(net(modified_samples_net), dim=1)\n",
    "    \n",
    "    cnt += len(np.argwhere(predictions - predictions_modified != 0).view(-1))\n",
    "    cnt_net += len(np.argwhere(predictions_net - predictions_modified_net != 0).view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rate (Stein/Net): 0.350/0.260\tCross Entropy (Original)(Stein/Net): 0.094/0.124\tCross Entropy (Modified)(Stein/Net): 2.035/1.762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(('Error Rate (Stein/Net): {0:.3f}/{1:.3f}\\t' + \n",
    "       'Cross Entropy (Original)(Stein/Net): {2:.3f}/{3:.3f}\\t' +  \n",
    "       'Cross Entropy (Modified)(Stein/Net): {4:.3f}/{5:.3f}\\n'\n",
    "      ).format(cnt / len(dataloader_m_test.dataset), cnt_net / len(dataloader_m_test.dataset), \n",
    "               cross_entr / len(dataloader_m_test.dataset), cross_entr_net / len(dataloader_m_test.dataset),\n",
    "               cross_entr_modified / len(dataloader_m_test.dataset), cross_entr_modified_net / len(dataloader_m_test.dataset)\n",
    "              )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MNIST with FC neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_nn = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ").double()\n",
    "optim_nn = torch.optim.Adam(net_nn.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for _, (X, y) in enumerate(dataloader_m_train):\n",
    "        X = X.double().view(X.shape[0], -1)\n",
    "        \n",
    "        optim_nn.zero_grad()\n",
    "        train_loss = nn.CrossEntropyLoss()(net_nn(X), y)\n",
    "        train_loss.backward()\n",
    "        optim_nn.step()\n",
    "   \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_train)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(net_nn(X_train), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 30.\n",
    "        train_acc /= (30. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_test)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(net_nn(X_test), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 30.\n",
    "        test_acc /= (30. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            _, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X.shape[0], -1)\n",
    "y_pred = net_nn(X_test)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marginal_density = lambda x : (0.3 * normal_density(mu=-2., std=1., n=1)(x) + 0.7 * normal_density(mu=2., std=1., n=1)(x))\n",
    "#marginal_density = lambda x : (normal_density(mu=0., std=2., n=1)(x))\n",
    "\n",
    "pdf = []\n",
    "for _ in np.linspace(-10, 10, 1000): \n",
    "    _ = torch.tensor(_, dtype=t_type, device=device)\n",
    "    pdf.append(marginal_density(_))\n",
    "    \n",
    "from pynverse import inversefunc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "cum_density = interp1d(np.linspace(-20, 20, 250), [quad(marginal_density, -20, x)[0] for x in np.linspace(-20, 20, 250)])\n",
    "inv_cum_density = inversefunc(cum_density)\n",
    "real_samples = [inv_cum_density(np.random.random()) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean = quad(lambda x : marginal_density(x) * x, -20, 20)[0]\n",
    "var = quad(lambda x : marginal_density(x) * (x - mean) ** 2, -20, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10, 10, 1000), pdf)\n",
    "plt.plot(real_samples, np.zeros_like(real_samples))\n",
    "sns.kdeplot(real_samples, kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=20, n_hidden_dims=5, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot_projections(dm, use_real=True, N_plots_max=1, pdf=pdf)\n",
    "# plot_projections(dm, use_real=False, N_plots_max=1, pdf=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_projections(dm, use_real=True, kernel='tri', pdf=pdf)\n",
    "            plot_projections(dm, use_real=False, kernel='tri', pdf=pdf)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(torch.mean(dm.lt.transform(dm.particles, n_particles_second=True)),\n",
    " torch.mean(torch.std(dm.lt.transform(dm.particles, n_particles_second=True), dim=1) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conditional Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=2, n_hidden_dims=1, use_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_condition_distribution(dm, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(200): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "            \n",
    "            #plot_projections(dm, use_real=True, pdf=pdf)\n",
    "            plot_condition_distribution(dm, 100000)\n",
    "            \n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-5, 5, 0.025)\n",
    "Y = np.arange(-5, 5, 0.025)\n",
    "XX, YY = np.meshgrid(X, Y)\n",
    "XY = torch.stack([torch.tensor(XX, dtype=t_type), torch.tensor(YY, dtype=t_type)], dim=2)\n",
    "Z = torch.zeros([XY.shape[0], XY.shape[1]])\n",
    "\n",
    "Z = dm.real_target_density(XY.permute(2, 0, 1).view(2, -1)).view(Z.shape)\n",
    "Z = Z.cpu().data.numpy()\n",
    "\n",
    "# Plot the surface.\n",
    "plt.contour(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "# Plot conditioning line\n",
    "XXX, YYY = dm.lt.transform(torch.tensor(X, dtype=t_type, device=device).view(1, -1), n_particles_second=True).data.cpu().numpy()\n",
    "plt.plot(XXX, YYY, 'r', label='Linear manifold', )\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Density contour lines\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare implementations of Stein Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class SVGD():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def svgd_kernel(self, theta, h = -1):\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = h / np.log(theta.shape[0]+1)\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = 2. * dxkxy / (h)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    " \n",
    "    def update(self, x0, lnprob, n_iter = 1000, stepsize = 1e-3, bandwidth = -1, alpha = 0.9, debug = False):\n",
    "        # Check input\n",
    "        if x0 is None or lnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        theta = np.copy(x0) \n",
    "        \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter+1) % 1000 == 0:\n",
    "                print( 'iter ' + str(iter+1) )\n",
    "            \n",
    "            lnpgrad = lnprob(theta)\n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(theta, h = bandwidth)  \n",
    "            grad_theta = (np.matmul(kxy, lnpgrad) + dxkxy) / x0.shape[0]  \n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + grad_theta ** 2\n",
    "            else:\n",
    "                historical_grad = alpha * historical_grad + (1 - alpha) * (grad_theta ** 2)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            theta = theta + stepsize * adj_grad \n",
    "            \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_ex = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lnprob(particles):\n",
    "    grad_log_term = torch.zeros([particles.shape[0], particles.shape[1]], dtype=t_type, device=device)\n",
    "        \n",
    "    for idx in range(100):\n",
    "        particle = torch.tensor(particles[idx:idx+1], dtype=t_type, device=device, requires_grad=True)\n",
    "        log_term = torch.log(dm_ex.target_density(particle.t()))\n",
    "        grad_log_term[idx] = torch.autograd.grad(log_term, particle,\n",
    "                                                     only_inputs=True, retain_graph=False, \n",
    "                                                     create_graph=False, allow_unused=False)[0]\n",
    "    return grad_log_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = dm_ex.particles.data.clone().detach().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare kernel implementation\n",
    "\n",
    "ker, grad_ker = dm_ex.calc_kernel_term_latent(10.)\n",
    "print(dm_ex.h)\n",
    "grad_ker_sum = torch.sum(grad_ker, dim=1)\n",
    "\n",
    "ker_l, grad_ker_sum_l = dm_svgd.svgd_kernel(result, h=10.)\n",
    "\n",
    "print((ker - torch.tensor(ker_l, dtype=t_type, device=device)).norm())\n",
    "print((grad_ker_sum - torch.tensor(grad_ker_sum_l, dtype=t_type, device=device).t()).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare log_term implementation\n",
    "(lnprob(result) - dm_ex.calc_log_term_latent().t()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(201): \n",
    "    try:\n",
    "        result = dm_svgd.update(result, lnprob, stepsize=1e-2, bandwidth=-1, n_iter=20)\n",
    "        result = torch.tensor(result, dtype=t_type, device=device)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(result.data.cpu().numpy(), torch.zeros_like(result).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(result.data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(2000): \n",
    "    try:\n",
    "        dm_ex.update_latent(h_type=0)\n",
    "\n",
    "        if _ % 30 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm_ex.h))\n",
    "            \n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(dm_ex.particles.t().data.cpu().numpy(), torch.zeros_like(dm_ex.particles.t()).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(dm_ex.particles.t().data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(result - dm_ex.particles.t()).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"Stein-Variational-Gradient-Descent/python/\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.matlib as nm\n",
    "from svgd import SVGD\n",
    "\n",
    "r\"\"\"\n",
    "    Example of Bayesian Logistic Regression (the same setting as Gershman et al. 2012):\n",
    "    The observed data D = {X, y} consist of N binary class labels, \n",
    "    y_t \\in {-1,+1}, and d covariates for each datapoint, X_t \\in R^d.\n",
    "    The hidden variables \\theta = {w, \\alpha} consist of d regression coefficients w_k \\in R,\n",
    "    and a precision parameter \\alpha \\in R_+. We assume the following model:\n",
    "        p(\\alpha) = Gamma(\\alpha; a, b)\n",
    "        p(w_k | a) = N(w_k; 0, \\alpha^-1)\n",
    "        p(y_t = 1| x_t, w) = 1 / (1+exp(-w^T x_t))\n",
    "\"\"\"\n",
    "class BayesianLR:\n",
    "    def __init__(self, X, Y, batchsize=100, a0=1, b0=0.01):\n",
    "        self.X, self.Y = X, Y\n",
    "        # TODO. Y in \\in{+1, -1}\n",
    "        self.batchsize = min(batchsize, X.shape[0])\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.permutation = np.random.permutation(self.N)\n",
    "        self.iter = 0\n",
    "    \n",
    "        \n",
    "    def dlnprob(self, theta):\n",
    "        \n",
    "        if self.batchsize > 0:\n",
    "            batch = [ i % self.N for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize) ]\n",
    "            ridx = self.permutation[batch]\n",
    "            self.iter += 1\n",
    "        else:\n",
    "            ridx = np.random.permutation(self.X.shape[0])\n",
    "            \n",
    "        Xs = self.X[ridx, :]\n",
    "        Ys = self.Y[ridx]\n",
    "        \n",
    "        w = theta[:, :-1]  # logistic weights\n",
    "        alpha = np.exp(theta[:, -1])  # the last column is logalpha\n",
    "        d = w.shape[1]\n",
    "        \n",
    "        wt = np.multiply((alpha / 2), np.sum(w ** 2, axis=1))\n",
    "        \n",
    "        coff = np.matmul(Xs, w.T)\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-1 * coff))\n",
    "        \n",
    "        dw_data = np.matmul(((nm.repmat(np.vstack(Ys), 1, theta.shape[0]) + 1) / 2.0 - y_hat).T, Xs)  # Y \\in {-1,1}\n",
    "        dw_prior = -np.multiply(nm.repmat(np.vstack(alpha), 1, d) , w)\n",
    "        dw = dw_data * 1.0 * self.X.shape[0] / Xs.shape[0] + dw_prior  # re-scale\n",
    "        \n",
    "        dalpha = d / 2.0 - wt + (self.a0 - 1) - self.b0 * alpha + 1  # the last term is the jacobian term\n",
    "        \n",
    "        return np.hstack([dw, np.vstack(dalpha)])  # % first order derivative \n",
    "    \n",
    "    def evaluation(self, theta, X_test, y_test):\n",
    "        theta = theta[:, :-1]\n",
    "        M, n_test = theta.shape[0], len(y_test)\n",
    "\n",
    "        prob = np.zeros([n_test, M])\n",
    "        for t in range(M):\n",
    "            coff = np.multiply(y_test, np.sum(-1 * np.multiply(nm.repmat(theta[t, :], n_test, 1), X_test), axis=1))\n",
    "            prob[:, t] = np.divide(np.ones(n_test), (1 + np.exp(coff)))\n",
    "        \n",
    "        prob = np.mean(prob, axis=1)\n",
    "        acc = np.mean(prob > 0.5)\n",
    "        llh = np.mean(np.log(prob))\n",
    "        return [acc, llh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = X_test.shape[0] + X_train.shape[0]\n",
    "X_input = np.hstack([X, np.ones([N, 1])])\n",
    "y_input = y\n",
    "d = X_input.shape[1]\n",
    "D = d + 1\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.2, random_state=42)\n",
    "    \n",
    "a0, b0 = 1, 0.01 #hyper-parameters\n",
    "model = BayesianLR(X_train, y_train, 100, a0, b0) # batchsize = 100\n",
    "    \n",
    "# initialization\n",
    "M = 100  # number of particles\n",
    "theta0 = np.zeros([M, D]);\n",
    "alpha0 = np.random.gamma(a0, b0, M); \n",
    "for i in range(M):\n",
    "    theta0[i, :] = np.hstack([np.random.normal(0, np.sqrt(1 / alpha0[i]), d), np.log(alpha0[i])])\n",
    "    \n",
    "theta = SVGD().update(x0=theta0, lnprob=model.dlnprob, bandwidth=-1, n_iter=6000, stepsize=0.05, alpha=0.9, debug=True)\n",
    "    \n",
    "print('[accuracy, log-likelihood]')\n",
    "print(model.evaluation(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Metropolis–Hastings algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MHAlgorithm():\n",
    "    def __init__(self, n_dims): \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                           0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "        self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "        \n",
    "#         self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "#         self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "    \n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_dims, 1],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., -1.)\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "    def generate_next(self):\n",
    "        previous = self.particles[:, -1].unsqueeze(1)\n",
    "#         self.additional_distribution = normal_density(self.n_dims, previous, 1, n_particles_second=True)\n",
    "#         candidate = self.additional_distribution.get_sample()\n",
    "        \n",
    "#         alpha = (self.target_density(candidate) * self.additional_distribution.unnormed_density(previous) / \n",
    "#                 (self.target_density(previous) * self.additional_distribution.unnormed_density(candidate)))\n",
    "        candidate = previous + torch.zeros([self.n_dims, 1], device=device, dtype=t_type).uniform_(-1., 1.)\n",
    "        alpha = self.target_density(candidate) / self.target_density(previous)\n",
    "        \n",
    "#         print(self.target_density(candidate)[0].data.numpy(), self.additional_distribution.unnormed_density(previous)[0].data.numpy(),\n",
    "#               self.target_density(previous)[0].data.numpy(), self.additional_distribution.unnormed_density(candidate)[0].data.numpy())\n",
    "        \n",
    "        if alpha > self.one:\n",
    "            self.particles = torch.cat([self.particles, candidate], dim=1)\n",
    "        else:\n",
    "            random = torch.bernoulli(alpha)\n",
    "            if random:\n",
    "                self.particles = torch.cat([self.particles, candidate], dim=1)                \n",
    "            else:\n",
    "                self.particles = torch.cat([self.particles, previous], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mh = MHAlgorithm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    mh.generate_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(mh.particles.t().data.cpu().numpy()[:,1], \n",
    "            kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(mh.particles.t().data.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import time\n",
    "\n",
    "mu,sig,N = 0,1,1000000\n",
    "pts = []\n",
    "\n",
    "def q(x):\n",
    "#     return (1/(math.sqrt(2*math.pi*sig**2)))*(math.e**(-((x-mu)**2)/(2*sig**2)))\n",
    "#     return normal_density(1, 0., 2., n_particles_second=True).unnormed_density(x)\n",
    "    return (0.3 * normal_density(1, -2., 1., n_particles_second=True).unnormed_density(x) +\n",
    "            0.7 * normal_density(1, 2., 1., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "def metropolis(N):\n",
    "    r = np.zeros(1)\n",
    "    p = q(r[0])\n",
    "    pts = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        rn = r + np.random.uniform(-1,1)\n",
    "        pn = q(rn[0])\n",
    "        if pn >= p:\n",
    "            p = pn\n",
    "            r = rn\n",
    "        else:\n",
    "            u = np.random.rand()\n",
    "            if u < pn/p:\n",
    "                p = pn\n",
    "                r = rn\n",
    "        pts.append(r)\n",
    "    \n",
    "    pts = np.array(pts)\n",
    "    return pts\n",
    "    \n",
    "def hist_plot(array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1,)\n",
    "    ax.hist(array, bins=1000)    \n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(metropolis(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gg =  lambda : (normal_density(1, -60., 20., n_particles_second=True).get_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot([gg()[0,0] for _ in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

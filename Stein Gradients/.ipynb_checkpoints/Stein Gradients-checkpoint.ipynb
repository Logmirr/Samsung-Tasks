{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orth\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "device = None\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    use_cuda = True\n",
    "\n",
    "# Ignore cuda\n",
    "use_cuda = False\n",
    "device = None\n",
    "\n",
    "# Set DoubleTensor as a base type for computations\n",
    "t_type = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import functools\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_projections(dm=None, use_real=True, kernel='tri', pdf=None, N_plots_max=10):\n",
    "    \"\"\"\n",
    "        Plot marginal kernel density estimation\n",
    "    Args:\n",
    "        dm (DistributionMover): class containing particles which define distribution\n",
    "        use_real (bool): If set to True then apply transformation dm.lt.transform before creating plot\n",
    "        kernel (str): Kernel type for kernel density estimation\n",
    "        pdf (array_like, None): Samples from target distribution\n",
    "        N_plots_max (int): Maximum number of plots\n",
    "    \"\"\"\n",
    "    N_plots = None\n",
    "    scale_factor = None\n",
    "    \n",
    "    if use_real:\n",
    "        if not dm.use_latent:\n",
    "            return\n",
    "        N_plots = dm.lt.A.shape[0]\n",
    "    else:\n",
    "        N_plots = dm.particles.shape[0]\n",
    "    if N_plots > 6:\n",
    "        scale_factor = 15\n",
    "    else:\n",
    "        scale_factor = 5\n",
    "        \n",
    "    N_plots = min(N_plots, N_plots_max)\n",
    "        \n",
    "    plt.figure(figsize=(3 * scale_factor, (N_plots // 3 + 1) * scale_factor))\n",
    "    \n",
    "    for idx in range(N_plots):\n",
    "        slice_dim = idx\n",
    "        \n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        \n",
    "        particles = None\n",
    "        if use_real:\n",
    "            particles = dm.lt.transform(dm.particles, n_particles_second=True).t()[:, slice_dim]\n",
    "        else:\n",
    "            particles = dm.particles.t()[:, slice_dim]\n",
    "        \n",
    "        if pdf is not None:\n",
    "            plt.plot(np.linspace(-10, 10, len(pdf), dtype=np.float64), pdf)\n",
    "        plt.plot(particles.data.cpu().numpy(), torch.zeros_like(particles).data.cpu().numpy(), 'ro')\n",
    "        sns.kdeplot(particles.data.cpu().numpy(), \n",
    "                    kernel=kernel, color='darkblue', linewidth=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_condition_distribution(dm, n_samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dm (DistributionMover): object contains unconditioned density, linear manifold and particles\n",
    "        n_samples (int): number of samples\n",
    "    Return:\n",
    "        (points, weight)\n",
    "    \"\"\"\n",
    "    if not dm.use_latent:\n",
    "        return\n",
    "    \n",
    "    points = torch.zeros([dm.n_hidden_dims, n_samples], dtype=t_type, device=device).uniform_(-10, 10)\n",
    "    weight = dm.real_target_density(dm.lt.transform(points, n_particles_second=True))\n",
    "    points = points.view(-1)\n",
    "    \n",
    "    plt.hist(points.data.cpu().numpy(), weights=weight.data.cpu().numpy(), density=True, bins=100, alpha=0.5, label='True conditional density')\n",
    "    plt.plot(dm.particles.data.cpu().numpy(), torch.zeros_like(dm.particles).data.cpu().numpy(), 'ro')\n",
    "    sns.kdeplot(dm.particles[0, :].data.cpu().numpy(), \n",
    "                kernel='tri', color='darkblue', linewidth=4, label='Approximated conditional density')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_diffs(x, y, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: x is a dxN matrix\n",
    "                   y is an optional dxM matrix\n",
    "            Output: diffs is a dxNxM matrix where diffs[i,j] is the subtraction between x[:,i] and y[:,j]\n",
    "            i.e. diffs[i,j] = x[:,i] - y[:,j]\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: x is a Nxd matrix\n",
    "                   y is an optional Mxd matrix\n",
    "            Output: diffs is a NxMxd matrix where diffs[i,j] is the subtraction between x[i,:] and y[j,:]\n",
    "            i.e. diffs[i,j] = x[i,:]-y[j,:]\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return x[:,:,np.newaxis] - y[:,np.newaxis,:]        \n",
    "    return x[:,np.newaxis,:] - y[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_dists(diffs=None, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: diffs is a dxNxM matrix where diffs[i,j] = x[:,i] - y[:,j]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[:,i] - y[:,j]||\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: diffs is a NxMxd matrix where diffs[i,j] = x[i,:] - y[j, :]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[i,:]-y[j,:]||\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return torch.norm(diffs, dim=0)\n",
    "    return torch.norm(diffs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [],
   "source": [
    "class normal_density():\n",
    "    \"\"\"\n",
    "        Multinomial normal density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, mu=0., std=1., n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            mu (float, array_like): mean of distribution\n",
    "                Default: 0.\n",
    "                if mu is float - use same mean across all dimensions\n",
    "                if mu is 1D array_like - use different mean for each dimension but same for each particles dimension\n",
    "                if mu is 2D array_like - use different mean for each dimension\n",
    "            std (float, array_like): std of distribution\n",
    "                Default: 1.\n",
    "                if std is float - use same std across all dimensions\n",
    "                if std is 1D array_like - use different std for each dimension but same for each particles dimension\n",
    "                if std is 2D array_like - use different std for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis\n",
    "                Input will be reduced along all axis exclude particle axis\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        \n",
    "        if type(self.mu) == float:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.mu.shape) == 1:\n",
    "            if self.mu.shape[0] == 1:\n",
    "                self.mu = self.mu.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.mu.shape) == 2:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if type(self.std) == float:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.std.shape) == 1:\n",
    "            if len(self.std) == 1:\n",
    "                self.std = self.std.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.zero = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.pi = torch.tensor(math.pi, dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "    def __call__(self, x, n_axis=None):        \n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.pow(self.two * self.pi, -self.n / self.two) / \n",
    "                torch.prod(self.std, dim=n_axis) * \n",
    "                torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):      \n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std , self.two), dim=n_axis))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):  \n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (-self.n / self.two * torch.log(self.two * self.pi) + \n",
    "                torch.sum(torch.log(self.std), dim=n_axis) -\n",
    "                self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return -self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)\n",
    "    \n",
    "    def get_sample(self):\n",
    "        \"\"\"\n",
    "            Sample from normal distribution\n",
    "        \"\"\"\n",
    "        sample = torch.normal(self.mu, self.std)\n",
    "        if self.n_particles_second:\n",
    "            return sample.view(-1, 1)\n",
    "        else:\n",
    "            return sample.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class gamma_density():\n",
    "    \"\"\"\n",
    "        Multinomial gamma density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, alpha=1, betta=1, n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            alpha (float, array_like): shape of distribution\n",
    "                Default: 1.\n",
    "                if alpha is float - use same shape across all dimensions\n",
    "                if alpha is 1D array_like - use different shape for each dimension but same for each particles dimension\n",
    "                if alpha is 2D array_like - use different shape for each dimension\n",
    "            betta (float, array_like): rate of distribution\n",
    "                Default: 1.\n",
    "                if betta is float - use same rate across all dimensions\n",
    "                if betta is 1D array_like - use different rate for each dimension but same for each particles dimension\n",
    "                if betta is 2D array_like - use different rate for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis  \n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.betta = betta\n",
    "        \n",
    "        if type(self.alpha) == float:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.alpha.shape) == 1:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.alpha.shape) == 2:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if type(self.betta) == float:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.betta.shape) == 1:\n",
    "            if len(self.betta) == 1:\n",
    "                self.betta = self.betta.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "        ### log Г(alpha)\n",
    "        self.lgamma = torch.lgamma(self.alpha)\n",
    "        ### Г(alpha)\n",
    "        self.gamma = torch.exp(self.lgamma)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(self.betta, self.alpha) / self.gamma * torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class SteinLinear(nn.Module):\n",
    "    \"\"\"\n",
    "        Custom full connected layer for Stein Gradient Neural Networks\n",
    "        Transformation: y = xA + b\n",
    "        Parameters prior: \n",
    "            1 - p(w, a) = p(w|a)p(a) = П p(w_i|a_i)p(a_i)\n",
    "                p(w_i|a_i) = N(w_i|0, a_i^(-1)); p(a_i) = G(1e-4, 1e-4)\n",
    "                          \n",
    "            2 - p(w) = П p(w_i)\n",
    "                p(w_i) = N(w_i|0, alpha^(-1))\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, n_particles=1, use_bias=True, use_var_prior=True, alpha=1e-2):\n",
    "        super(SteinLinear, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features (int): size of each input sample\n",
    "            out_features (int): size of each output sample\n",
    "            n_particles (int): number of particles\n",
    "            use_bias (bool): If set to False, the layer will not learn an additive bias.\n",
    "                Default: True\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of weight variance\n",
    "                Default: True\n",
    "            alpha (float): If use_var_prior == False - defines weight variance\n",
    "                Default: 1e-2\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_particles = n_particles\n",
    "        self.use_bias = use_bias\n",
    "        self.use_var_prior = use_var_prior\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.zeros([in_features, out_features, n_particles], dtype=t_type, device=device))\n",
    "        if self.use_var_prior:\n",
    "            self.log_weight_alpha = torch.nn.Parameter(torch.zeros([in_features * out_features, n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_weight_alpha = torch.tensor([math.log(self.alpha)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        self.bias = None\n",
    "        self.log_bias_alpha = None\n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros([1, out_features, n_particles], dtype=t_type, device=device))\n",
    "            if self.use_var_prior:\n",
    "                self.log_bias_alpha = torch.nn.Parameter(torch.zeros([out_features, n_particles], dtype=t_type, device=device))\n",
    "            else:\n",
    "                self.log_bias_alpha = torch.tensor([math.log(self.alpha)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        self.weight_alpha_log_prior = None\n",
    "        self.bias_alpha_log_prior = None\n",
    "        if self.use_var_prior:\n",
    "            ### define prior on alpha p(a) = G(1e-4, 1e-4)\n",
    "            self.weight_alpha_log_prior = lambda x: (gamma_density(n=self.log_weight_alpha.shape[0],\n",
    "                                                                   alpha=1e-4,\n",
    "                                                                   betta=1e-4,\n",
    "                                                                   n_particles_second=True\n",
    "                                                                  ).log_unnormed_density_log_x(x))\n",
    "            if self.use_bias:\n",
    "                self.bias_alpha_log_prior = lambda x: (gamma_density(n=self.log_bias_alpha.shape[0],\n",
    "                                                                     alpha=1e-4,\n",
    "                                                                     betta=1e-4,\n",
    "                                                                     n_particles_second=True\n",
    "                                                                    ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    ### useless function - all initialization defined in DistributionMover class\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.log_weight_alpha.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.log_bias_alpha.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Apply transformation: X_out[i, :, :] = X_in[i, :, :] * W + b[i]\n",
    "        Args:\n",
    "            X (torch.tensor): tensor \n",
    "        Shape:\n",
    "            Input: [n_particles, batch_size, in_features]\n",
    "            Output: [n_particles, batch_size, out_features]\n",
    "        \"\"\"\n",
    "        ### NEED SOME OPTIMIZATION TO OMMIT .permute\n",
    "        if self.use_bias:\n",
    "            return torch.bmm(X, self.weight.permute(2, 0, 1)) + self.bias.permute(2, 0, 1)\n",
    "        return torch.bmm(X, self.weight.permute(2, 0, 1))\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())\n",
    "        \n",
    "    def calc_log_prior(self):\n",
    "        \"\"\"\n",
    "            Evaluate log prior of trainable parameters\n",
    "        log p(w,a) = log p(w|a) + log p(a\n",
    "        \"\"\"\n",
    "        ### define prior on weight p(w|a) = N(0, 1 / alpha)\n",
    "        self.weight_log_prior = lambda x: (normal_density(n=self.weight.numel() // self.n_particles,\n",
    "                                                          mu=0.,\n",
    "                                                          std=self.one / torch.exp(self.log_weight_alpha),\n",
    "                                                          n_particles_second=True\n",
    "                                                         ).log_unnormed_density(x))\n",
    "\n",
    "        self.bias_log_prior = lambda x: (normal_density(self.bias.numel() // self.n_particles,\n",
    "                                                        mu=0.,\n",
    "                                                        std=self.one / torch.exp(self.log_bias_alpha),\n",
    "                                                        n_particles_second=True\n",
    "                                                       ).log_unnormed_density(x))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            if self.use_var_prior:\n",
    "                return (self.weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha) +\n",
    "                        self.bias_log_prior(self.bias.view(-1, self.n_particles)) + self.bias_alpha_log_prior(self.log_bias_alpha))   \n",
    "            return (self.weight_log_prior(self.weight.view(-1, self.n_particles)) +\n",
    "                    self.bias_log_prior(self.bias.view(-1, self.n_particles)))        \n",
    "        if self.use_var_prior:\n",
    "            return self.weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha)\n",
    "        return self.weight_log_prior(self.weight.view(-1, self.n_particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LinearTransform():\n",
    "    \"\"\"\n",
    "        Class for various linear transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, n_dims, n_hidden_dims, use_identity=False, normalize=False, A=None, tetta_0=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_dims (int): dimension of the space\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_identity (bool): If set to True, use 'eye' matrix for transformations\n",
    "            normalize (bool): If set to True, columns of the transformation matrix is an orthonormal basis \n",
    "            A (2D array_like, None): Initial value for transformation matrix\n",
    "                If None then matrix will be sampled from uniform distribution and then orthonormate\n",
    "                Default: None\n",
    "            tetta_0 (1D array_like, None): Initial value for bias\n",
    "                If None then matrix will be sampled from uniform distribution\n",
    "                Default: None\n",
    "        \"\"\"\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_identity = use_identity\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if self.use_identity:\n",
    "            return\n",
    "        \n",
    "        self.A = A\n",
    "        self.tetta_0 = tetta_0\n",
    "        \n",
    "        if self.A is None:\n",
    "            self.A = torch.zeros([self.n_dims, self.n_hidden_dims], dtype=t_type, device=device)\n",
    "            self.A.uniform_(-1., 1.)\n",
    "            if self.normalize:\n",
    "                ### normalize columns of matrix A\n",
    "                self.A = torch.tensor(orth(self.A.data.cpu().numpy()), dtype=t_type, device=device)\n",
    "                    \n",
    "        if self.tetta_0 is None:\n",
    "            self.tetta_0 = torch.zeros([self.n_dims, 1], dtype=t_type, device=device)\n",
    "            self.tetta_0.uniform_(-1.,1.)\n",
    "        \n",
    "        ### A^(t)A\n",
    "        self.AtA = torch.matmul(self.A.t(), self.A)\n",
    "        ### (A^(t)A)^(-1)\n",
    "        self.AtA_1 = torch.inverse(self.AtA)\n",
    "        ### (A^(t)A)^(-1)A^(t)\n",
    "        self.inverse_base = torch.matmul(self.AtA_1, self.A.t())\n",
    "        # ### A(A^(t)A)^(-1)A^(t)\n",
    "        # self.projector_base = torch.matmul(self.A, self.inverse_base)\n",
    "        \n",
    "    def transform(self, tetta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Transform tettas as follows: \n",
    "                tetta = Atetta` + tetta_0\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.A, tetta) + self.tetta_0\n",
    "        return (torch.matmul(self.A, tetta.t()) + self.tetta_0).t()\n",
    "    \n",
    "    def inverse_transform(self, tetta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Apply inverse transformation: \n",
    "                tetta` = (A^(t)A)^(-1)A^(t)(tetta - tetta_0)\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.inverse_base, tetta - self.tetta_0)\n",
    "        return torch.matmul(self.inverse_base, tetta.t() - self.tetta_0).t()\n",
    "        \n",
    "    def project(self, tetta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Project tettas onto Linear Space X = {Atetta` + tetta_0 for all tetta` in R^d}:\n",
    "                tetta_projected = A(A^(t)A)^(-1)A^(t)(tetta - tetta_0) + tetta_0\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.projector_base, tetta - self.tetta_0) + self.tetta_0\n",
    "        return (torch.matmul(self.projector_base, tetta.t() - self.tetta_0) + self.tetta_0).t()\n",
    "    \n",
    "    def project_inverse(self, tetta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Project and then apply inverse transform to tetta - tetta_0:\n",
    "                tetta_s_p_i = T^(-1)P(tetta - tetta_0)= (A^(t)A)^(-1)A^(t)tetta\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        ### use solver trick: tetta_s_p_i : A^(t)Atetta_s_p_i = A^(t)tetta\n",
    "        if n_particles_second:\n",
    "            return torch.gesv(torch.matmul(self.A.t(), tetta), self.AtA)[0]\n",
    "        return torch.gesv(torch.matmul(self.A.t(), tetta.t()), self.AtA)[0].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class RegressionDistribution(nn.Module):\n",
    "    \"\"\"\n",
    "        Distribution over data for regression task: p(D|w) = N(y_predicted|y, )\n",
    "    \"\"\"\n",
    "    def __init__(self, n_particles, use_var_prior=True, betta=10-1):\n",
    "        super(RegressionDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of prediction variance\n",
    "                Default: True\n",
    "            betta (float): If use_var_prior == False - defines variance of prediction\n",
    "                Default: 1e-1\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.use_var_prior = use_var_prior\n",
    "        self.betta = betta\n",
    "        \n",
    "        ### define betta - variance of data distribution for regression tasks (betta = 1 / std ** 2)\n",
    "        self.log_betta = None\n",
    "        if self.use_var_prior:\n",
    "            self.log_betta = torch.nn.Parameter(torch.zeros([1, self.n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_betta = torch.tensor([math.log(self.betta)], dtype=t_type, device=device, requires_grad=False)\n",
    "\n",
    "        self.betta_log_prior = None\n",
    "        if self.use_var_prior:\n",
    "            ### define prior on betta p(betta)\n",
    "            self.betta_log_prior = lambda x: (gamma_density(n=1,\n",
    "                                                            alpha=1e-4,\n",
    "                                                            betta=1e-4,\n",
    "                                                            n_particles_second=True\n",
    "                                                           ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        ### Support tensors for computations\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "    \n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(tetta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predicted values\n",
    "            train_size (int) - size of training dataset\n",
    "        Shapes:\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, 1]\n",
    "        \"\"\"\n",
    "        ### squeeze last axis because regression task is being solved\n",
    "        y_predict.squeeze_(2)\n",
    "        \n",
    "        batch_size = torch.tensor(y.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        self.log_data_distr = None\n",
    "        if self.use_var_prior:\n",
    "            self.log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                            mu=y,\n",
    "                                                            std=self.one / torch.sqrt(torch.exp(self.log_betta.expand(X.shape[0], self.n_particles).t())),\n",
    "                                                            n_particles_second=False\n",
    "                                                           ).log_unnormed_density(x))\n",
    "        else:\n",
    "            self.log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                            mu=y,\n",
    "                                                            std=self.one / torch.sqrt(torch.exp(self.log_betta)),\n",
    "                                                            n_particles_second=False\n",
    "                                                           ).log_unnormed_density(x))\n",
    "            \n",
    "        if self.use_var_prior:\n",
    "            return train_size / batch_size * self.log_data_distr(y_predict) + self.betta_log_prior(self.log_betta)\n",
    "        return train_size / batch_size * self.log_data_distr(y_predict)\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ClassificationDistribution(nn.Module):\n",
    "    def __init__(self, n_particles):\n",
    "        super(ClassificationDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        \"\"\"\n",
    "        self.n_particles = n_particles\n",
    "    \n",
    "\n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(tetta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shapes: \n",
    "            X.shape = [batch_size, in_features]\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        batch_size = torch.tensor(X.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        \n",
    "        probas = nn.LogSoftmax(dim=2)(y_predict)\n",
    "        probas_selected = torch.gather(input=probas, dim=2, index=y.view(1, -1, 1).expand(probas.shape[0], probas.shape[1], 1)).squeeze(2)\n",
    "        log_data = torch.sum(probas_selected, dim=1)\n",
    "        \n",
    "        return train_size / batch_size  * log_data\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "code_folding": [
     1,
     129,
     226,
     257,
     277,
     320,
     326,
     350,
     389,
     432,
     443,
     450
    ]
   },
   "outputs": [],
   "source": [
    "class DistributionMover():\n",
    "    def __init__(self,\n",
    "                 task='app', \n",
    "                 n_particles=None,\n",
    "                 particles=None,\n",
    "                 target_density=None,\n",
    "                 n_dims=None,\n",
    "                 n_hidden_dims=None,\n",
    "                 use_latent=False,\n",
    "                 net=None,\n",
    "                 precomputed_params=None,\n",
    "                 data_distribution=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task (str):\n",
    "                'app' | 'net_reg' | 'net_class'\n",
    "                - approximate target distribution\n",
    "                - solve regression task using net\n",
    "                - solve classification task using net\n",
    "            n_particles (int): number of particles\n",
    "            particles (2D array_like): array which contains initialized particles\n",
    "            target_density (callable): computes probability density function of target distribution (only for 'app' task)\n",
    "            n_dims (int): dimension of the space where optimization is performed\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_latent (bool): If set to True, Subspace Stein is used\n",
    "            net (nn.Sequential): object which is used to make predictions (for 'net_reg' and' net_class' tasks)\n",
    "            precomputed_params (1D array_like): Precomputed parameters, which will be used for particles initialization\n",
    "            data_distribution (callable): computes probability over data p(D|w) (for 'net_reg' and' net_class' tasks)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.task = task\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.particles = particles\n",
    "        self.target_density = target_density\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_latent = use_latent\n",
    "        self.net = net\n",
    "        self.precomputed_params = precomputed_params\n",
    "        self.data_distribution = data_distribution\n",
    "        \n",
    "\n",
    "        if self.task == 'net_reg' or self.task == 'net_class':\n",
    "            self.n_dims = 0\n",
    "            for module in self.modules_net():\n",
    "                if \"numel\" in dir(module):\n",
    "                    self.n_dims += module.numel() // self.n_particles\n",
    "\n",
    "        if not self.use_latent:\n",
    "            self.n_hidden_dims = self.n_dims\n",
    "\n",
    "        ### Learnable samples from the target distribution\n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_hidden_dims, self.n_particles],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., 2.)\n",
    "\n",
    "        ### Class for performing linear transformations\n",
    "        self.lt = None\n",
    "        if self.use_latent:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=False, \n",
    "                normalize=True\n",
    "            )\n",
    "        else:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=True, \n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "        if self.precomputed_params is not None:\n",
    "            self.particles = self.lt.inverse_transform(self.precomputed_params.unsqueeze(1).expand(self.n_dims, self.n_particles))\n",
    "\n",
    "        ### Functions of probability density of target distribution\n",
    "        self.target_density = None\n",
    "        self.real_target_density = None\n",
    "        if self.net is None:\n",
    "            # use unnormed probability density to speedup computations\n",
    "            if target_density is not None:\n",
    "                self.target_density = target_density\n",
    "                self.real_target_density = target_density\n",
    "            else:\n",
    "                self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                                   0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "                self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                        0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "\n",
    "                # self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "                # self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "        ### Number of iterations since beginning\n",
    "        self.iter = 0\n",
    "\n",
    "        ### Adagrad parameters\n",
    "        self.fudge_factor = torch.tensor(1e-6, dtype=t_type, device=device)\n",
    "        self.step_size = torch.tensor(1e-2, dtype=t_type, device=device)\n",
    "        self.auto_corr = torch.tensor(0.9, dtype=t_type, device=device)\n",
    "\n",
    "        ### Gradient history term for adagrad optimization\n",
    "        self.historical_grad = None\n",
    "        self.historical_grad_tetta_0 = None\n",
    "        if self.use_latent:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_hidden_dims, n_particles], dtype=t_type, device=device)\n",
    "            self.historical_grad_tetta_0 = torch.zeros(\n",
    "                [self.n_dims, 1], dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_dims, n_particles], dtype=t_type, device=device)\n",
    "            \n",
    "\n",
    "        ### Factor from kernel\n",
    "        self.med = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.h = torch.tensor(0., dtype=t_type, device=device)\n",
    "\n",
    "        ### Support tensors for computations\n",
    "        self.N = torch.tensor(self.n_particles, dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.three = torch.tensor(3., dtype=t_type, device=device)\n",
    "\n",
    "    def calc_kernel_term_latent(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(tetta-tetta`)^2) / logN\n",
    "                    1 - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    3 - var(tetta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(tetta-tetta`) / logN * n_dims\n",
    "                    5 - med(dist(tetta-tetta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(tetta-tetta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atetta`_i + tetta_0, Atetta`_j + tetta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        ### power for rational kernel\n",
    "        self.p = torch.tensor(p, dtype=t_type, device=device) if p is not None else None\n",
    "        \n",
    "        ### tetta = Atetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### diffs[i, j] = A(tetta`_i - tetta`_j)\n",
    "        diffs = pairwise_diffs(real_particles, real_particles, n_particles_second=True)\n",
    "        ### dists[i, j] = ||A(tetta`_i - tetta`_j)||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        ### sq_dists[i, j] = ||A(tetta`_i - tetta`_j)||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "        \n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 2:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            self.var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            self.var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 5:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  self.med / (torch.pow(self.N, self.two) - self.one)\n",
    "        elif h_type == 6:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  self.med / (torch.pow(self.N, -self.one / self.p) - self.one)\n",
    "        elif h_type == 7:\n",
    "            self.med = torch.median(torch.matmul(real_particles.t(), real_particles)) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        \n",
    "        kernel = None\n",
    "        grad_kernel = None\n",
    "        if kernel_type == 'rbf':\n",
    "            ### RBF Kernel:\n",
    "            ### kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "            kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "            ### grad_kernel[i, j] = -2/h * A(tetta`_i - tetta`_j) * kernel[i, j]\n",
    "            grad_kernel = -self.two / self.h * kernel.unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'imq':\n",
    "            ### IMQ Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, -self.one / self.two)\n",
    "            ### grad_kernel[i, j] = -1/h * A(tetta`_i - tetta`_j) * kernel^(3)[i, j]\n",
    "            grad_kernel = -self.one / self.h * torch.pow(kernel, self.three).unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'exp':\n",
    "            ### Exponential Kernel:\n",
    "            ### kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "            kernel = torch.exp(self.one / self.h * torch.matmul(real_particles.t(), real_particles))\n",
    "            ### grad_kernel[i, j] = 1/h * (Atetta`_j + tetta_0) * kernel[i, j]\n",
    "            grad_kernel = 1. / self.h * kernel.unsqueeze(0) * real_particles.unsqueeze(1)\n",
    "        elif kernel_type == 'rat':\n",
    "            ### RAT Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, self.p)\n",
    "            ### grad_kernel[i, j] = p/h * A(tetta`_i - tetta`_j) * kernel^((p - 1)/p)[i, j]\n",
    "            grad_kernel = self.p / self.h * torch.pow(kernel, (self.p - self.one) / self.p).unsqueeze(0) * diffs\n",
    "            \n",
    "        return kernel, grad_kernel\n",
    "    \n",
    "    def calc_kernel_term_latent_net(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(tetta-tetta`)^2) / logN\n",
    "                    1 - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    3 - var(tetta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(tetta-tetta`) / logN * n_dims\n",
    "                    5 - med(dist(tetta-tetta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(tetta-tetta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atetta`_i + tetta_0, Atetta`_j + tetta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        return self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "\n",
    "    def calc_log_term_latent(self):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(tetta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        \n",
    "        ### tetta = A tetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_term = torch.log(self.target_density(real_particles))\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "    \n",
    "        grad_log_term = real_particles.grad\n",
    "        \n",
    "        return grad_log_term\n",
    "    \n",
    "    def calc_log_term_latent_net(self, X, y, train_size):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(tetta)) \n",
    "        Args:\n",
    "            X (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input: \n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output: \n",
    "                [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        self.log_data = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        self.log_prior = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        \n",
    "        ### get real net parameters: tetta_i = A tetta`_i + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### init net with real parameters\n",
    "        self.vector_to_parameters(real_particles.view(-1), self.parameters_net())\n",
    "        ### compute log prior of all weight in the net\n",
    "        for module in self.modules_net():\n",
    "            if \"calc_log_prior\" in dir(module):\n",
    "                self.log_prior += module.calc_log_prior()\n",
    "        \n",
    "        ### get prediction for the batch of data\n",
    "        y_predict = self.predict_net(X)\n",
    "        ### compute log data term log p(D|w)\n",
    "        self.log_data = self.data_distribution.calc_log_data(X, y, y_predict, train_size)\n",
    "        \n",
    "        ### log_term = log p(tetta) = log p_prior(tetta) + log p_data(D|tetta)\n",
    "        log_term = self.log_prior + self.log_data\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "        \n",
    "        ### collect all gradients into one vector\n",
    "        grad_log_term = self.parameters_grad_to_vector(self.parameters_net()).view(-1, self.n_particles)\n",
    "            \n",
    "        return grad_log_term\n",
    "    \n",
    "    def parameters_net(self):\n",
    "        \"\"\"\n",
    "            Return all trainable parameters\n",
    "        \"\"\"\n",
    "        return chain(self.net.parameters(), self.data_distribution.parameters())\n",
    "    \n",
    "    def modules_net(self):\n",
    "        \"\"\"\n",
    "            Return all modules\n",
    "        \"\"\"\n",
    "        return chain(self.net.modules(), self.data_distribution.modules())\n",
    "\n",
    "    def predict_net(self, X, inference=False):\n",
    "        \"\"\"\n",
    "            Use net to make predictions        \n",
    "            Args:\n",
    "                X (array_like): batch of data\n",
    "        \"\"\"\n",
    "        predictions = self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape))\n",
    "        if self.task == 'net_reg':\n",
    "            if inference:\n",
    "                return torch.mean(predictions, dim=0)\n",
    "            else:\n",
    "                return predictions\n",
    "        elif self.task == 'net_class':\n",
    "            if inference:\n",
    "                return torch.mean(torch.nn.LogSoftmax(dim=2)(predictions), dim=0)\n",
    "            else:\n",
    "                return predictions\n",
    "\n",
    "    def update_latent(self, h_type, kernel_type='rbf', p=None, step_size=None, move_tetta_0=False):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.move_tetta_0 = move_tetta_0\n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent()\n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "        \n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "        \n",
    "        ### Update tetta_0 in LinearTransform\n",
    "        if self.use_latent and self.move_tetta_0:\n",
    "            ### Compute value of step in functional space\n",
    "            tetta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "            \n",
    "            ### Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_tetta_0 = self.historical_grad_tetta_0 + tetta_0_update * tetta_0_update\n",
    "            else:\n",
    "                self.historical_grad_tetta_0 = self.auto_corr * self.historical_grad_tetta_0 + (self.one - self.auto_corr) * tetta_0_update * tetta_0_update\n",
    "\n",
    "            ### Adjust gradient and make step\n",
    "            adj_tetta_0_update = tetta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_tetta_0))\n",
    "            self.lt.tetta_0 = self.lt.tetta_0 + self.step_size * adj_tetta_0_update\n",
    "               \n",
    "    def update_latent_net(self, h_type, kernel_type='rbf', p=None, X_batch=None, y_batch=None, train_size=None, step_size=None, move_tetta_0=False):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.move_tetta_0 = move_tetta_0\n",
    "        self.iter += 1\n",
    "        self.net.zero_grad()\n",
    "        self.data_distribution.zero_grad()\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent_net(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_net(X_batch, y_batch, train_size)\n",
    "        \n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "                \n",
    "        ### Update tetta_0 in LinearTransform\n",
    "        if self.use_latent and self.move_tetta_0:\n",
    "            ### Compute value of step in functional space\n",
    "            tetta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "            \n",
    "            ### Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_tetta_0 = self.historical_grad_tetta_0 + tetta_0_update * tetta_0_update\n",
    "            else:\n",
    "                self.historical_grad_tetta_0 = self.auto_corr * self.historical_grad_tetta_0 + (self.one - self.auto_corr) * tetta_0_update * tetta_0_update\n",
    "\n",
    "            ### Adjust gradient and make step\n",
    "            adj_tetta_0_update = tetta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_tetta_0))\n",
    "            self.lt.tetta_0 = self.lt.tetta_0 + self.step_size * adj_tetta_0_update\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_to_parameters(vec, parameters):\n",
    "        pointer = 0\n",
    "        for param in parameters:\n",
    "            # The length of the parameter\n",
    "            num_param = param.numel()\n",
    "            # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "            param.data = vec[pointer:pointer + num_param].view_as(param).data\n",
    "            # Increment the pointer\n",
    "            pointer += num_param\n",
    "            \n",
    "    @staticmethod \n",
    "    def parameters_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    @staticmethod \n",
    "    def parameters_grad_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.grad.view(-1))\n",
    "        return torch.cat(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.7471, dtype=torch.float64) tensor(22.9835, dtype=torch.float64)\n",
      "tensor(77.4397, dtype=torch.float64) tensor(88.1444, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "### Bostor housing dataset for regression task\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = (torch.tensor(X_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(X_test, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_test, dtype=t_type, device=device))\n",
    "\n",
    "### Linear Regression baseline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.tensor(model.predict(X_test), dtype=t_type, device=device), y_test), \n",
    "      torch.nn.MSELoss()(torch.tensor(model.predict(X_train), dtype=t_type, device=device), y_train))\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.mean(y_train).expand(y_test.shape[0]), y_test), \n",
    "      torch.nn.MSELoss()(torch.mean(y_train).expand(y_train.shape[0]), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.12806   0.0378    0.05861   3.24007 -16.22227   3.89352  -0.01279\n",
      "  -1.42327   0.23451  -0.0082   -0.92995   0.01192  -0.54849]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Check all functions\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=5, n_particles=10, use_latent=True)\n",
    "dm.calc_log_term_latent()\n",
    "dm.calc_kernel_term_latent(0)\n",
    "dm.update_latent(0)\n",
    "\n",
    "ss = nn.Sequential(SteinLinear(13, 1, 10))\n",
    "dd = RegressionDistribution(n_particles=10)\n",
    "dm = DistributionMover(task='net_reg', n_hidden_dims=5, n_particles=10, use_latent=True, net=ss, data_distribution=dd)\n",
    "dm.calc_log_term_latent_net(X_train, y_train, X_train.shape[0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(13, 1, 100, use_var_prior=False, alpha=1e-2, use_bias=True))\n",
    "data_distr = RegressionDistribution(100, use_var_prior=False, betta=1e-1)\n",
    "dm = DistributionMover(task='net_reg', n_particles=100, use_latent=False, net=net, data_distribution=data_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alpha = dm.net[0].alpha\n",
    "betta = dm.data_distribution.betta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Append column of ones to data \n",
    "# XX = X_train\n",
    "# XX_test = X_test\n",
    "XX = torch.cat([X_train, torch.ones([X_train.shape[0], 1], dtype=t_type, device=device)], dim=1)\n",
    "XX_test = torch.cat([X_test, torch.ones([X_test.shape[0], 1], dtype=t_type, device=device)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sigma = torch.inverse(betta * XX.t() @ XX + alpha * torch.eye(XX.shape[1], dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mu = betta * sigma @ XX.t() @ y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu @ XX.t(), y_train), torch.nn.MSELoss()(mu @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_test = torch.load('particles_12400.txt').cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_mean = torch.mean(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(particles_mean @ XX.t(), y_train), torch.nn.MSELoss()(particles_mean @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.var(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.diag(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import *\n",
    "\n",
    "mod = WLS(y_train.data.cpu().numpy(), XX.data.cpu().numpy())\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stein_mu = (torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu, torch.tensor(results.params, dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu, particles_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(sum(np.logical_and(results.conf_int()[:,0] < stein_mu.data.cpu().numpy(), stein_mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "      sum(np.logical_and(results.conf_int()[:,0] < mu.data.cpu().numpy(), mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "      sum(np.logical_and(results.conf_int()[:,0] < particles_mean.data.cpu().numpy(), particles_mean.data.cpu().numpy() < results.conf_int()[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(mu, torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    step_size = 0.00075\n",
    "    dm.historical_grad.zero_()\n",
    "    for _ in range(100000):\n",
    "        dm.update_latent_net(h_type=1, kernel_type='rbf', p=-1, X_batch=X_train, y_batch=y_train, train_size=X_train.shape[0], step_size=step_size)\n",
    "        train_loss = torch.nn.MSELoss()(dm.predict_net(X_train, inference=True).view(-1), y_train)\n",
    "        test_loss = torch.nn.MSELoss()(dm.predict_net(X_test, inference=True).view(-1), y_test)\n",
    "        \n",
    "        if _ % 10 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train): {1:.3f}\\t Empirical Loss(Test): {2:.3f}\\t Kernel factor: {3:.3f}'.format(\n",
    "                            _, train_loss, test_loss, dm.h))\n",
    "            \n",
    "            plot_projections(dm, use_real=True)\n",
    "            plot_projections(dm, use_real=False)\n",
    "            plt.pause(1e-300)\n",
    "            \n",
    "        if _ % 3000 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset with class selection\n",
    "\n",
    "sys.path.insert(0, '/home/m.nakhodnov/Samsung-Tasks/Datasets/MyMNIST')\n",
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform)\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform)\n",
    "\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=32, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAP estimate for MNIST classification task\n",
    "net_map = nn.Sequential(SteinLinear(28 * 28, 200, 1), nn.ReLU(), SteinLinear(200, 200, 1), nn.ReLU(), SteinLinear(200, 10, 1)).to(device=device)\n",
    "data_distr_map = ClassificationDistribution(1)\n",
    "dm_map = DistributionMover(task='net_class', n_particles=1, use_latent=False, net=net_map, data_distribution=data_distr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17... Empirical Loss(Train/Test): 162.325/217.285\t Accuracy(Train/Test): 0.904/0.869\t Kernel factor: 0.0000"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step_size = 0.003\n",
    "    for epoch in range(30):\n",
    "        for train_epoch in range(30):\n",
    "            X, y = next(iter(dataloader_m_train))\n",
    "            X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "            y = y.to(device=device)\n",
    "            dm_map.update_latent_net(h_type=0, X_batch=X, y_batch=y, train_size=dataloader_m_train.__len__(), step_size=step_size)\n",
    "        \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().to(device=device).view(X.shape[0], -1)\n",
    "            y_train = y_train.to(device=device)\n",
    "            \n",
    "            net_pred = dm_map.predict_net(X_train, inference=True)\n",
    "            y_pred = torch.argmax(net_pred, dim=1)\n",
    "            train_loss += nn.NLLLoss()(net_pred, y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 15.\n",
    "        train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().to(device=device).view(X.shape[0], -1)\n",
    "            y_test = y_test.to(device=device)\n",
    "\n",
    "            net_pred = dm_map.predict_net(X_test, inference=True)\n",
    "            y_pred = torch.argmax(net_pred, dim=1)\n",
    "            test_loss += nn.NLLLoss()(net_pred, y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 15.\n",
    "        test_acc /= (15. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            epoch, train_loss, test_loss, train_acc, test_acc, dm_map.h))\n",
    "        if (epoch * 30) % 600 == 0 and epoch > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(28 * 28, 200, 10), nn.ReLU(), SteinLinear(200, 200, 10), nn.ReLU(), SteinLinear(200, 10, 10)).to(device=device)\n",
    "data_distr = ClassificationDistribution(10)\n",
    "dm = DistributionMover(task='net_class',\n",
    "                       n_particles=10,\n",
    "                       n_hidden_dims=700,\n",
    "                       use_latent=True,\n",
    "                       net=net,\n",
    "                       data_distribution=data_distr\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.7252, dtype=torch.float64, grad_fn=<DivBackward0>), tensor(0.1250))"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = 0. \n",
    "train_acc = 0.\n",
    "for __ in range(15):\n",
    "    X_train, y_train = next(iter(dataloader_m_train))\n",
    "    X_train = X_train.double().to(device=device).view(X_train.shape[0], -1)\n",
    "    y_train = y_train.to(device=device)\n",
    "            \n",
    "    net_pred = dm.predict_net(X_train, inference=True)\n",
    "    y_pred = torch.argmax(net_pred, dim=1)\n",
    "    train_loss += nn.NLLLoss()(net_pred, y_train)\n",
    "    train_acc += torch.sum(y_pred == y_train).float()\n",
    "    \n",
    "train_loss /= 15.\n",
    "train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm.lt.tetta_0 = dm_map.parameters_to_vector(dm_map.parameters_net()).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20... Empirical Loss(Train/Test): 4.556/9.285\t Accuracy(Train/Test): 0.875/0.831\t Kernel factor: 703.70821"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step_size = 0.01\n",
    "    for epoch in range(30):\n",
    "        for train_epoch in range(30):\n",
    "            X, y = next(iter(dataloader_m_train))\n",
    "            X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "            y = y.to(device=device)\n",
    "            dm.update_latent_net(h_type=0, X_batch=X, y_batch=y, train_size=dataloader_m_train.__len__(), step_size=step_size, move_tetta_0=True)\n",
    "        \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().to(device=device).view(X.shape[0], -1)\n",
    "            y_train = y_train.to(device=device)\n",
    "            \n",
    "            net_pred = dm.predict_net(X_train, inference=True)\n",
    "            y_pred = torch.argmax(net_pred, dim=1)\n",
    "            train_loss += nn.NLLLoss()(net_pred, y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 15.\n",
    "        train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.s\n",
    "        test_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().to(device=device).view(X.shape[0], -1)\n",
    "            y_test = y_test.to(device=device)\n",
    "            \n",
    "            net_pred = dm.predict_net(X_test, inference=True)\n",
    "            y_pred = torch.argmax(net_pred, dim=1)\n",
    "            test_loss += nn.NLLLoss()(net_pred, y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 15.\n",
    "        test_acc /= (15. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            epoch, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "        if (epoch * 30) % 600 == 0 and epoch > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5), tensor(3))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X_test.shape[0], -1)\n",
    "y_pred = dm.predict_net(X_test, inference=True)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projections(dm, use_real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MNIST with FC neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_nn = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ").double()\n",
    "optim_nn = torch.optim.Adam(net_nn.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for _, (X, y) in enumerate(dataloader_m_train):\n",
    "        X = X.double().view(X.shape[0], -1)\n",
    "        \n",
    "        optim_nn.zero_grad()\n",
    "        train_loss = nn.CrossEntropyLoss()(net_nn(X), y)\n",
    "        train_loss.backward()\n",
    "        optim_nn.step()\n",
    "   \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_train)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(net_nn(X_train), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 30.\n",
    "        train_acc /= (30. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_test)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(net_nn(X_test), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 30.\n",
    "        test_acc /= (30. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            _, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X.shape[0], -1)\n",
    "y_pred = net_nn(X_test)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marginal_density = lambda x : (0.3 * normal_density(mu=-2., std=1., n=1)(x) + 0.7 * normal_density(mu=2., std=1., n=1)(x))\n",
    "#marginal_density = lambda x : (normal_density(mu=0., std=2., n=1)(x))\n",
    "\n",
    "pdf = []\n",
    "for _ in np.linspace(-10, 10, 1000): \n",
    "    _ = torch.tensor(_, dtype=t_type, device=device)\n",
    "    pdf.append(marginal_density(_))\n",
    "    \n",
    "from pynverse import inversefunc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "cum_density = interp1d(np.linspace(-20, 20, 250), [quad(marginal_density, -20, x)[0] for x in np.linspace(-20, 20, 250)])\n",
    "inv_cum_density = inversefunc(cum_density)\n",
    "real_samples = [inv_cum_density(np.random.random()) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean = quad(lambda x : marginal_density(x) * x, -20, 20)[0]\n",
    "var = quad(lambda x : marginal_density(x) * (x - mean) ** 2, -20, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7999999999999998, 4.359999999999999)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff6d7673518>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(-10, 10, 1000), pdf)\n",
    "plt.plot(real_samples, np.zeros_like(real_samples))\n",
    "sns.kdeplot(real_samples, kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=20, n_hidden_dims=5, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot_projections(dm, use_real=True, N_plots_max=1, pdf=pdf)\n",
    "# plot_projections(dm, use_real=False, N_plots_max=1, pdf=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_projections(dm, use_real=True, kernel='tri', pdf=pdf)\n",
    "            plot_projections(dm, use_real=False, kernel='tri', pdf=pdf)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(torch.mean(dm.lt.transform(dm.particles, n_particles_second=True)),\n",
    " torch.mean(torch.std(dm.lt.transform(dm.particles, n_particles_second=True), dim=1) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=2, n_hidden_dims=1, use_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAD8CAYAAABaf4GAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl4lNXZP/DvnQnZ94WwG5ZMwoSdAAUEFZWi9YK6I62iYhWUi7pVfOtSS6tFfLV9f3UXl0pd6lIRkaooKCAFCRKWLCQBIoSwZCM7WWbO74+ZhJlnZpJJMmGWfD/XNZdzznOemTNPRnLnPuc5R5RSICIiIiL3CPB0B4iIiIj8CYMrIiIiIjdicEVERETkRgyuiIiIiNyIwRURERGRGzG4IiIiInIjBldEREREbsTgioiIiMiNGFwRERERuVGgpzuglZCQoJKTkz3dDSIin7J79+4ypVRiN1+jb2Bg4GoAo8A/vonaYwJwoKWl5Y6JEyee1h70uuAqOTkZmZmZnu4GEZFPEZGfuvsagYGBq/v16zcyMTGxMiAggHujETlhMpmktLTUcPLkydUA5mqP8y8TIiJqNSoxMbGagRVR+wICAlRiYmIVzFle++PnuT9EROS9AhhYEbnG8v+KwziKwRURERGRGzG4IiIir/L222/HiMjEPXv2hHiyH/fee++AtWvXRnb3dcrKynQrV67s9M0G999//4DHH388qbvv74qDBw8GpaSkpAPAli1bwm699dbBALB+/frIjRs3hre2W7VqVeLzzz8f31Pv3Z5rr702+c0334x153uPHz8+rbUPL7/8cpy7XpfBFREReZX3338/bsKECbVr1qxx2y+75ubmTp/zt7/9reSXv/xlTXffu7y8XPf666/37e7rnC8zZ86sf+utt44BwKZNmyK3bt0a0XrsoYceKl26dGm553rnXnv27MkDgIKCguB//etfDK6IiKhniPzvxJ58tPfeVVVVAZmZmRFvvvlm0SeffNKWpVi/fn1kRkZG6uWXXz58+PDh6QsWLBhiNBoBAGFhYeN/85vfDDIYDCOnTp2qLykpCQSAyZMnpy5dunTgpEmTUv/85z8n5efnB02dOlWv1+sNU6dO1RcUFAQBwKWXXjq8NRvzzDPPJMydO3coYJspGThw4OilS5cOHDduXNqoUaNGbtu2LezCCy9MGTx48KhVq1YltvZ96tSpeoPBMFKv1xv++c9/xgDAAw88MOjYsWPBaWlphrvuumsQADz22GNJo0aNGqnX6w333XffgNbPuXz58n7Jycmjpk2bpi8oKAh2dI2OHTsWePnllw9PTU01pKamGlozS0888URSSkpKekpKSvqKFSv6AuaMzLBhw9Lnz59/wYgRI9KnT5+eUltbKwCwdevWsNTUVMO4cePSnnvuubbgb/369ZGXXHLJiIMHDwa9/fbbiS+//HJSWlqa4YsvvoiwzqZt3749dOzYsWl6vd5w+eWXDy8tLdW1XvclS5YMHD169Mjk5ORRX3zxRURrXyZOnJhqMBhGGgyGkdYZMUdMJhNuueWWIcOHD0+/+OKLR5SVlbWtcLB169awSZMmpaanp4+88MILU3766ac+7b13ZmZmyOjRo0empaUZ9Hq9Yf/+/cGt3x0AeOSRRwZmZmZGpKWlGf74xz/2nThxYur27dtDW99vwoQJaTt37gyFixhcEbnZ6dN1yM31mz/siM6rd955J+biiy+uGjNmTGNMTIxx27ZtYa3H9u/fH/5///d/xw4ePJhdVFQU/Pbbb8cCQENDQ8CECRPqc3JycqdPn17z8MMPtwUrZ86c0e3atevgH//4x1OLFy8esmDBgvL8/PycG2+8sXzJkiWDAeCtt976adWqVf2/+OKLiBdeeKHfa6+9dtRR3wYPHtyUlZWVN2XKlNrbb789+bPPPju0c+fOvJUrVw4AgLCwMNPnn39emJOTk/vdd9/l//73vx9kMpnw7LPPFg8ePLgxLy8v55VXXin+97//HVVYWBiyb9++3Nzc3JysrKyw//znPxFbt24N++STT+L279+fs379+sK9e/c6DD4WL148ZMaMGTUHDx7Myc7OzpkwYcLZrVu3hr377rvxu3fvzs3MzMx9++23E7///vtQADh69GjIsmXLThcWFmZHR0cbW6/bokWLkp977rmjWVlZeY7eJzU1temWW24pXbx48am8vLycOXPm1Fofv/XWW4c+9dRTxfn5+Tnp6ekNy5cvb7vuLS0tsn///tynn3762IoVKwYAwIABA1q2bt2an5OTk/uvf/3r8H333Tekve/CmjVrYgoLC4MPHjyY/dZbb/30448/RgBAY2OjLFu2bMinn356KDs7O3fhwoVlDz744MD23vvvf/974t13330qLy8vZ9++fblDhw5tsn6vJ5988nhGRkZtXl5ezh/+8IfTt956a9nq1asTAGDfvn3BTU1NMmXKlIb2+muNwRWRm1RXN+LXv/4cSUkvwWB4E/fdt9nTXSLyOR988EHcTTfdVAkA1157bYX10ODo0aPrDAZDU2BgIG644YaK1uGqgIAA3HHHHRUAcPvtt5f/8MMPbcNYN910U0Xr8z179oTfeeedFQCwZMmSit27d0cAwODBg1t+//vfl1x11VWpTz311LGkpCSjo77dcMMNZyz9qJ8wYUJdbGysacCAAS3BwcGmsrIynclkknvvvXeQXq83XHLJJfrTp08HFRcX260n+cUXX0Rt2bIlymAwGNLT0w2HDh0KycvLC9m8eXPElVdeeSYyMtIUFxdnmj179hlH/di+fXvk7373u1IACAwMRHx8vPHbb7+NuPLKK89ERUWZoqOjTb/4xS8qN2/eHAkAAwcObJw2bVoDAIwfP76+qKgouLy8XFdTU6P7xS9+Udt63Vz/KZmHOq3P/81vflO+Y8eOtut+/fXXVwLAtGnT6oqLi4MAoKmpSRYsWJCs1+sN119//fBDhw61O6fuu+++i7zhhhsqAgMDkZyc3Dx16tQawBzsFBQUhM6aNUuflpZmeOaZZ/qXlJT0ae+9p06dWvfss8/2f+SRR/oVFBQERUREtHtX7K233lr59ddfRzc2NsrLL7+csGDBgrLOXB+vW0SUyFf95jdf4YMPDraVn39+Dx599GeIj3c5k0zUq508eVK3Y8eOqPz8/NClS5fCaDSKiKiXXnqpGABExKa9tuyoPjIy0uTKe+/fvz80Ojq65fjx432ctQkJCVGAOZgLCgpq++UcEBCA5uZmeeWVV+LKy8sD9+/fnxscHKwGDhw4uqGhwS6JoZTCvffee+J3v/udzS/sFStW9HX2mTqilPNYwbqvOp1ONTQ0BCilnF4/d2i9VoGBgTAajQIATz75ZFLfvn2bP/744yMmkwmhoaHtDhEDjn/GSikZMWJEg7OMm6P3Xrx4ccWMGTPqPvnkk+grrrhC/+KLLxbNnTvX6Xy6yMhI04wZM6rffffdmHXr1sXt3r07x6UPbsHMFZEbmEwK69cfsqlraTHh6NFqD/WIqOuUenB3Tz6cve+aNWtir7nmmvKSkpL9x48f33/y5Ml9gwYNavrqq68iAPOwYF5eXpDRaMRHH30UN2PGjBrAPDendW7UW2+9FT958mSHvzTHjx9ft3r16lgAeOWVV+IyMjJqAWDz5s1h33zzTfTu3btznn/++X55eXlBXbluVVVVuoSEhObg4GD12WefRZaUlAQBQHR0tLGurq7t9+0VV1xRvWbNmoSqqqoAADhy5Eif48ePB86aNav2888/j6mtrZXKysqAjRs3xjh6n+nTp9c888wziQDQ0tKCioqKgFmzZtVu2LAhpqamJqC6ujpgw4YNsZdcconT4CEhIcEYERFh/PLLLyMs183hZO7IyEhjTU2NTlsfHx9vjIqKMrbOaXr99dfjp06dWmv/CrbXp3///s06nQ4vvvhifOucOWcuuuiimg8//DCupaUFP/30U58dO3ZEAsCYMWPOVlRUBH799dfhgHmYMDMzs90sWE5OTtDIkSMbH3300dOzZ88+k5WVZfNXb3R0tLG2ttbmcy5evLhs+fLlg8eOHVvnLJvpDIMrIjc4cqQK9fUtdvWnT9d7oDdEvunDDz+Mv+aaayqt6+bNm1fZOjQ4bty42gceeGCQXq9PHzJkSOPNN998BgBCQ0NN2dnZoenp6SO3bNkS+Ze//OWEo9d/6aWXjq5ZsyZBr9cb3nvvvfgXX3zxWENDgyxevDh59erVRcnJyc1PPfXUsYULFyabTC4lvGzccccdFXv37g0fNWrUyH/+859xQ4cOPQsA/fr1M06cOLE2JSUl/a677hp0zTXXVF9//fUVkyZNStPr9Yarr756+JkzZ3QXXnhh/dVXX10xatSo9Kuuumr45MmTHQYrL7300tHvvvsuUq/XG0aNGmX48ccfQy+88ML6BQsWlE+YMGHkxIkTR958882l06dPb3eO0Ouvv160bNmyIePGjUsLDQ11mPq69tprz3z++ecxrRParY+9+eabR5YvXz5Ir9cb9u3bF7py5cqS9t7v3nvvPf3ee+/Fjx07Ni0/Pz8kNDS03Yt88803nxk2bFhjampq+qJFi4a0Bs0hISHq/fffP/Twww8PSk1NNaSnpxu+++67iPZea82aNXF6vT49LS3NUFBQEHLXXXfZDINOnjy5ITAwUKWmphr++Mc/9gWAGTNm1IeHhxtvu+22Tg0JAoC0l0r0hIyMDMW9BcnXrF1bgKuv/tSu/u23r8DNN3e4fAtRt4nIbqVURndeY+/evUVjx47t9C+S82H9+vWRzz77bNLmzZsLtcfCwsLG19fX7/FEv8h/FRUV9bn44otTDx06dECns0veAQD27t2bMHbs2GRtPTNXRG5w4IDj30fMXBER+Z7nn38+/mc/+9nIxx9//LizwKo9LgVXIjJHRA6KSKGIPOzg+GIR2S8iWSKyTUQMlvpkEWmw1GeJyMud7iGRD2BwRdSzrrrqqhpHWSsAYNaK3G3p0qXlJ0+e3Hf77bdXdtzaXod3C4qIDsALAC4HUAxgl4isU0pZz5x/Vyn1sqX9XADPAZhjOXZIKTWuK50j8hX79zO4IiIiM1cyV5MBFCqlDiulmgC8D2CedQOllPUtUeEAvGsiF1EPMhpNKChw/McNgysiot7HleBqIIBjVuViS50NEblHRA4BWAVgmdWhoSKyR0S+E5EZjt5ARO4UkUwRySwtLe1E94k879ixGjQ3O77phcEVEVHv40pw5WiVMbvMlFLqBaXUcADLATxqqT4BYIhSajyA+wG8KyJRDs59VSmVoZTKSEzs9MbhRB5VWOhwEWUAQGmpy7slEBGRn3BlhfZiAIOtyoMAtLeWxfsAXgIApVQjgEbL892WzJYeANdaIL/hbEgQMGeuenolZKKe8qf1OQM6buW6x64ytLsO0smTJ3UXX3xxKgCUlZX1CQgIUHFxcS0AkJWVldu68ra3SkpKGpOdnZ0dExNjnDJlSuru3bsP5uTkBG3bti38zjvvrASATZs2hb/zzjtxr7/++rGOXq8r752QkOB0scvnnnsu4cCBA6FvvPGG2977uuuuS37sscdOjBo1qvGxxx7r99RTT51012v7MlcyV7sApIjIUBEJAjAfwDrrBiKSYlX8BYACS32iZUI8RGQYgBQAh93RcSJvUVjoPLhqaGhBXV3zeewNke/q16+fMS8vLycvLy/HesPgvLy8nNbAymQyoaOVvT0tMDAQu3fvPggA+fn5wR988EHb6uezZs2qc3dg5UkfffRR0dixYxtNJhP+/ve/9/N0f7xFh8GVUqoFwFIAXwLIBfCBUipbRFZY7gwEgKUiki0iWTAP/y201M8EsE9E9gL4CMBipVQFiPxIe8OCAOddEXXXgQMHglNSUtIXLFgwxLLRcVBkZGTbXeivvvpq7I033ngBABw7dixw9uzZw0eNGjVy9OjRI7/55ptw7es1Nzdj0aJFg1NSUtL1er1h5cqViQDwySefRKWlpRn0er1h/vz5F5w9e1YAc1bo/vvvHzBy5EiDZTXyYAAoKSkJnDZtWorBYBj5q1/9akjrotzNzc1o7d+jjz46aOfOnZFpaWmGP//5z33Xrl0bedlllw0HgBMnTgTOmjVrhF6vN4wfPz5t165dIQCwbNmyATfccMMFkyZNSh00aNDov/zlL23zZWbNmjUiPT195IgRI9Kfe+65hI6u3XPPPZeQnJw8avLkyak7d+5suxbOrpOz966srAyYOXNmSmpqqiElJSW9dbuhiRMnpm7fvj30nnvuGVRfX69LS0szXH311cn33HPPQOt+L1myZGDrde4NXFrnSim1QSmlV0oNV0o9aal7XCm1zvL8t0qpdKXUOKXUJUqpbEv9x5b6sUqpCUqpz3ruoxB5RkfBVUXF2fPUEyL/dejQoZC77rqrLDc3N2fo0KFNztotXrx4yPLly08eOHAg96OPPjq0ePHiZG2bVatW9T158mSf3Nzc7Pz8/JzbbrutoqamJmDJkiXJH3/88aH8/Pyc+vr6AOvgJSkpqTk3NzfnlltuKVu5cmUSADz00EMDZs6cWZOTk5N7xRVXVJeWltpt+vznP/+5eMqUKTV5eXk5jz766GnrYw8++OCASZMm1ebn5+c89thjJbfddttQ68+7devW/J07d+Y+/fTTA1tazNtrvffee0eys7Nz9+zZk/vCCy8klZaWOl3h8tChQ33+93//t/+OHTtyt2zZkp+Xl9e2n15718nRe3/00UfRgwcPbjx48GBOQUFB9ty5c202Tn3hhReKw8LCjHl5eTmffPJJ0d133132zjvvJADm/Q8/++yzuEWLFvWa5ApXaCfqBqUUjhypsqkbM8b2j7PKSgZXRN01ePDgxosuuqjDNPD3338fdc8991yQlpZmmDdv3oiqqipdbW2tzaTHTZs2RS5evLg0MNA87TgpKcmYlZUVkpycfDY9Pb0RABYuXFi+bdu2yNZzFixYUAkAkydPrjt27FgwAOzcuTOyNWD49a9/fSY8PLxTGxLu2rUr4o477qgAgGuuuab69OnTfaqrqwMAYM6cOVUhISFq4MCBLdHR0S0lJSWBAPDUU08lpaamGjIyMtJOnToVlJubG+zs9bds2RIxffr0mn79+hlDQkLU1Vdf3TaHob3r5Oi9J06c2PDtt99G33333QO/+uqr8Pj4+HbHZtPT0xsjIiKMP/zwQ+iHH34YPW7cuLrExETvHs91I1cmtBORE3V1zWhoOLdhc3CwDsOHx2DfvnNLijC4Iuo+601+AwJs8wJnz55tq1BKdTj5XSkl2ptMOtpnt3VjY51OB6PR2HayiHR5kr1SSjTltufBwcHWn1c1NzfL2rVrI7dv3x65e/fu3IiICDVx4sTUhoaGdpMkzvrX3nVy9N4TJkw4u3v37pyPP/44evny5YM3bdp0ZuXKle1OXl+4cGHZa6+9Fn/06NHgu+66q1ets8TMFVE3aOdT9e0bhthY2z8kKysbz2eXiPyeTqdDVFSUcf/+/cFGoxGffvppTOux6dOnVz/99NNt6ePt27eHas+/7LLLql566aXE1qG2U6dO6caPH3+2qKgoJCcnJwgA1qxZEz9jxoya9voxZcqUmjfeeCMeAN59993ouro6u9+pUVFRprq6OodDd5bz4wBg7dq1kUlJSc1RUVFOs19nzpzRxcTEtERERKjMzMyQ/fv3280nszZz5sza77//PurUqVO6s2fPyqeffhrbesyV62TtyJEjfaKjo0333HNPxbJly05lZWWFWR/v08c8ItrcfO4GnoULF1Zu3LgxJicnJ3TevHnV6EWYuSLqBsfBVYhNHTNX5Ks6WjrBk5544oniK6+8MmXAgAFNer3+bFNTkwDA6tWrj95+++1D9Hp9gtFolGnTptVMmzbtqPW5DzzwQFlBQUFIWlpauk6nU4sWLSp96KGHSl988cWia665ZoTJZML48ePr7rvvPsf7WlmsWrWq5LrrrhtmMBhip0+fXtO3b1+7W4OnTZtWbzQaJTU11XDzzTeXjRo1qm3xu2eeeabkV7/6VbJerzeEh4eb3nzzzSPtvd8NN9xQtXr16sTU1FTDiBEjzo4ZM6auvfbDhw9vfuCBB05MmTJlZN++fZvHjh3b1t6V62Tthx9+CHvssccGBgQEoE+fPuqll176Sdtm/vz5ZWlpaeljxoyp++STT4rCwsLUlClTapKSkpq7svmxL5OOUqHnW0ZGhsrM5DJY5BvWrSvEvHlr28pXXDEU06cPxKOPbmure+ihSXj66Ys80T3qRURkt1IqozuvsXfv3qKxY8e2G1AQucpoNGLkyJGGtWvXFhoMBqc3IfiyvXv3JowdOzZZW89hQaJu4LAgEZG9H374IXTIkCGjZ82aVe2vgVV7OCxI1A0cFiQisjd58uSG48eP7/d0PzyFmSuibjh1yja4SkpicEU+zWQymbhXE5ELLP+vOLwBgcEVkZXKyrN49dW92LTpaIe3ZgOuZq44LEg+40BpaWk0Ayyi9plMJiktLY0GcMDRcQ4LElm0tJgwffq7yM01LyL8j39cgVtuSW/3HNfmXDFzRb6hpaXljpMnT64+efLkKPCPb6L2mAAcaGlpucPRQQZXRBbffnusLbACgCVLNnYxuOKwIPmmiRMnngYwt8OGRNQu/mVCZLFz5wmbcn19Cw4f7tymzH37hiEmxjZzVVXVCJPJu5Y8ISKinsPgisji2DH7xZg3bDjstL3JpFBW1mBTl5AQij59dIiMDGqrU8ocYBERUe/A4IrIIien3K7uiy+KnLavqWmyyUiFh/dBcLB5pF0776qigkODRES9BYMrIovcXPvgKivrtNP22rlU1nOt4uJst+mqqLDNcBERkf9icEUEoLS03m6IDwCOH6/FmTOOs07abFRc3LngKiHBNrgqL2fmioiot2BwRQTHWatWjoYLAUeZq3NDgfHxtncMOgrciIjIPzG4IgJQUOD8rsDsbFeDK+eZKwZXRES9B4MrIgAFBZVOj2Vnlzms78ywIIMrIqLew6XgSkTmiMhBESkUkYcdHF8sIvtFJEtEtomIwerY/1jOOygiP3dn54ncpf3gqvuZq/JyBldERL1Fh8GViOgAvADgCgAGADdZB08W7yqlRiulxgFYBeA5y7kGAPMBpAOYA+BFy+sReZX2gqsjR6oc1mv3DLQNrsJsjjFzRUTUe7iSuZoMoFApdVgp1QTgfQDzrBsopaqtiuEAWhf/mQfgfaVUo1LqCIBCy+sReQ2TSaGw0Pmcq+LiGoebOLc3LMgJ7UREvZcrwdVAAMesysWWOhsico+IHII5c7WsM+cSedKJE7VoaGhpK0dHB9ussN7YaHQ4rMcJ7URE5IgrwZU4qLP7M14p9YJSajiA5QAe7cy5InKniGSKSGZpaakLXSJyn++/P25TTkmJweDBkTZ1jrbGaW8pBs65IiLqvVwJrooBDLYqDwJQ0k779wH8sjPnKqVeVUplKKUyEhMTXegSkXuYTApPPbXTpm7ixH4YNMg2uCourrU7VzvnynZY0D5z5WhokYiI/I8rwdUuACkiMlREgmCeoL7OuoGIpFgVfwGgwPJ8HYD5IhIsIkMBpAD4ofvdJnKPXbtOYu/ec9lSEWDZsvEYNCjCpt2xY9XaU+22tLEeFgwL64OwsMC2cnOzCTU1Te7qNhERebHAjhoopVpEZCmALwHoALyhlMoWkRUAMpVS6wAsFZHLADQDqASw0HJutoh8ACAHQAuAe5RSxh76LESdlp9fYVOeM2coDIYEu2FBVzJX1sEVYM5e1defG04sK2tAVJTths5EROR/OgyuAEAptQHABk3d41bPf9vOuU8CeLKrHSTqrpYWE4qKqjBkSBSCgmxXAikttc0+jRgRAwB2w4LaOVctLSZUVdkGVzExtoFTQkKozXllZQ0YNiymax+CiIh8BldoJ79WVdWI8ePfRkrK6xg37h84darO5vjp0/U25b59zetT2c+5sg2uHN0pqNPZ/u+knXfFSe1ERL0Dgyvya++8k4MDB8zb1+TmVuChh76zOV5aahtcJSaagyvtsOBPP9nOudIGStp1rRzVlZeftWtDRET+h8EV+bUdO07YlN9+Owd5eee2s9EOCyYmmrNNF1wQZVN/9Gg1WlpMbWVtoKTNUjmqY+aKiKh3YHBFfq22ttmu7sUXs9qeOxsWjIgIagu0AMBoVDZDg65krrjWFRFR78TgivyaNngCgA8+ONiWhXI2LAgAQ4dG2xyz3mNQu+K6a5krDgsSEfUGDK7Ir504Yb+EwqlT9di06SgA55krAHZ39lkHV/aZK0fBlXbOFTNXRES9AYMr8ltKKZw4Uefw2MaNRWhoaLYZNgwMDLBZTqG9zJU2C6UdAgQcr9JORET+j8EV+a2qqkabDZmt/fRTtd1k9oSEUIic2w6z/eCKdwsSEZFjDK7IbznLWgHmRUG1862shwQB++Dq8GHnmStHw4Kc0E5E1DsxuCK/1V5wVVxcazffyvruQODcau2t9u49jeZm8+5Nrs25YnBFRNQbMbgiv+VoMnurkpJam2E+AOjXL9ymfMEFUTZ19fUt+PHH0wAc3S1oPywYGRmEwMBz/4vV17egocF+aQgiIvIvDK7Ib7WXuTKZFLZsKbapS0mJtSmLCGbOHGRTt2XLMQCuZa5EhPOuiIh6IQZX5Le0+whqtS7H0EobXAFwEFwVQymFigrtnCv7zJW5nkODRES9DYMr8luOFhC1pr1bMCUlxq6NNrjaseMESkvr0dx8biuciIg+CA3t4/A9tJPkT55sP+AjIiLfx+CK/JY2uNJuxqzlKHNlMMQjJCSwrVxW1oD//td2v0LtXYXWBgyIsCm3N1RJRET+gcEV+S1tZmrChCSnbRMTQxETYz+0p9MFYNSoeJu6Tz8ttCm3F1z17287SZ7BFRGR/2NwRX5Lm7maOrW/07aOslatxoxJtCl3L7hyfgcjERH5BwZX5JeUUnbB1aWXXuC0vV7venClnczefnBlOyxYUsLgiojI3zG4Ir9UU9OExkZjWzkkJBDjxvVFcLDOYftJk/o5fS1tcKXV/pwrDgsSEfU2DK7IL2nnW/XjOfFmAAAf7klEQVTtG4rAwAAYDPEO20+Z4nzIcOzYRFhtOWinM5krBldERP7PpeBKROaIyEERKRSRhx0cv19EckRkn4h8IyIXWB0zikiW5bHOnZ0ncsZ+axvzkgijRyfYtQ0JCWw3OxUXF4oZMwY5Pd7ZCe1KKaftiYjI93UYXImIDsALAK4AYABwk4gYNM32AMhQSo0B8BGAVVbHGpRS4yyPuW7qN1G7tMFV63pTo0bZB1cTJyahTx/Hw4WtFiwY6bD+gguiEBER5PS8yMgghIefWwPr7NkWnDnT2O57ERGRb3MlczUZQKFS6rBSqgnA+wDmWTdQSm1WSrX+NtsBwPmf+UTngbPg6tpr9dDpbMf45sxJ7vD1rrtO73C+1pIlY9s9T0R4xyARUS/jSnA1EMAxq3Kxpc6ZRQD+Y1UOEZFMEdkhIr90dIKI3Glpk1laWupCl4jaV1rqOLgaNiwGW7fehHnzRmDEiBgsWDASv/3txA5fLz4+FE89NcOmLi4uBHffPb7Dc7ULiR4/zuCKiMifBXbcBI6m8jqcNCIivwaQAeAiq+ohSqkSERkGYJOI7FdKHbJ5MaVeBfAqAGRkZHBCCnVbWZnthPaEhHN7/E2dOgBr1zqM89t1//0ZAIA//OF76HQBeO+9qxAZ6XxIsNXAgQyuiIh6E1eCq2IAg63KgwCUaBuJyGUAHgFwkVKqbVKJUqrE8t/DIvItgPEADmnPJ3KnykrbtahiYx1vrNxZ99+fgbvvHofgYB2kvVsIrQwaZLvtTnFxjVv6QkRE3smVYcFdAFJEZKiIBAGYD8Dmrj8RGQ/gFQBzlVKnrepjRSTY8jwBwHQAOe7qPJEz2knjsbHBbnvtkJBAlwMrwFFwxcwVEZE/6zBzpZRqEZGlAL4EoAPwhlIqW0RWAMhUSq0D8AyACAAfWn7pHLXcGTgSwCsiYoI5kFuplGJwRT1Om7lytG/g+TJokO2wIDNXRET+zZVhQSilNgDYoKl73Or5ZU7O2w5gdHc6SNQVPZm56iwOCxIR9S5coZ38Uk/NueoKDgsSEfUuDK7IL2kzVzExnstcJSWF2aytVV7egIaGZo/1h4iIepZLw4JEvsRoNKG6usmmLjrac8GVTheAAQMicOzYueHA48drMWJEbI+951835tuU77tc32PvRUREtpi5Ir9TVWWbtYqKCoJO59mvunZo8MiRKg/1hIiIehqDK/I7lZXayeyem2/VKjXVNkt14ECZh3pCREQ9jcOC5HfOnNEuw+C5IcFWY8Yk2pT37XPPNk/Ww38c+iMi8g7MXJHf8cbMlTa42ruXe2gSEfkrBlfkd7SZK28MrvbsOY2ysnonrYmIyJdxWJD8jjZz5Q3DgomJYejfPxwnTtS11SUlvYT6+t8iONg9/xtq7xAkIiLPYOaK/I595srzwRUATJyYZFM2mRS++KLIM50hIqIew+CK/I595srzw4IA8NBDk+3qeNcgEZH/YXBFfscb7xYEgBkzBuHxx6fa1GVnM7giIvI3DK7I73jT6uxal146xKack1PuoZ4QEVFPYXBFfke7Qrs3BVfp6Qk25by8ChiNJg/1hoiIegLvFiS/U1Vlm7mKigryUE/sxceHom/fMJw+bV6GobHRiMOHq5CS4vo+g7wrkIjIuzFzRX7HmzNXAJCeHm9T5tAgEZF/YXBFfqe62ruDK70+zqZ86NAZD/WEiIh6AoMr8jvePCwIACNGxNiUCwsrPdQTIiLqCQyuyK8opbw+czV8uG1wdehQlYd6QkREPcGl4EpE5ojIQREpFJGHHRy/X0RyRGSfiHwjIhdYHVsoIgWWx0J3dp5Iq66uGUajaiuHhAQiKEjnwR7Zsw+uOCxIRORPOrxbUER0AF4AcDmAYgC7RGSdUirHqtkeABlKqXoRWQJgFYAbRSQOwB8AZABQAHZbzuU4CPUI+zWuvGtIEACGDYu2KRcVVaG52Yg+fXouCLS+w/C+y/U99j5ERORa5moygEKl1GGlVBOA9wHMs26glNqslKq3FHcAGGR5/nMAG5VSFZaAaiOAOe7pOpE97Z2CUVHeNSQIABERQejXL7ytbDQqHD1a48EeERGRO7kSXA0EcMyqXGypc2YRgP908VyibrFfhsH7MleA/dAgJ7UTEfkPV4IrcVCnHNRBRH4N8xDgM505V0TuFJFMEcksLS11oUtEjnn7GletUlJsg6vzudbVXzfmtz2IiMj9XAmuigEMtioPAlCibSQilwF4BMBcpVRjZ85VSr2qlMpQSmUkJia62nciO9o5V944LAgAo0fbfs/37eMfFURE/sKV4GoXgBQRGSoiQQDmA1hn3UBExgN4BebA6rTVoS8BzBaRWBGJBTDbUkfUI3xlWHDMGG1wVeahnhARkbt1eLegUqpFRJbCHBTpALyhlMoWkRUAMpVS62AeBowA8KGIAMBRpdRcpVSFiPwJ5gANAFYopSp65JMQwXeGBceOtQ2usrPL0NJiQmAgl54jIvJ1Lm3crJTaAGCDpu5xq+eXtXPuGwDe6GoHiTrD/m5B78xcJSaGoV+/cJw8WQfAvIFzQUElRo6M7+BMIiLydvwzmfyK/TpX3pm5AuyHBnftOumhnhARkTu5lLki8hW+MiwIAJMm9cNXXxW1lTdu/Am33JLusC3v7CMi8h3MXJFfOXPGNriKifHe4OrnP0+2KX/1VRFMJoernBARkQ9hcEV+xZcyVz/7WX9ERp6bE3b6dD2ysk63cwYREfkCBlfkV6qqfGfOVZ8+Olx66RCbuk2bjnqoN0RE5C4MrsivnDlz1qbszcOCADBrlm1wtW3bcQ/1hIiI3IXBFfkVX8pcAcCMGYNsytu2Hee8KyIiH8fgivyGUsqn5lwBwOjRCTZrcZWXN+DgQa6zS0Tkyxhckd+or29GS4uprRwcrENIiHevNqLTBWDatAE2dd9/z6FBIiJfxuCK/IavDQm2mjrVNrjau5ebOBMR+TIGV+Q3fG1IsNXYsX1tygyuiIh8G4Mr8hu+tICotTFjEmzK+/aVQilOaici8lUMrshv+GrmKjk52mYx0aqqRhw9Wu3BHhERUXd492xfok6wD66CnLT0LiKCMWMSbSay791bin/nn/Jgr4iIqKuYuSK/oR0WLK5rxF835vvEpsdjxybalDdvPnZe3rf1+vjCNSIi8hUMrshvaDNXIeG+k5jVboPz6aeFnHdFROSjfOe3D5GGUgp/+7qgrbwx64TN8dDwPue7S102e3YygoN1aGw0AgCOHKlCUU4FhqbHe7hnRETUWcxckc8pKanFZZd9gIEDX8Z3Hx9qq2+obbZpF+JDwVVERJBd9ur9Z7PQ2NDioR4REVFXMbgin/PEE9vxzTdHceJEHda9lo2TP9UAAOprbIOrsEjfCa4AYOHCdJtyaXEddn5x1EO9ISKirmJwRT7ntdf2tT1XJmDHhp8AAHWaOVfhUb5xt2Cr669PxTXXpNjUZf/3pId6Q0REXeVScCUic0TkoIgUisjDDo7PFJEfRaRFRK7THDOKSJblsc5dHafe6cyZs3Z1p46aM1d1msxVuI8sxdBKRLBy5UybusMHynG2rtnJGURE5I06DK5ERAfgBQBXADAAuElEDJpmRwHcCuBdBy/RoJQaZ3nM7WZ/qZfbt89+a5hj+WdgMinUafYW9LXMFQCkpMRCr49tKxtbFPL3cDscIiJf4krmajKAQqXUYaVUE4D3AcyzbqCUKlJK7QNg6oE+ErXJyrIPNOprmnHqpxrUVTsPrnxpPacrrxxmUz568IyHekJERF3hSnA1EID1iobFljpXhYhIpojsEJFfOmogInda2mSWlvKvdHIuK+u0w/qCrDI0W5YxAABdoCA4zDdXGhk3znZB0fIT9R7qCRERdYUrwZU4qOvM6oZDlFIZABYA+JuIDLd7MaVeVUplKKUyEhMT7V+ByCInp9xx/U7brWLCo4Ig4uir6/2GDYuxKZefqPNQT4iIqCtcCa6KAQy2Kg8CUOLqGyilSiz/PQzgWwDjO9E/IhtFRVUO6/N/tM14hvngfKtWw4ZF25TLTzJzRUTkS1wJrnYBSBGRoSISBGA+AJfu+hORWBEJtjxPADAdQE5XO0u9W0NDM06dci3Q8LU7Ba317x+B4GBdW7mhphn1NU3tnEFERN6kw+BKKdUCYCmALwHkAvhAKZUtIitEZC4AiMgkESkGcD2AV0Qk23L6SACZIrIXwGYAK5VSDK6oS45allxwRYQPZ64CAsQ+e8V5V0REPsOlGb9KqQ0ANmjqHrd6vgvm4ULtedsBjO5mH4nw1435yMt0PJndEV8dFmy9m1GibFeXLz9Zj8H6GEenEBGRl+EK7eQzKjox98gX17iyFtcvzKZcXsJJ7UREvsI371WnXqnydINNefwlA7Fn83GHbX15zhUAxPYNtSlXVzY6aek+1muA3Xe5vsffj4jIXzG4Ip9RoZnMnjIuAYVZZahxEHhExYY4fR1fCCK0mbf6ak5oJyLyFRwWJJ9xRpO5iusXhmGj4hy2TU6PdVjvK7TBlXb1eSIi8l4MrshnaAOMyJhgjJrW365d/IAwxPYNs6v3JdoJ+QyuiIh8B4Mr8hnatZ7CooIwalo/u3ZD9L6dtQI4LEhE5MsYXJFPUEqhobbZpi40og+CQwMxafZgm/qJl9qtCuJztBPymbkiIvIdnNBOPqHprBHGlnNbWgYGBSDIsor5VYsMOF5YhRNF1ci4bDBGTu7r8ut66+T20PA+EAGU5SOfrWuBscUEXSD/HiIi8nYMrsgn1NfYZq3CIs4tshkZG4wHX74YTY3GtoDL1wXoBKERfWw+d31NEyLbuQuSiIi8A/8MJp9gN98q0n4dK38JrFrZDw02O2lJRETehJkr8lrWQ3Z2860i+2ib+53wqCCU4tzK7HVVnHdFROQLmLkin+BK5srfaD8jJ7UTEfkGBlfkE9qbc+WvtMOC2gCTiIi8E4Mr8gl2wVUvGRa0xmFBIiLfwOCKfIKjNa78HYcFiYh8E4Mr8gmOVmf3dxwWJCLyTbxbkHyCP2eurO+KtBYeZfsZuRQDEZFvYOaKfIJ2b71ecbeg3ZyrRg/1hIiIOoPBFfmE+tpeeLdgpHbzZmauiIh8gUvBlYjMEZGDIlIoIg87OD5TRH4UkRYRuU5zbKGIFFgeC93Vcepd7O4WjOoFwZV2hfbzOOfqrxvz2x5ERNQ5Hc65EhEdgBcAXA6gGMAuEVmnlMqxanYUwK0AHtScGwfgDwAyACgAuy3nVrqn++RvnP0y9+c5V85ohz7ra5pgMikEBIiHekRERK5wJXM1GUChUuqwUqoJwPsA5lk3UEoVKaX2ATBpzv05gI1KqQpLQLURwBw39Jt6EZNR9crgKrBPAILDzv39o0z2QSYREXkfV+4WHAjgmFW5GMAUF1/f0bkDXTyXCADQUGcbUISEBUKnc/90Qeus2X2X693++l0RHhmExvqWtnJ9TZPd4qJERORdXPkN5WgMQrn4+i6dKyJ3ikimiGSWlpa6+NLUW/TGfQVb2c274kKiRERez5XgqhjAYKvyIAAlLr6+S+cqpV5VSmUopTISExNdfGnqLeyGBHvB1jettBP3eccgEZH3cyW42gUgRUSGikgQgPkA1rn4+l8CmC0isSISC2C2pY7IZb1xX8FW2uUYuNYVEZH36zC4Ukq1AFgKc1CUC+ADpVS2iKwQkbkAICKTRKQYwPUAXhGRbMu5FQD+BHOAtgvACksdkcsatMFVL5jM3spuIVFmroiIvJ5L298opTYA2KCpe9zq+S6Yh/wcnfsGgDe60Ufq5eprbecZhfamOVdR3F+QiMjXcIV28nq9elhQM6G9torBFRGRt+PGzeT1tBPa/SG4cnXlc7vMFe8WJCLyesxckdfTDoWFRvSiYUEuxUBE5HMYXJHX69XDgtoJ7RwWJCLyehwWJK9nF1z1orsFvSFz5Y0r1xMReTMGV+RxHc0/auDdgm3qqpqglIIIN28mIvJWDK7I63liWNBbsjV9gnQIDtWhscEIADCZFM7WtfSKjauJiHwV51yR17NbRLQXZa4AIDwq2KZcy1XaiYi8GoMr8motTUY0NRrbygEBguBQnQd7dP55w7wrIiJyHYMr8mr1Dta46m3zjbSbN/OOQSIi78bgiryadr5VaC9ahqGV3aR2Zq6IiLwagyvyato7BXvbfCsACI+2nXPFzBURkXfj3YLk1eqrNZkrH75LztUtb7QimLkiIvIpzFyRV3M056q3sZvQzswVEZFXY+aKvJp2X0FPDAt6es0rb9oCx9PXgojIFzC4Io9wdYisgZkrLsVARORjOCxIXs3ubkEfnnPVVd6UuSIioo4xuCKv5g3Dgp7GzBURkW9hcEVezW5fQWauUF/TBJNReag3RETUEQZX5NXs5lxF9b7gShcYgJDwc9MjlQLqa5m9IiLyVi5NaBeROQD+D4AOwGql1ErN8WAAbwOYCKAcwI1KqSIRSQaQC+CgpekOpdRi93SdegPtsGBohGeHBTt7t1xX17bSCo8Owtm6lrZyXVUTIjSLixIRkXfoMLgSER2AFwBcDqAYwC4RWaeUyrFqtghApVJqhIjMB/A0gBstxw4ppca5ud/US2gnb2uHyHqL8KhglJfUt5U574qIyHu5Miw4GUChUuqwUqoJwPsA5mnazAPwD8vzjwBcKr1td11yO5NRcVjQIoILiRIR+QxXgquBAI5ZlYstdQ7bKKVaAFQBiLccGyoie0TkOxGZ4egNROROEckUkczS0tJOfQDyX/W1TVBW87ZDI/pAp+ud0wS5eTMRke9wZc6VowyU9lYlZ21OABiilCoXkYkA1opIulKq2qahUq8CeBUAMjIyeBsUAfD+IcHzuVo5t8AhIvIdrgRXxQAGW5UHAShx0qZYRAIBRAOoUEopAI0AoJTaLSKHAOgBZHa34+R7Oju5226NKy8Lrpxx1yR2a8xcERH5DleCq10AUkRkKIDjAOYDWKBpsw7AQgD/BXAdgE1KKSUiiTAHWUYRGQYgBcBht/We/Jo2O6Odd9SbaDNXtWcaPdSTc7jPIBGRYx0GV0qpFhFZCuBLmJdieEMplS0iKwBkKqXWAXgdwBoRKQRQAXMABgAzAawQkRYARgCLlVIVPfFByP/UevmwoLWeyFZZi4yxXXahptLzwRURETnm0jpXSqkNADZo6h63en4WwPUOzvsYwMfd7CP1UtqhL232pjeJjLMNrqoZXBERea3eeesV+YT6au4r2CoyNsSmXMvgiojIazG4Iq/FzNU5kTGaOVdVjdxfkIjISzG4Iq/l7UsxnE+BQTqERZ5bQFWZzAEWERF5H5fmXBF1VXcmettlrnpxcAUAkXEhqK85t2J9TWUjouJC2jmDiIg8gZkr8lp2matePCwIAFGxmkntFcxcERF5IwZX5LW0d8RFxPTu4CpSE1zVVp71UE+IiKg9HBYkr9TY0ILG+pa2si5QOCzoxcsxcEFRIqJzmLkir1RdYZuViYwNgYijLSx7D+1yDDUcFiQi8koMrsgraQOHKE3WpjfSXoOqcg4LEhF5Iw4Lktu5YysYbeaKd8UBMX1DbcqVp+o91BMiImoPM1fklbR3wkXFM3MV1zfMplx5usFDPSEiovYwc0VeyW7OFTNXiE4IgQSYFxAFzOtcNTUaERSs82zHNDi5nYh6O2auyCvZz7licKULDEBMgu3Q4Blmr4iIvA6DK/JK9nOuOCwIALGcd0VE5PU4LEhu4Y5J7NY4od2x2KQw4EBFW7mCwRURkddhcEVex2RSKD9hGzRExzO4Ahxkrrx8WJDzr4ioN+KwIHmd8hN1aDprbCuHRvaxW528t4rrZ3vHYFlJnYd6QkREzjBzRV3m7qHAVieOVNuUBwyL6vWrs7fqOzjCpnz8UJWHekJERM4wuCKvU3JYG1xFe6gn3mfg8Gib5RhKi+vQUNeM0PA+nu2YCzhESES9hUvDgiIyR0QOikihiDzs4HiwiPzLcnyniCRbHfsfS/1BEfm5+7pOnvDXjfltj55Scsg2uBo4LKrH3svXBIcGImlwpE1dcQGzV0RE3qTDzJWI6AC8AOByAMUAdonIOqVUjlWzRQAqlVIjRGQ+gKcB3CgiBgDzAaQDGADgaxHRK6WMIL/W2NCC08dqu3Tegf+etKnrP9QcXKV+8xku/dtjCGq0n8TdEBmDgxddgdQt/0Fo9ZmuddpLKADaQVAFYO9VN+HbZU9gUEo0Tv5U03Zs4zv5CAnrWhJ6sD6my/3sDmaxiMifufIv8mQAhUqpwwAgIu8DmAfAOriaB+AJy/OPADwv5kky8wC8r5RqBHBERAotr/df93SfzoeuZKlKDlfj7/dt6/Z7B4cFon9yJFK/+Qw/X/U76JRy2C6s5gzGrX/PLijxRY4+gwAYt/49AMBW/S3I/Lq47Vjh3jL8demWzr+PAM9+ObeLvXQfBlpE5G9cCa4GAjhmVS4GMMVZG6VUi4hUAYi31O/QnDuwy70lt+vJ4T13mHn1MAQG6XDhm885Daxa+UNg1R4BMHbDBxj5xu/w6SvZMBnbvx6+yNn3kUEXEfkSV4IrR7+ztP+qO2vjyrkQkTsB3Gkp1orIQRf65UwCgLJunN9T2K/OSQBQtvEdYOM7wDvARE93yCuYjNh964TdDo50+ueoFHD/7N+5p1/OueX7db8bOqLh1d/7Lp57gTs7QkRd50pwVQxgsFV5EIASJ22KRSQQQDSAChfPhVLqVQCvut5t50QkUymV4Y7Xcif2q3PYr85hvzqH/SKinuTK3YK7AKSIyFARCYJ5gvo6TZt1ABZanl8HYJNSSlnq51vuJhwKIAXAD+7pOhEREZH36TBzZZlDtRTAlwB0AN5QSmWLyAoAmUqpdQBeB7DGMmG9AuYADJZ2H8A8+b0FwD28U5CIiIj8mUv3byulNgDYoKl73Or5WQDXOzn3SQBPdqOPneWW4cUewH51DvvVOexX57BfRNRjRHVwBxYRERERuY4bNxMRERG5kU8GVyJyvYhki4hJRDI0xzrcbscyOX+niBRYtu0J6oE+/ktEsiyPIhHJctKuSET2W9plursfDt7vCRE5btW3K520a3fLox7o1zMikici+0TkExFxuHT4+bpe3dnyqQf7NFhENotIruX7/1sHbS4WkSqrn+/jjl6rB/rW7s9FzP6f5XrtE5EJ56FPqVbXIUtEqkXkXk2b83K9ROQNETktIges6uJEZKPl36GNIhLr5NyFljYFIrLQURsi8jJKKZ97ABgJIBXAtwAyrOoNAPYCCAYwFMAhADoH538AYL7l+csAlvRwf58F8LiTY0UAEs7jtXsCwIMdtNFZrt0wAEGWa2ro4X7NBhBoef40gKc9db1c+fwA7gbwsuX5fAD/Og8/u/4AJlieRwLId9CviwGsP1/fJ1d/LgCuBPAfmNe++xmAnee5fzoAJwFc4InrBWAmgAkADljVrQLwsOX5w46+8wDiABy2/DfW8jz2fP98+eCDj849fDJzpZTKVUo5Wmi0bbsdpdQRAK3b7bSxbMszC+ZtegDgHwB+2VN9tbzfDQDe66n36AFtWx4ppZoAtG551GOUUl8ppVosxR0wr4nmKa58/nkwf3cA83fpUsvPuscopU4opX60PK8BkAvf2fFgHoC3ldkOADEi0v88vv+lAA4ppX46j+/ZRim1BeY7qa1Zf4ec/Tv0cwAblVIVSqlKABsBzOmxjhKRW/hkcNUOR1v1aH/5xAM4Y/WLvKe35JkB4JRSqsDJcQXgKxHZbVmp/nxYahmaecPJUIQr17En3Q5zlsOR83G9XPn8Nls+AWjd8um8sAxDjgew08HhqSKyV0T+IyLp56lLHf1cPP2dmg/nf+B44noBQJJS6gRgDpwB9HXQxtPXjYi6wKWlGDxBRL4G0M/BoUeUUp86O81Bnatb9XSai328Ce1nraYrpUpEpC+AjSKSZ/krt8va6xeAlwD8CebP/CeYhyxv176Eg3O7fVupK9dLRB6BeU20d5y8jNuvl6OuOqjrse9RZ4lIBICPAdyrlKrWHP4R5qGvWst8urUwL97b0zr6uXjyegUBmAvgfxwc9tT1cpXHrhsRdZ3XBldKqcu6cJor2+2UwTwkEWjJODjckscdfRTzVkDXoJ198ZRSJZb/nhaRT2AekupWsODqtROR1wCsd3DIpW2L3N0vy2TdqwBcqpRy+AukJ66XA93Z8qlHiUgfmAOrd5RS/9Yetw62lFIbRORFEUlQSvXoPnou/Fx65DvloisA/KiUOqU94KnrZXFKRPorpU5YhkhPO2hTDPO8sFaDYJ5rSkRezN+GBTvcbsfyS3szzNv0AOZte5xlwrrrMgB5SqliRwdFJFxEIlufwzyp+4Cjtu6imedytZP3c2XLI3f3aw6A5QDmKqXqnbQ5X9erO1s+9RjLnK7XAeQqpZ5z0qZf69wvEZkM8//j5T3cL1d+LusA3GK5a/BnAKpah8TOA6fZY09cLyvW3yFn/w59CWC2iMRahvBnW+qIyJt5ekZ9Vx4wBwXFABoBnALwpdWxR2C+0+sggCus6jcAGGB5PgzmoKsQwIcAgnuon28BWKypGwBgg1U/9loe2TAPj/X0tVsDYD+AfTD/495f2y9L+UqY70Y7dJ76VQjz3JIsy+Nlbb/O5/Vy9PkBrIA5+AOAEMt3p9DyXRp2Hq7RhTAPCe2zuk5XAljc+j0DsNRybfbCfGPAtPPQL4c/F02/BMALluu5H1Z3+fZw38JgDpairerO+/WCObg7AaDZ8m/XIpjn6H0DoMDy3zhL2wwAq63Ovd3yPSsEcNv5uG588MFH9x5coZ2IiIjIjfxtWJCIiIjIoxhcEREREbkRgysiIiIiN2JwRURERORGDK6IiIiI3IjBFREREZEbMbgiIiIiciMGV0RERERu9P8BU+0NYVM7IDgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_condition_distribution(dm, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 100...\n",
      "Kernel factor 0.19547862144313558"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(200): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "            \n",
    "            #plot_projections(dm, use_real=True, pdf=pdf)\n",
    "            plot_condition_distribution(dm, 100000)\n",
    "            \n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-5, 5, 0.025)\n",
    "Y = np.arange(-5, 5, 0.025)\n",
    "XX, YY = np.meshgrid(X, Y)\n",
    "XY = torch.stack([torch.tensor(XX, dtype=t_type), torch.tensor(YY, dtype=t_type)], dim=2)\n",
    "Z = torch.zeros([XY.shape[0], XY.shape[1]])\n",
    "\n",
    "Z = dm.real_target_density(XY.permute(2, 0, 1).view(2, -1)).view(Z.shape)\n",
    "Z = Z.cpu().data.numpy()\n",
    "\n",
    "# Plot the surface.\n",
    "plt.contour(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "# Plot conditioning line\n",
    "XXX, YYY = dm.lt.transform(torch.tensor(X, dtype=t_type, device=device).view(1, -1), n_particles_second=True).data.cpu().numpy()\n",
    "plt.plot(XXX, YYY, 'r', label='Linear manifold', )\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Density contour lines\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare implementations of Stein Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class SVGD():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def svgd_kernel(self, theta, h = -1):\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = h / np.log(theta.shape[0]+1)\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = 2. * dxkxy / (h)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    " \n",
    "    def update(self, x0, lnprob, n_iter = 1000, stepsize = 1e-3, bandwidth = -1, alpha = 0.9, debug = False):\n",
    "        # Check input\n",
    "        if x0 is None or lnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        theta = np.copy(x0) \n",
    "        \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter+1) % 1000 == 0:\n",
    "                print( 'iter ' + str(iter+1) )\n",
    "            \n",
    "            lnpgrad = lnprob(theta)\n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(theta, h = bandwidth)  \n",
    "            grad_theta = (np.matmul(kxy, lnpgrad) + dxkxy) / x0.shape[0]  \n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + grad_theta ** 2\n",
    "            else:\n",
    "                historical_grad = alpha * historical_grad + (1 - alpha) * (grad_theta ** 2)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            theta = theta + stepsize * adj_grad \n",
    "            \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_ex = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lnprob(particles):\n",
    "    grad_log_term = torch.zeros([particles.shape[0], particles.shape[1]], dtype=t_type, device=device)\n",
    "        \n",
    "    for idx in range(100):\n",
    "        particle = torch.tensor(particles[idx:idx+1], dtype=t_type, device=device, requires_grad=True)\n",
    "        log_term = torch.log(dm_ex.target_density(particle.t()))\n",
    "        grad_log_term[idx] = torch.autograd.grad(log_term, particle,\n",
    "                                                     only_inputs=True, retain_graph=False, \n",
    "                                                     create_graph=False, allow_unused=False)[0]\n",
    "    return grad_log_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = dm_ex.particles.data.clone().detach().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare kernel implementation\n",
    "\n",
    "ker, grad_ker = dm_ex.calc_kernel_term_latent(10.)\n",
    "print(dm_ex.h)\n",
    "grad_ker_sum = torch.sum(grad_ker, dim=1)\n",
    "\n",
    "ker_l, grad_ker_sum_l = dm_svgd.svgd_kernel(result, h=10.)\n",
    "\n",
    "print((ker - torch.tensor(ker_l, dtype=t_type, device=device)).norm())\n",
    "print((grad_ker_sum - torch.tensor(grad_ker_sum_l, dtype=t_type, device=device).t()).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare log_term implementation\n",
    "(lnprob(result) - dm_ex.calc_log_term_latent().t()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(201): \n",
    "    try:\n",
    "        result = dm_svgd.update(result, lnprob, stepsize=1e-2, bandwidth=-1, n_iter=20)\n",
    "        result = torch.tensor(result, dtype=t_type, device=device)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(result.data.cpu().numpy(), torch.zeros_like(result).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(result.data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(2000): \n",
    "    try:\n",
    "        dm_ex.update_latent(h_type=0)\n",
    "\n",
    "        if _ % 30 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm_ex.h))\n",
    "            \n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(dm_ex.particles.t().data.cpu().numpy(), torch.zeros_like(dm_ex.particles.t()).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(dm_ex.particles.t().data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(result - dm_ex.particles.t()).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"Stein-Variational-Gradient-Descent/python/\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.matlib as nm\n",
    "from svgd import SVGD\n",
    "\n",
    "r\"\"\"\n",
    "    Example of Bayesian Logistic Regression (the same setting as Gershman et al. 2012):\n",
    "    The observed data D = {X, y} consist of N binary class labels, \n",
    "    y_t \\in {-1,+1}, and d covariates for each datapoint, X_t \\in R^d.\n",
    "    The hidden variables \\theta = {w, \\alpha} consist of d regression coefficients w_k \\in R,\n",
    "    and a precision parameter \\alpha \\in R_+. We assume the following model:\n",
    "        p(\\alpha) = Gamma(\\alpha; a, b)\n",
    "        p(w_k | a) = N(w_k; 0, \\alpha^-1)\n",
    "        p(y_t = 1| x_t, w) = 1 / (1+exp(-w^T x_t))\n",
    "\"\"\"\n",
    "class BayesianLR:\n",
    "    def __init__(self, X, Y, batchsize=100, a0=1, b0=0.01):\n",
    "        self.X, self.Y = X, Y\n",
    "        # TODO. Y in \\in{+1, -1}\n",
    "        self.batchsize = min(batchsize, X.shape[0])\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.permutation = np.random.permutation(self.N)\n",
    "        self.iter = 0\n",
    "    \n",
    "        \n",
    "    def dlnprob(self, theta):\n",
    "        \n",
    "        if self.batchsize > 0:\n",
    "            batch = [ i % self.N for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize) ]\n",
    "            ridx = self.permutation[batch]\n",
    "            self.iter += 1\n",
    "        else:\n",
    "            ridx = np.random.permutation(self.X.shape[0])\n",
    "            \n",
    "        Xs = self.X[ridx, :]\n",
    "        Ys = self.Y[ridx]\n",
    "        \n",
    "        w = theta[:, :-1]  # logistic weights\n",
    "        alpha = np.exp(theta[:, -1])  # the last column is logalpha\n",
    "        d = w.shape[1]\n",
    "        \n",
    "        wt = np.multiply((alpha / 2), np.sum(w ** 2, axis=1))\n",
    "        \n",
    "        coff = np.matmul(Xs, w.T)\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-1 * coff))\n",
    "        \n",
    "        dw_data = np.matmul(((nm.repmat(np.vstack(Ys), 1, theta.shape[0]) + 1) / 2.0 - y_hat).T, Xs)  # Y \\in {-1,1}\n",
    "        dw_prior = -np.multiply(nm.repmat(np.vstack(alpha), 1, d) , w)\n",
    "        dw = dw_data * 1.0 * self.X.shape[0] / Xs.shape[0] + dw_prior  # re-scale\n",
    "        \n",
    "        dalpha = d / 2.0 - wt + (self.a0 - 1) - self.b0 * alpha + 1  # the last term is the jacobian term\n",
    "        \n",
    "        return np.hstack([dw, np.vstack(dalpha)])  # % first order derivative \n",
    "    \n",
    "    def evaluation(self, theta, X_test, y_test):\n",
    "        theta = theta[:, :-1]\n",
    "        M, n_test = theta.shape[0], len(y_test)\n",
    "\n",
    "        prob = np.zeros([n_test, M])\n",
    "        for t in range(M):\n",
    "            coff = np.multiply(y_test, np.sum(-1 * np.multiply(nm.repmat(theta[t, :], n_test, 1), X_test), axis=1))\n",
    "            prob[:, t] = np.divide(np.ones(n_test), (1 + np.exp(coff)))\n",
    "        \n",
    "        prob = np.mean(prob, axis=1)\n",
    "        acc = np.mean(prob > 0.5)\n",
    "        llh = np.mean(np.log(prob))\n",
    "        return [acc, llh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = X_test.shape[0] + X_train.shape[0]\n",
    "X_input = np.hstack([X, np.ones([N, 1])])\n",
    "y_input = y\n",
    "d = X_input.shape[1]\n",
    "D = d + 1\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.2, random_state=42)\n",
    "    \n",
    "a0, b0 = 1, 0.01 #hyper-parameters\n",
    "model = BayesianLR(X_train, y_train, 100, a0, b0) # batchsize = 100\n",
    "    \n",
    "# initialization\n",
    "M = 100  # number of particles\n",
    "theta0 = np.zeros([M, D]);\n",
    "alpha0 = np.random.gamma(a0, b0, M); \n",
    "for i in range(M):\n",
    "    theta0[i, :] = np.hstack([np.random.normal(0, np.sqrt(1 / alpha0[i]), d), np.log(alpha0[i])])\n",
    "    \n",
    "theta = SVGD().update(x0=theta0, lnprob=model.dlnprob, bandwidth=-1, n_iter=6000, stepsize=0.05, alpha=0.9, debug=True)\n",
    "    \n",
    "print('[accuracy, log-likelihood]')\n",
    "print(model.evaluation(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Metropolis–Hastings algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MHAlgorithm():\n",
    "    def __init__(self, n_dims): \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                           0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "        self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "        \n",
    "#         self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "#         self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "    \n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_dims, 1],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., -1.)\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "    def generate_next(self):\n",
    "        previous = self.particles[:, -1].unsqueeze(1)\n",
    "#         self.additional_distribution = normal_density(self.n_dims, previous, 1, n_particles_second=True)\n",
    "#         candidate = self.additional_distribution.get_sample()\n",
    "        \n",
    "#         alpha = (self.target_density(candidate) * self.additional_distribution.unnormed_density(previous) / \n",
    "#                 (self.target_density(previous) * self.additional_distribution.unnormed_density(candidate)))\n",
    "        candidate = previous + torch.zeros([self.n_dims, 1], device=device, dtype=t_type).uniform_(-1., 1.)\n",
    "        alpha = self.target_density(candidate) / self.target_density(previous)\n",
    "        \n",
    "#         print(self.target_density(candidate)[0].data.numpy(), self.additional_distribution.unnormed_density(previous)[0].data.numpy(),\n",
    "#               self.target_density(previous)[0].data.numpy(), self.additional_distribution.unnormed_density(candidate)[0].data.numpy())\n",
    "        \n",
    "        if alpha > self.one:\n",
    "            self.particles = torch.cat([self.particles, candidate], dim=1)\n",
    "        else:\n",
    "            random = torch.bernoulli(alpha)\n",
    "            if random:\n",
    "                self.particles = torch.cat([self.particles, candidate], dim=1)                \n",
    "            else:\n",
    "                self.particles = torch.cat([self.particles, previous], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mh = MHAlgorithm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    mh.generate_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(mh.particles.t().data.cpu().numpy()[:,1], \n",
    "            kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(mh.particles.t().data.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import time\n",
    "\n",
    "mu,sig,N = 0,1,1000000\n",
    "pts = []\n",
    "\n",
    "def q(x):\n",
    "#     return (1/(math.sqrt(2*math.pi*sig**2)))*(math.e**(-((x-mu)**2)/(2*sig**2)))\n",
    "#     return normal_density(1, 0., 2., n_particles_second=True).unnormed_density(x)\n",
    "    return (0.3 * normal_density(1, -2., 1., n_particles_second=True).unnormed_density(x) +\n",
    "            0.7 * normal_density(1, 2., 1., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "def metropolis(N):\n",
    "    r = np.zeros(1)\n",
    "    p = q(r[0])\n",
    "    pts = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        rn = r + np.random.uniform(-1,1)\n",
    "        pn = q(rn[0])\n",
    "        if pn >= p:\n",
    "            p = pn\n",
    "            r = rn\n",
    "        else:\n",
    "            u = np.random.rand()\n",
    "            if u < pn/p:\n",
    "                p = pn\n",
    "                r = rn\n",
    "        pts.append(r)\n",
    "    \n",
    "    pts = np.array(pts)\n",
    "    return pts\n",
    "    \n",
    "def hist_plot(array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1,)\n",
    "    ax.hist(array, bins=1000)    \n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(metropolis(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gg =  lambda : (normal_density(1, -60., 20., n_particles_second=True).get_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot([gg()[0,0] for _ in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

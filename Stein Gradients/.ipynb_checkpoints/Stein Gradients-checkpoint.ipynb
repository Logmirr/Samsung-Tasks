{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orth\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "device = None\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    use_cuda = True\n",
    "\n",
    "# Ignore cuda\n",
    "use_cuda = False\n",
    "device = None\n",
    "\n",
    "# Set DoubleTensor as a base type for computations\n",
    "t_type = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import functools\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Support classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pairwise_diffs(x, y, n_particles_second=True):\n",
    "    '''\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: x is a dxN matrix\n",
    "                   y is an optional dxM matirx\n",
    "            Output: diffs is a dxNxM matrix where diffs[i,j] is the subtraction between x[:,i] and y[:,j]\n",
    "            i.e. diffs[i,j] = x[:,i] - y[:,j]\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: x is a Nxd matrix\n",
    "                   y is an optional Mxd matirx\n",
    "            Output: diffs is a NxMxd matrix where diffs[i,j] is the subtraction between x[i,:] and y[j,:]\n",
    "            i.e. diffs[i,j] = x[i,:]-y[j,:]\n",
    "    '''\n",
    "    if n_particles_second:\n",
    "        return x[:,:,np.newaxis] - y[:,np.newaxis,:]        \n",
    "    return x[:,np.newaxis,:] - y[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pairwise_dists(diffs=None, n_particles_second=True):\n",
    "    '''\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: diffs is a dxNxM matrix where diffs[i,j] = x[:,i] - y[:,j]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[:,i] - y[:,j]||\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: diffs is a NxMxd matrix where diffs[i,j] = x[i,:] - y[j, :]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[i,:]-y[j,:]||\n",
    "    '''\n",
    "    if n_particles_second:\n",
    "        return torch.norm(diffs, dim=0)\n",
    "    return torch.norm(diffs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     71,
     86,
     99,
     114,
     127
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class normal_density():\n",
    "    '''\n",
    "        Multinomial normal density for independent random variables. \n",
    "    '''\n",
    "    def __init__(self, n=1, mu=0., std=1., n_particles_second=True):\n",
    "        '''\n",
    "        Args:\n",
    "            n (int): number of dimentions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            mu (float, array_like): mean of distribution\n",
    "                Default: 0.\n",
    "                if mu is float - use same mean across all dimentions\n",
    "                if mu is 1D array_like - use different mean for each dimention but same for each particles dimention\n",
    "                if mu is 2D array_like - use different mean for each dimention\n",
    "            std (float, array_like): std of distribution\n",
    "                Default: 1.\n",
    "                if std is float - use same std across all dimentions\n",
    "                if std is 1D array_like - use different std for each dimention but same for each particles dimention\n",
    "                if std is 2D array_like - use different std for each dimention\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applyed along particles axis\n",
    "                Input will be reduced along all axis exclude particle axis\n",
    "        '''\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        \n",
    "        if type(self.mu) == float:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.mu.shape) == 1:\n",
    "            if self.mu.shape[0] == 1:\n",
    "                self.mu = self.mu.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.mu.shape) == 2:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if type(self.std) == float:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.std.shape) == 1:\n",
    "            if len(self.std) == 1:\n",
    "                self.std = self.std.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.zero = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.pi = torch.tensor(math.pi, dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "    def __call__(self, x, n_axis=None):        \n",
    "        '''\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.pow(self.two * self.pi, -self.n / self.two) / \n",
    "                torch.prod(self.std, dim=n_axis) * \n",
    "                torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):      \n",
    "        '''\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std , self.two), dim=n_axis))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):  \n",
    "        '''\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (-self.n / self.two * torch.log(self.two * self.pi) + \n",
    "                torch.sum(torch.log(self.std), dim=n_axis) -\n",
    "                self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return -self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)\n",
    "    \n",
    "    def get_sample(self):\n",
    "        '''\n",
    "            Sample from normal distribution\n",
    "        '''\n",
    "        sample = torch.normal(self.mu, self.std)\n",
    "        if self.n_particles_second:\n",
    "            return sample.view(-1, 1)\n",
    "        else:\n",
    "            return sample.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     73,
     87,
     101,
     115,
     129,
     143
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class gamma_density():\n",
    "    '''\n",
    "        Multinomial gamma density for independent random variables. \n",
    "    '''\n",
    "    def __init__(self, n=1, alpha=1, betta=1, n_particles_second=True):\n",
    "        '''\n",
    "        Args:\n",
    "            n (int): number of dimentions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            alpha (float, array_like): shape of distribution\n",
    "                Default: 1.\n",
    "                if alpha is float - use same shape across all dimentions\n",
    "                if alpha is 1D array_like - use different shape for each dimention but same for each particles dimention\n",
    "                if alpha is 2D array_like - use different shape for each dimention\n",
    "            betta (float, array_like): rate of distribution\n",
    "                Default: 1.\n",
    "                if betta is float - use same rate across all dimentions\n",
    "                if betta is 1D array_like - use different rate for each dimention but same for each particles dimention\n",
    "                if betta is 2D array_like - use different rate for each dimention\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applyed along particles axis  \n",
    "        '''\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.betta = betta\n",
    "        \n",
    "        if type(self.alpha) == float:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.alpha.shape) == 1:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.alpha.shape) == 2:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if type(self.betta) == float:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.betta.shape) == 1:\n",
    "            if len(self.betta) == 1:\n",
    "                self.betta = self.betta.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "        ### log Г(alpha)\n",
    "        self.lgamma = torch.lgamma(self.alpha)\n",
    "        ### Г(alpha)\n",
    "        self.gamma = torch.exp(self.lgamma)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(self.betta, self.alpha) / self.gamma * torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_density_log_x(self, log_x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density_log_x(self, log_x, n_axis=None):\n",
    "        '''\n",
    "            Evaluate log unnormed density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        '''\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     51,
     59,
     71,
     83
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SteinLinear(nn.Module):\n",
    "    '''\n",
    "        Custom full connected layer for Stein Gradient Neural Networks\n",
    "        Transformation: y = xA + b\n",
    "        Parameters prior: p(w, a) = p(w|a)p(a) = П p(w_i|a_i)p(a_i)\n",
    "                          p(w_i|a_i) = N(w_i|0, a_i^(-1)); p(a_i) = G(1e-4, 1e-4)\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, n_particles=1, bias=True):\n",
    "        super(SteinLinear, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            in_features (int): size of each input sample\n",
    "            out_features (int): size of each output sample\n",
    "            n_particles (int): number of particles\n",
    "            bias (bool): If set to False, the layer will not learn an additive bias.\n",
    "                Default: ``True``\n",
    "        '''\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_particles = n_particles\n",
    "        \n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.zeros([in_features, out_features, n_particles], dtype=t_type, device=device))\n",
    "        self.log_weight_alpha = torch.nn.Parameter(torch.zeros([in_features * out_features, n_particles], dtype=t_type, device=device))\n",
    "        \n",
    "        self.bias = None\n",
    "        self.log_bias_alpha = None\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros([1, out_features, n_particles], dtype=t_type, device=device))\n",
    "            self.log_bias_alpha = torch.nn.Parameter(torch.zeros([out_features, n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        ### define prior on alpha p(a) = G(1e-4, 1e-4)\n",
    "        self.weight_alpha_log_prior = lambda x: (gamma_density(n=self.log_weight_alpha.shape[0],\n",
    "                                                               alpha=1e-4,\n",
    "                                                               betta=1e-4,\n",
    "                                                               n_particles_second=True\n",
    "                                                              ).log_unnormed_density_log_x(x))\n",
    "        self.bias_alpha_log_prior = lambda x: (gamma_density(n=self.log_bias_alpha.shape[0],\n",
    "                                                             alpha=1e-4,\n",
    "                                                             betta=1e-4,\n",
    "                                                             n_particles_second=True\n",
    "                                                            ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    ### useless function - all initialization defined in DistributionMover class\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.log_weight_alpha.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.log_bias_alpha.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "            Apply transformation: X_out[i, :, :] = X_in[i, :, :] * W + b[i]\n",
    "        Args:\n",
    "            X (torch.tensor): tensor \n",
    "        Shape:\n",
    "            Input: [n_particles, batch_size, in_features]\n",
    "            Output: [n_particles, batch_size, out_features]\n",
    "        '''\n",
    "        ### NEED SOME OPTIMIZATION TO OMMIT .permute\n",
    "        return torch.bmm(X, self.weight.permute(2, 0, 1)) + self.bias.permute(2, 0, 1)\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        '''\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        '''\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())\n",
    "        \n",
    "    def calc_log_prior(self):\n",
    "        '''\n",
    "            Evaluate log prior of trainable parameters\n",
    "        log p(w,a) = log p(w|a) + log p(a\n",
    "        '''\n",
    "        ### define prior on weight p(w|a) = N(0, 1 / alpha)\n",
    "        self.weight_log_prior = lambda x: (normal_density(n=self.weight.numel() // self.n_particles,\n",
    "                                                          mu=0.,\n",
    "                                                          std=self.one / torch.exp(self.log_weight_alpha),\n",
    "                                                          n_particles_second=True\n",
    "                                                         ).log_unnormed_density(x))\n",
    "        \n",
    "        self.bias_log_prior = lambda x: (normal_density(self.bias.numel() // self.n_particles,\n",
    "                                                        mu=0.,\n",
    "                                                        std=self.one / torch.exp(self.log_bias_alpha),\n",
    "                                                        n_particles_second=True\n",
    "                                                       ).log_unnormed_density(x))\n",
    "        \n",
    "        return (self.weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha) +\n",
    "                self.bias_log_prior(self.bias.view(-1, self.n_particles)) + self.bias_alpha_log_prior(self.log_bias_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0,
     44,
     77
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LinearTransform():\n",
    "    '''\n",
    "        Class for various linear transformations\n",
    "    '''\n",
    "    def __init__(self, n_dims, n_hidden_dims, use_identity=False, normalize=False):\n",
    "        '''\n",
    "        Args:\n",
    "            n_dims (int): dimension of the space\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_identity (bool): If set to True, use 'eye' matrix for transformations\n",
    "            normalize (bool): If set to True, columns of the transformation matrix is an orthonormal basis \n",
    "        '''\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_identity = use_identity\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if self.use_identity:\n",
    "            return\n",
    "        \n",
    "        self.A = torch.zeros([self.n_dims, self.n_hidden_dims], dtype=t_type, device=device)\n",
    "        self.tetta_0 = torch.zeros([self.n_dims, 1], dtype=t_type, device=device)\n",
    "        if not self.use_identity: \n",
    "            self.A.uniform_(-1., 1.)\n",
    "            self.tetta_0.uniform_(-1.,1.)\n",
    "        else:\n",
    "            if self.n_dims != self.n_hidden_dims:\n",
    "                raise RuntimeError(\"Cannot use identity transformation in spaces with differrent dimensions\")\n",
    "            self.A = torch.eye(self.n_dims)\n",
    "            \n",
    "        ### normalize columns of matrix A\n",
    "        if self.normalize:\n",
    "            self.A = torch.tensor(orth(self.A.data.numpy()), dtype=t_type, device=device)\n",
    "        #self.A /= self.A.norm(dim=0)\n",
    "        \n",
    "        ### A^(t)A\n",
    "        self.AtA = torch.matmul(self.A.t(), self.A)\n",
    "        ### (A^(t)A)^(-1)\n",
    "        self.AtA_1 = torch.inverse(self.AtA)\n",
    "        ### (A^(t)A)^(-1)A^(t)\n",
    "        self.inverse_base = torch.matmul(self.AtA_1, self.A.t())\n",
    "#         ### A(A^(t)A)^(-1)A^(t)\n",
    "#         self.projector_base = torch.matmul(self.A, self.inverse_base)\n",
    "        \n",
    "    def transform(self, tetta, n_particles_second=True):\n",
    "        '''\n",
    "            Transform tettas as follows: \n",
    "                tetta = Atetta` + tetta_0\n",
    "        '''\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.A, tetta) + self.tetta_0\n",
    "        return (torch.matmul(self.A, tetta.t()) + self.tetta_0).t()\n",
    "    \n",
    "    def inverse_transform(self, tetta, n_particles_second=True):\n",
    "        '''\n",
    "            Apply inverse transformation: \n",
    "                tetta` = (A^(t)A)^(-1)A^(t)(tetta - tetta_0)\n",
    "        '''\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.inverse_base, tetta - self.tetta_0)\n",
    "        return torch.matmul(self.inverse_base, tetta.t() - self.tetta_0).t()\n",
    "        \n",
    "    def project(self, tetta, n_particles_second=True):\n",
    "        '''\n",
    "            Project tettas onto Linear Space X = {Atetta` + tetta_0 for all tetta` in R^d}:\n",
    "                tetta_projected = A(A^(t)A)^(-1)A^(t)(tetta - tetta_0) + tetta_0\n",
    "        '''\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.projector_base, tetta - self.tetta_0) + self.tetta_0\n",
    "        return (torch.matmul(self.projector_base, tetta.t() - self.tetta_0) + self.tetta_0).t()\n",
    "    \n",
    "    def project_inverse(self, tetta, n_particles_second=True):\n",
    "        '''\n",
    "            Project and then apply inverse transform to tetta - tetta_0:\n",
    "                tetta_s_p_i = T^(-1)P(tetta - tetta_0)= (A^(t)A)^(-1)A^(t)tetta\n",
    "        '''\n",
    "        if self.use_identity:\n",
    "            return tetta\n",
    "        ### use solver trick: tetta_s_p_i : A^(t)Atetta_s_p_i = A^(t)tetta\n",
    "        if n_particles_second:\n",
    "            return torch.gesv(torch.matmul(self.A.t(), tetta), self.AtA)[0]\n",
    "        return torch.gesv(torch.matmul(self.A.t(), tetta.t()), self.AtA)[0].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     4,
     54,
     57
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RegressionDistribution(nn.Module):\n",
    "    '''\n",
    "        Distribution over data for regression task: p(D|w) = N(y_predicted|y, )\n",
    "    '''\n",
    "    def __init__(self, n_particles):\n",
    "        super(RegressionDistribution, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        '''\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        \n",
    "        ### define betta - variance of data distribution for regression tasks (betta = 1 / std ** 2)\n",
    "        self.log_betta = torch.nn.Parameter(torch.zeros([1, self.n_particles], dtype=t_type, device=device))\n",
    "        \n",
    "        ### define prior on betta p(betta)\n",
    "        self.betta_log_prior = lambda x: (gamma_density(n=1,\n",
    "                                                        alpha=1e-4,\n",
    "                                                        betta=1e-4,\n",
    "                                                        n_particles_second=True\n",
    "                                                       ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        ### Support tensors for computations\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "    \n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        '''\n",
    "            Evaluate log p(tetta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predicted values\n",
    "            train_size (int) - size of training dataset\n",
    "        Shapes:\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, 1]\n",
    "        '''\n",
    "        ### squeeze last axis because regression task is being solved\n",
    "        y_predict.squeeze_(2)\n",
    "        \n",
    "        batch_size = torch.tensor(y.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        self.log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                        mu=y,\n",
    "                                                        std=self.one / torch.sqrt(torch.exp(self.log_betta.expand(X.shape[0], self.n_particles).t())),\n",
    "                                                        n_particles_second=False\n",
    "                                                       ).log_unnormed_density(x))\n",
    "\n",
    "        return train_size / batch_size * self.log_data_distr(y_predict) + self.betta_log_prior(self.log_betta)\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        '''\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        '''\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     1,
     10,
     35,
     38
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ClassificationDistribution(nn.Module):\n",
    "    def __init__(self, n_particles):\n",
    "        super(ClassificationDistribution, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        '''\n",
    "        self.n_particles = n_particles\n",
    "    \n",
    "\n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        '''\n",
    "            Evaluate log p(tetta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predictions (array_like): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shapes: \n",
    "            X.shape = [batch_size, in_features]\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, n_classes]\n",
    "        '''\n",
    "        batch_size = torch.tensor(X.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        \n",
    "        probas = nn.LogSoftmax(dim=2)(y_predict)\n",
    "        probas_selected = torch.gather(input=probas, dim=2, index=y.view(1, -1, 1).expand(probas.shape[0], probas.shape[1], 1)).squeeze(2)\n",
    "        log_data = torch.sum(probas_selected, dim=1)\n",
    "        \n",
    "        return train_size / batch_size  * log_data\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        '''\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        '''\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0,
     1,
     127,
     175,
     272,
     304,
     333,
     354,
     391,
     434,
     440,
     447,
     467,
     485,
     506,
     530,
     553,
     580,
     591,
     598
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DistributionMover():\n",
    "    def __init__(self,\n",
    "                 task='app', \n",
    "                 n_particles=None,\n",
    "                 particles=None,\n",
    "                 target_density=None,\n",
    "                 n_dims=None,\n",
    "                 n_hidden_dims=None,\n",
    "                 use_latent=False,\n",
    "                 net=None,\n",
    "                 precomputed_params=None,\n",
    "                 data_distribution=None):\n",
    "        '''\n",
    "        Args:\n",
    "            task (str):\n",
    "                'app' | 'reg' | 'net_reg' | 'net_class'\n",
    "                - approximate target distribution\n",
    "                - @deprecated solve regression task inplace\n",
    "                - solve regression task using net\n",
    "                - solve classification task using net\n",
    "            n_particles (int): number of particles\n",
    "            particles (2D array_like): array which contains initialized particles\n",
    "            target_density (callable): computes probability density function of target distribution (only for 'app' task)\n",
    "            n_dims (int): dimension of the space where optimization is performed\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_latent (bool): If set to True, Subspace Stein is used\n",
    "            net (nn.Sequential): object which is used to make predictions (for 'net_reg' and' net_class' tasks)\n",
    "            precomputed_params (1D array_like): Precomputed parameters, which will be used for particles initialization\n",
    "            data_distribution (callable): computes probability over data p(D|w) (for 'net_reg' and' net_class' tasks)\n",
    "        '''\n",
    "        \n",
    "        self.task = task\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.particles = particles\n",
    "        self.target_density = target_density\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_latent = use_latent\n",
    "        self.net = net\n",
    "        self.precomputed_params = precomputed_params\n",
    "        self.data_distribution = data_distribution\n",
    "        \n",
    "\n",
    "        if self.task == 'net_reg' or self.task == 'net_class':\n",
    "            self.n_dims = 0\n",
    "            for module in self.modules_net():\n",
    "                if \"numel\" in dir(module):\n",
    "                    self.n_dims += module.numel() // self.n_particles\n",
    "\n",
    "        if not self.use_latent:\n",
    "            self.n_hidden_dims = self.n_dims\n",
    "\n",
    "        ### Learnable samples from the target distribution\n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_hidden_dims, self.n_particles],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., 2.)\n",
    "\n",
    "        ### Class for performing linear transformations\n",
    "        self.lt = None\n",
    "        if self.use_latent:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=False, \n",
    "                normalize=True\n",
    "            )\n",
    "        else:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=True, \n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "        if self.precomputed_params is not None:\n",
    "            self.particles = self.lt.inverse_transform(self.precomputed_params.unsqueeze(1).expand(self.n_dims, self.n_particles))\n",
    "\n",
    "        ### Functions of probability density of target distribution\n",
    "        self.target_density = None\n",
    "        self.real_target_density = None\n",
    "        if self.net is None:\n",
    "            # use unnormed probability density to speedup computations\n",
    "            if target_density is not None:\n",
    "                self.target_density = target_density\n",
    "                self.real_target_density = target_density\n",
    "            else:\n",
    "                self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                                   0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "                self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                        0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "\n",
    "                # self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "                # self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "        ### Number of iterations since begining\n",
    "        self.iter = 0\n",
    "\n",
    "        ### Adagrad parameters\n",
    "        self.fudge_factor = torch.tensor(1e-6, dtype=t_type, device=device)\n",
    "        self.step_size = torch.tensor(1e-2, dtype=t_type, device=device)\n",
    "        self.auto_corr = torch.tensor(0.9, dtype=t_type, device=device)\n",
    "\n",
    "        ### Gradient history term for adagrad optimization\n",
    "        self.historical_grad = None\n",
    "        if self.use_latent:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_hidden_dims, n_particles], dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_dims, n_particles], dtype=t_type, device=device)\n",
    "\n",
    "        ### Factor from kernel\n",
    "        self.med = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.h = torch.tensor(0., dtype=t_type, device=device)\n",
    "\n",
    "        ### Support tensors for computations\n",
    "        self.N = torch.tensor(self.n_particles, dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.three = torch.tensor(3., dtype=t_type, device=device)\n",
    "\n",
    "    @deprecated # use calc_kernel_term_latent / calc_kernel_term_latent_net\n",
    "    def calc_kernel_term(self, h_type):\n",
    "        '''\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4:\n",
    "                    - med(dist(tetta-tetta`)^2) / logN\n",
    "                    - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    - var(tetta) / logN * 2 * n_dims\n",
    "                    - var(diff(tetta-tetta`) / logN * n_dims\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        '''\n",
    "        ### diffs[i, j] = tetta_i - tetta_j\n",
    "        diffs = pairwise_diffs(self.particles, self.particles, n_particles_second=True)\n",
    "        ### dists[i, j] = ||tetta_i - tetta_j||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        ### sq_dists[i, j] = ||tetta_i - tetta_j||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "\n",
    "        ### RBF Kernel\n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 2:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            self.var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            self.var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1) * (self.n_dims)\n",
    "\n",
    "        kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "\n",
    "        grad_kernel = -self.two / self.h * kernel.unsqueeze(dim=0) * diffs\n",
    "\n",
    "        return kernel, grad_kernel\n",
    "\n",
    "    def calc_kernel_term_latent(self, h_type, kernel_type='rbf', p=None):\n",
    "        '''\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(tetta-tetta`)^2) / logN\n",
    "                    1 - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    3 - var(tetta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(tetta-tetta`) / logN * n_dims\n",
    "                    5 - med(dist(tetta-tetta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(tetta-tetta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atetta`_i + tetta_0, Atetta`_j + tetta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        '''\n",
    "        ### power for rational kernel\n",
    "        self.p = torch.tensor(p, dtype=t_type, device=device) if p is not None else None\n",
    "        \n",
    "        ### tetta = Atetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### diffs[i, j] = A(tetta`_i - tetta`_j)\n",
    "        diffs = pairwise_diffs(real_particles, real_particles, n_particles_second=True)\n",
    "        ### dists[i, j] = ||A(tetta`_i - tetta`_j)||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        ### sq_dists[i, j] = ||A(tetta`_i - tetta`_j)||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "\n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 2:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            self.var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            self.var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = self.var / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 5:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  self.med / (torch.pow(self.N, self.two) - self.one)\n",
    "        elif h_type == 6:\n",
    "            self.med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  self.med / (torch.pow(self.N, -self.one / self.p) - self.one)\n",
    "        elif h_type == 7:\n",
    "            self.med = torch.median(torch.matmul(real_particles.t(), real_particles)) + self.fudge_factor\n",
    "            self.h = self.med / torch.log(self.N + 1)\n",
    "        \n",
    "        kernel = None\n",
    "        grad_kernel = None\n",
    "        if kernel_type == 'rbf':\n",
    "            ### RBF Kernel:\n",
    "            ### kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "            kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "            ### grad_kernel[i, j] = -2/h * A(tetta`_i - tetta`_j) * kernel[i, j]\n",
    "            grad_kernel = -self.two / self.h * kernel.unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'imq':\n",
    "            ### IMQ Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, -self.one / self.two)\n",
    "            ### grad_kernel[i, j] = -1/h * A(tetta`_i - tetta`_j) * kernel^(3)[i, j]\n",
    "            grad_kernel = -self.one / self.h * torch.pow(kernel, self.three).unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'exp':\n",
    "            ### Exponential Kernel:\n",
    "            ### kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "            kernel = torch.exp(self.one / self.h * torch.matmul(real_particles.t(), real_particles))\n",
    "            ### grad_kernel[i, j] = 1/h * (Atetta`_j + tetta_0) * kernel[i, j]\n",
    "            grad_kernel = 1. / self.h * kernel.unsqueeze(0) * real_particles.unsqueeze(1)\n",
    "        elif kernel_type == 'rat':\n",
    "            ### RAT Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, self.p)\n",
    "            ### grad_kernel[i, j] = p/h * A(tetta`_i - tetta`_j) * kernel^((p - 1)/p)[i, j]\n",
    "            grad_kernel = self.p / self.h * torch.pow(kernel, (self.p - self.one) / self.p).unsqueeze(0) * diffs\n",
    "            \n",
    "        return kernel, grad_kernel\n",
    "    \n",
    "    def calc_kernel_term_latent_net(self, h_type, kernel_type='rbf', p=None):\n",
    "        '''\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(tetta-tetta`)^2) / logN\n",
    "                    1 - med(dist(tetta-tetta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(tetta-tetta`)) / logN * 2 * n_dims\n",
    "                    3 - var(tetta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(tetta-tetta`) / logN * n_dims\n",
    "                    5 - med(dist(tetta-tetta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(tetta-tetta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atetta`_i + tetta_0, Atetta`_j + tetta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(tetta`_i - tetta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atetta`_i + tetta_0, Atetta`_j + tetta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(tetta`_i - tetta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        '''\n",
    "        return self.calc_kernel_term_latent(h_type, p)\n",
    "\n",
    "    @deprecated # use calc_log_term_latent / calc_log_term_latent_regression / calc_log_term_latent_net\n",
    "    def calc_log_term(self):\n",
    "        '''\n",
    "            Calculate grad(log p(tetta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        '''\n",
    "        grad_log_term = torch.zeros(\n",
    "            [self.n_dims, self.n_particles], dtype=t_type, device=device)\n",
    "\n",
    "        for idx in range(self.n_particles):\n",
    "            ### tetta_i\n",
    "            particle = torch.tensor(\n",
    "                self.particles[:, idx:idx+1],\n",
    "                dtype=t_type,\n",
    "                requires_grad=True,\n",
    "                device=device)\n",
    "            ### log_term_i = log p(tetta_i)\n",
    "            log_term = torch.log(self.target_density(particle))\n",
    "            ### grad_log_term_i = dlog_term_i / dtetta_i\n",
    "            grad_log_term[:, idx:idx+1] = torch.autograd.grad(\n",
    "                log_term,\n",
    "                particle,\n",
    "                only_inputs=True,\n",
    "                retain_graph=False,\n",
    "                create_graph=False,\n",
    "                allow_unused=False)[0]\n",
    "\n",
    "        return grad_log_term\n",
    "    \n",
    "    def calc_log_term_latent(self):\n",
    "        '''\n",
    "            Calculate grad(log p(tetta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        '''\n",
    "        \n",
    "        ### tetta = A tetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_term = torch.log(self.target_density(real_particles))\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "    \n",
    "        grad_log_term = real_particles.grad\n",
    "        \n",
    "        return grad_log_term\n",
    "    \n",
    "    @deprecated\n",
    "    def calc_log_term_latent_regression(self, X, y, train_size):\n",
    "        '''\n",
    "            Calculate grad(log p(tetta)) \n",
    "        Args:\n",
    "            X (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input: \n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output: \n",
    "                [n_dims, n_particles]\n",
    "        '''\n",
    "        self.log_prior_distr = lambda x: (normal_density(self.n_dims, 0., 0.01, n_particles_second=True).log_unnormed_density(x))\n",
    "        self.log_data_distr = lambda x: (normal_density(X.shape[0], y, 1e-5, n_particles_second=True).log_unnormed_density(x))\n",
    "\n",
    "        batch_size = torch.tensor(X.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        grad_log_term = torch.zeros([self.n_dims, self.n_particles], dtype=t_type, device=device)\n",
    "        \n",
    "        ### tetta = A tetta` + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        ### get prediction for batch of data\n",
    "        predict_y = self.predict_regression(X, real_particles)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_data = self.log_data_distr(predict_y)\n",
    "        log_prior = self.log_prior_distr(real_particles)\n",
    "        log_term = (log_prior + train_size / batch_size * log_data)\n",
    "        \n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "    \n",
    "        grad_log_term = real_particles.grad\n",
    "        \n",
    "        return grad_log_term\n",
    "    \n",
    "    def calc_log_term_latent_net(self, X, y, train_size):\n",
    "        '''\n",
    "            Calculate grad(log p(tetta)) \n",
    "        Args:\n",
    "            X (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input: \n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output: \n",
    "                [n_dims, n_particles]\n",
    "        '''\n",
    "        self.log_data = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        self.log_prior = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        \n",
    "        ### get real net parameters: tetta_i = A tetta`_i + tetta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### init net with real parameters\n",
    "        self.vector_to_parameters(real_particles.view(-1), self.paramerets_net())\n",
    "        ### compute log prior of all weight in the net\n",
    "        for module in self.modules_net():\n",
    "            if \"calc_log_prior\" in dir(module):\n",
    "                self.log_prior += module.calc_log_prior()\n",
    "        \n",
    "        ### get prediction for the batch of data\n",
    "        y_predict = self.predict_net(X)\n",
    "        ### compute log data term log p(D|w)\n",
    "        self.log_data = self.data_distribution.calc_log_data(X, y, y_predict, train_size)\n",
    "        \n",
    "        ### log_term = log p(tetta) = log p_prior(tetta) + log p_data(D|tetta)\n",
    "        log_term = self.log_prior + self.log_data\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "        \n",
    "        ### collect all gradients into one vector\n",
    "        grad_log_term = self.parameters_grad_to_vector(self.paramerets_net()).view(-1, self.n_particles)\n",
    "            \n",
    "        return grad_log_term\n",
    "    \n",
    "    def paramerets_net(self):\n",
    "        '''\n",
    "            Return all trainable parameters\n",
    "        '''\n",
    "        return chain(self.net.parameters(), self.data_distribution.parameters())\n",
    "    \n",
    "    def modules_net(self):\n",
    "        '''\n",
    "            Return all modules\n",
    "        '''\n",
    "        return chain(self.net.modules(), self.data_distribution.modules())\n",
    "        \n",
    "    @deprecated # use one layer net for linear regression task \n",
    "    def predict_regression(self, X, weight=None):\n",
    "        '''\n",
    "            Predict values for regression task\n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            weight (array_like): \n",
    "                If None - average predictions across all particles\n",
    "                If not None - use weight to make prediction\n",
    "        Shapes:\n",
    "            X.shape = [batch_size, in_features]\n",
    "            weight.shape = [in_features, 1]\n",
    "        '''\n",
    "        X_1 = torch.cat(\n",
    "            (X, torch.tensor(1., dtype=t_type, device=device).expand(X.shape[0]).view(-1, 1)),dim=1)\n",
    "        if weight is not None:\n",
    "            return torch.matmul(X_1, weight)\n",
    "        else:\n",
    "            weight = self.lt.transform(self.particles, n_particles_second=True)\n",
    "            return torch.mean(torch.matmul(X_1, weight), dim=1)\n",
    "\n",
    "    def predict_net(self, X, inference=False):\n",
    "        '''\n",
    "            Use net to make predictions        \n",
    "            Args:\n",
    "                X (array_like): batch of data\n",
    "        '''\n",
    "        if self.task == 'net_reg':\n",
    "            if inference:\n",
    "                return torch.mean(self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape)), dim=0)\n",
    "            else:\n",
    "                return self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape))\n",
    "        elif self.task == 'net_class':\n",
    "            if inference:\n",
    "                return torch.mean(self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape)), dim=0)\n",
    "            else:\n",
    "                return self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape))\n",
    "\n",
    "    @deprecated # use update_latent / update_latent_regression / update_latent_net\n",
    "    def update(self, h_type, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term(h_type)\n",
    "        grad_log_term = self.calc_log_term()\n",
    "\n",
    "        ### Compute value of step in functiuonal space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "    def update_latent(self, h_type, kernel_type='rbf', p=None, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent()\n",
    "        ### Compute value of step in functiuonal space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "        \n",
    "    @deprecated # use one layer net for linear regression task \n",
    "    def update_latent_regression(self, h_type, kernel_type='rbf', p=None, X_batch=None, y_batch=None, train_size=None, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_regression(X_batch, y_batch, train_size)\n",
    "        ### Compute value of step in functiuonal space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "        \n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "    def update_latent_net(self, h_type, kernel_type='rbf', p=None, X_batch=None, y_batch=None, train_size=None, step_size=None):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.iter += 1\n",
    "        self.net.zero_grad()\n",
    "        self.data_distribution.zero_grad()\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent_net(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_net(X_batch, y_batch, train_size)\n",
    "\n",
    "        ### Compute value of step in functiuonal space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_to_parameters(vec, parameters):\n",
    "        pointer = 0\n",
    "        for param in parameters:\n",
    "            # The length of the parameter\n",
    "            num_param = param.numel()\n",
    "            # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "            param.data = vec[pointer:pointer + num_param].view_as(param).data\n",
    "            # Increment the pointer\n",
    "            pointer += num_param\n",
    "            \n",
    "    @staticmethod \n",
    "    def parameters_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    @staticmethod \n",
    "    def parameters_grad_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.grad.view(-1))\n",
    "        return torch.cat(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.8159, dtype=torch.float64) tensor(20.4882, dtype=torch.float64)\n",
      "tensor(88.1742, dtype=torch.float64) tensor(82.8522, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "### Bostor housing dataset for regression task\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "X_train, X_test, y_train, y_test = (torch.tensor(X_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(X_test, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_test, dtype=t_type, device=device))\n",
    "\n",
    "### Linear Regression baseline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(torch.nn.MSELoss()(torch.tensor(model.predict(X_test), dtype=t_type, device=device), y_test), \n",
    "      torch.nn.MSELoss()(torch.tensor(model.predict(X_train), dtype=t_type, device=device), y_train))\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.mean(y_train).expand(y_test.shape[0]), y_test), \n",
    "      torch.nn.MSELoss()(torch.mean(y_train).expand(y_train.shape[0]), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function calc_log_term.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated function calc_kernel_term.\n",
      "  \"\"\"\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated function update.\n",
      "  \n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:446: DeprecationWarning: Call to deprecated function calc_kernel_term.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:447: DeprecationWarning: Call to deprecated function calc_log_term.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated function calc_log_term_latent_regression.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:346: DeprecationWarning: Call to deprecated function predict_regression.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated function update_latent_regression.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:492: DeprecationWarning: Call to deprecated function calc_log_term_latent_regression.\n",
      "/home/m.nakhodnov/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/ipykernel_launcher.py:346: DeprecationWarning: Call to deprecated function predict_regression.\n"
     ]
    }
   ],
   "source": [
    "### Check all functions\n",
    "\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=13, n_particles=10, use_latent=False)\n",
    "dm.calc_log_term()\n",
    "dm.calc_kernel_term(0)\n",
    "dm.update(0)\n",
    "\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=5, n_particles=10, use_latent=True)\n",
    "dm.calc_log_term_latent()\n",
    "dm.calc_kernel_term_latent(0)\n",
    "dm.update_latent(0)\n",
    "\n",
    "ss = nn.Sequential(SteinLinear(13, 1, 10))#, nn.ReLU(), SteinLinear(5, 1, 10))\n",
    "dd = RegressionDistribution(n_particles=10)\n",
    "dm = DistributionMover(task='net_reg', n_hidden_dims=5, n_particles=10, use_latent=True, net=ss, data_distribution=dd)\n",
    "dm.calc_log_term_latent_net(X_train, y_train, X_train.shape[0])\n",
    "# dm.calc_kernel_term_latent_net(0)\n",
    "# dm.update_latent_net(0, X_train, y_train, X_train.shape[0])\n",
    "\n",
    "dm = DistributionMover(task='reg', n_particles=10, n_dims=X.shape[1] + 1, n_hidden_dims=5, use_latent=True)\n",
    "dm.calc_log_term_latent_regression(X_train, y_train, X_train.shape[0])\n",
    "dm.update_latent_regression(0, 'rbf', X_train, y_train, X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#net = nn.Sequential(SteinLinear(13, 5, 1), nn.Tanh(), SteinLinear(5, 1, 1))\n",
    "net = nn.Sequential(SteinLinear(13, 1, 1))\n",
    "data_distr = RegressionDistribution(1)\n",
    "dm = DistributionMover(task='net_reg', n_particles=1, use_latent=False, net=net, data_distribution=data_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: wrong matrix size, batch1: 32x784, batch2: 13x1 at /opt/conda/conda-bld/pytorch_1532502421238/work/aten/src/TH/generic/THTensorMath.cpp:2312",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-05ac72060ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_latent_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6851fd30943a>\u001b[0m in \u001b[0;36mupdate_latent_net\u001b[0;34m(self, h_type, kernel_type, X_batch, y_batch, train_size, step_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m### Compute additional terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_kernel_term_latent_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mgrad_log_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_log_term_latent_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m### Compute value of step in functiuonal space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6851fd30943a>\u001b[0m in \u001b[0;36mcalc_log_term_latent_net\u001b[0;34m(self, X, y, train_size)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m### get prediction for the batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;31m### compute log data term log p(D|w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_log_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6851fd30943a>\u001b[0m in \u001b[0;36mpredict_net\u001b[0;34m(self, X, inference)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'net_class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6.6_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f9994610c886>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     68\u001b[0m         '''\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m### NEED SOME OPTIMIZATION TO OMMIT .permute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: wrong matrix size, batch1: 32x784, batch2: 13x1 at /opt/conda/conda-bld/pytorch_1532502421238/work/aten/src/TH/generic/THTensorMath.cpp:2312"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step_size = 0.02\n",
    "    for _ in range(100000):\n",
    "        dm.update_latent_net(h_type=1, X_batch=X_train, y_batch=y_train, train_size=X_train.shape[0], step_size=step_size)\n",
    "        train_loss = torch.nn.MSELoss()(dm.predict_net(X_train, inference=True).view(-1), y_train)\n",
    "        test_loss = torch.nn.MSELoss()(dm.predict_net(X_test, inference=True).view(-1), y_test)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train): {1:.3f}\\t Empirical Loss(Test): {2:.3f}\\t Kernel factor: {3:.3f}'.format(\n",
    "                            _, train_loss, test_loss, dm.h))\n",
    "        if _ % 1200 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import MNIST dataset with class selection\n",
    "\n",
    "sys.path.insert(0, '/home/m.nakhodnov/Samsung-Tasks/Datasets/MyMNIST')\n",
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform)\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform)\n",
    "\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=32, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### MNIST with Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### MAP estimate for MNIST classification task\n",
    "net_map = nn.Sequential(SteinLinear(28 * 28, 200, 10), nn.ReLU(), SteinLinear(200, 200, 10), nn.ReLU(), SteinLinear(200, 10, 10))\n",
    "data_distr_map = ClassificationDistribution(10)\n",
    "dm_map = DistributionMover(task='net_class', n_particles=10, use_latent=False, net=net_map, data_distribution=data_distr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29... Empirical Loss(Train/Test): 9.072/17.357\t Accuracy(Train/Test): 0.971/0.960\t Kernel factor: 2158.1533"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step_size = 0.01\n",
    "    for epoch in range(30):\n",
    "        for train_epoch in range(30):\n",
    "            X, y = next(iter(dataloader_m_train))\n",
    "            X = X.double().view(X.shape[0], -1)\n",
    "            dm_map.update_latent_net(h_type=0, X_batch=X, y_batch=y, train_size=dataloader_m_train.__len__(), step_size=step_size)\n",
    "        \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(dm_map.predict_net(X_train, inference=True)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(dm_map.predict_net(X_train, inference=True), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 15.\n",
    "        train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(dm_map.predict_net(X_test, inference=True)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(dm_map.predict_net(X_test, inference=True), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 15.\n",
    "        test_acc /= (15. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            epoch, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "        if (epoch * 30) % 600 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(28 * 28, 200, 10), nn.ReLU(), SteinLinear(200, 200, 10), nn.ReLU(), SteinLinear(200, 10, 10))\n",
    "data_distr = ClassificationDistribution(10)\n",
    "dm = DistributionMover(task='net_class',\n",
    "                       n_particles=10,\n",
    "                       n_hidden_dims=2000,\n",
    "                       use_latent=True,\n",
    "                       net=net,\n",
    "                       precomputed_params=dm.parameters_to_vector(dm_map.paramerets_net()),\n",
    "                       data_distribution=data_distr\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24... Empirical Loss(Train/Test): 2.340/2.368\t Accuracy(Train/Test): 0.067/0.038\t Kernel factor: 0.000"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    step_size = 0.01\n",
    "    for epoch in range(30):\n",
    "        for train_epoch in range(30):\n",
    "            X, y = next(iter(dataloader_m_train))\n",
    "            X = X.double().view(X.shape[0], -1)\n",
    "            dm.update_latent_net(h_type=0, X_batch=X, y_batch=y, train_size=dataloader_m_train.__len__(), step_size=step_size)\n",
    "        \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(dm.predict_net(X_train, inference=True)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(dm.predict_net(X_train, inference=True), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 15.\n",
    "        train_acc /= (15. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(15):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(dm.predict_net(X_test, inference=True)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(dm.predict_net(X_test, inference=True), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 15.\n",
    "        test_acc /= (15. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            epoch, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "        if (epoch * 30) % 600 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X.shape[0], -1)\n",
    "y_pred = dm.predict_net(X_test, inference=True)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_projections(dm, use_real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### MNIST with FC neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_nn = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ").double()\n",
    "optim_nn = torch.optim.Adam(net_nn.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for _, (X, y) in enumerate(dataloader_m_train):\n",
    "        X = X.double().view(X.shape[0], -1)\n",
    "        \n",
    "        optim_nn.zero_grad()\n",
    "        train_loss = nn.CrossEntropyLoss()(net_nn(X), y)\n",
    "        train_loss.backward()\n",
    "        optim_nn.step()\n",
    "   \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_train)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(net_nn(X_train), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 30.\n",
    "        train_acc /= (30. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_test)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(net_nn(X_test), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 30.\n",
    "        test_acc /= (30. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            _, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X.shape[0], -1)\n",
    "y_pred = net_nn(X_test)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DistributionMover(n_particles=100, n_dims=2, n_hidden_dims=2, use_latent=False)\n",
    "\n",
    "#marginal_density = lambda x : (normal_density(mu=0., std=2., n=1)(x))\n",
    "marginal_density = lambda x : (0.3 * normal_density(mu=-2., std=1., n=1)(x) + 0.7 * normal_density(mu=2., std=1., n=1)(x))\n",
    "\n",
    "pdf = []\n",
    "for _ in np.linspace(-10, 10, 1000): \n",
    "    _ = torch.tensor(_, dtype=t_type, device=device)\n",
    "    pdf.append(marginal_density(_))\n",
    "    \n",
    "from pynverse import inversefunc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "cum_density = interp1d(np.linspace(-20, 20, 25), [quad(marginal_density, -20, x)[0] for x in np.linspace(-20, 20, 25)])\n",
    "inv_cum_density = inversefunc(cum_density)\n",
    "real_samples = [inv_cum_density(np.random.random()) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = quad(lambda x : marginal_density(x) * x, -20, 20)[0]\n",
    "var = quad(lambda x : marginal_density(x) * (x - mean) ** 2, -20, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10, 10, 1000), pdf)\n",
    "plt.plot(real_samples, np.zeros_like(real_samples))\n",
    "sns.kdeplot(real_samples, kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_projections(dm=None, use_real=True, kernel='tri'):\n",
    "    N_plots = None\n",
    "    scale_factor = None\n",
    "    \n",
    "    if use_real:\n",
    "        if not dm.use_latent:\n",
    "            return\n",
    "        N_plots = dm.lt.A.shape[0]\n",
    "    else:\n",
    "        N_plots = dm.particles.shape[0]\n",
    "    if N_plots > 6:\n",
    "        scale_factor = 15\n",
    "    else:\n",
    "        scale_factor = 5\n",
    "        \n",
    "    N_plots = min(N_plots, 20)\n",
    "        \n",
    "    plt.figure(figsize=(3 * scale_factor, (N_plots // 3 + 1) * scale_factor))\n",
    "    \n",
    "    for idx in range(N_plots):\n",
    "        slice_dim = idx\n",
    "        \n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        \n",
    "        particles = None\n",
    "        if use_real:\n",
    "            particles = dm.lt.transform(dm.particles, n_particles_second=True).t()[:, slice_dim]\n",
    "        else:\n",
    "            particles = dm.particles.t()[:, slice_dim]\n",
    "        \n",
    "        plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "        plt.plot(particles.data.cpu().numpy(), torch.zeros_like(particles).data.cpu().numpy(), 'ro')\n",
    "        sns.kdeplot(particles.data.cpu().numpy(), \n",
    "                    kernel=kernel, color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projections(dm, use_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projections(dm, use_real=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=1e1, kernel_type='rat', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_projections(dm, use_real=True, kernel='tri')\n",
    "            plot_projections(dm, use_real=False, kernel='tri')\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.mean(dm.lt.transform(dm.particles, n_particles_second=True)),\n",
    " torch.mean(torch.std(dm.lt.transform(dm.particles, n_particles_second=True), dim=1) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare implementations of Stein Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class SVGD():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def svgd_kernel(self, theta, h = -1):\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = h / np.log(theta.shape[0]+1)\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = 2. * dxkxy / (h)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    " \n",
    "    def update(self, x0, lnprob, n_iter = 1000, stepsize = 1e-3, bandwidth = -1, alpha = 0.9, debug = False):\n",
    "        # Check input\n",
    "        if x0 is None or lnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        theta = np.copy(x0) \n",
    "        \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter+1) % 1000 == 0:\n",
    "                print( 'iter ' + str(iter+1) )\n",
    "            \n",
    "            lnpgrad = lnprob(theta)\n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(theta, h = bandwidth)  \n",
    "            grad_theta = (np.matmul(kxy, lnpgrad) + dxkxy) / x0.shape[0]  \n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + grad_theta ** 2\n",
    "            else:\n",
    "                historical_grad = alpha * historical_grad + (1 - alpha) * (grad_theta ** 2)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            theta = theta + stepsize * adj_grad \n",
    "            \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_ex = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lnprob(particles):\n",
    "    grad_log_term = torch.zeros([particles.shape[0], particles.shape[1]], dtype=t_type)\n",
    "    if use_cuda:\n",
    "        grad_log_term = grad_log_term.cuda()\n",
    "        \n",
    "    for idx in range(100):\n",
    "        particle = torch.tensor(particles[idx:idx+1], dtype=t_type, requires_grad=True)\n",
    "        log_term = torch.log(dm_ex.target_density(particle.t()))\n",
    "        grad_log_term[idx] = torch.autograd.grad(log_term, particle,\n",
    "                                                     only_inputs=True, retain_graph=False, \n",
    "                                                     create_graph=False, allow_unused=False)[0]\n",
    "    return grad_log_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = dm_ex.particles.data.clone().detach().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare kernel implementation\n",
    "\n",
    "ker, grad_ker = dm_ex.calc_kernel_term_latent(10.)\n",
    "print(dm_ex.h)\n",
    "grad_ker_sum = torch.sum(grad_ker, dim=1)\n",
    "\n",
    "ker_l, grad_ker_sum_l = dm_svgd.svgd_kernel(result, h=10.)\n",
    "\n",
    "print((ker - torch.tensor(ker_l)).norm())\n",
    "print((grad_ker_sum - torch.tensor(grad_ker_sum_l).t()).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare log_term implementation\n",
    "\n",
    "(lnprob(result) - dm_ex.calc_log_term_latent().t()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(201): \n",
    "    try:\n",
    "        result = dm_svgd.update(result, lnprob, stepsize=1e-2, bandwidth=-1, n_iter=20)\n",
    "        result = torch.tensor(result, dtype=t_type)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(result.data.cpu().numpy(), torch.zeros_like(result).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(result.data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(2000): \n",
    "    try:\n",
    "        dm_ex.update_latent(h_type=0)\n",
    "\n",
    "        if _ % 30 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm_ex.h))\n",
    "            \n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(dm_ex.particles.t().data.cpu().numpy(), torch.zeros_like(dm_ex.particles.t()).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(dm_ex.particles.t().data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(result - dm_ex.particles.t()).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"Stein-Variational-Gradient-Descent/python/\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.matlib as nm\n",
    "from svgd import SVGD\n",
    "\n",
    "r'''\n",
    "    Example of Bayesian Logistic Regression (the same setting as Gershman et al. 2012):\n",
    "    The observed data D = {X, y} consist of N binary class labels, \n",
    "    y_t \\in {-1,+1}, and d covariates for each datapoint, X_t \\in R^d.\n",
    "    The hidden variables \\theta = {w, \\alpha} consist of d regression coefficients w_k \\in R,\n",
    "    and a precision parameter \\alpha \\in R_+. We assume the following model:\n",
    "        p(\\alpha) = Gamma(\\alpha; a, b)\n",
    "        p(w_k | a) = N(w_k; 0, \\alpha^-1)\n",
    "        p(y_t = 1| x_t, w) = 1 / (1+exp(-w^T x_t))\n",
    "'''\n",
    "class BayesianLR:\n",
    "    def __init__(self, X, Y, batchsize=100, a0=1, b0=0.01):\n",
    "        self.X, self.Y = X, Y\n",
    "        # TODO. Y in \\in{+1, -1}\n",
    "        self.batchsize = min(batchsize, X.shape[0])\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.permutation = np.random.permutation(self.N)\n",
    "        self.iter = 0\n",
    "    \n",
    "        \n",
    "    def dlnprob(self, theta):\n",
    "        \n",
    "        if self.batchsize > 0:\n",
    "            batch = [ i % self.N for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize) ]\n",
    "            ridx = self.permutation[batch]\n",
    "            self.iter += 1\n",
    "        else:\n",
    "            ridx = np.random.permutation(self.X.shape[0])\n",
    "            \n",
    "        Xs = self.X[ridx, :]\n",
    "        Ys = self.Y[ridx]\n",
    "        \n",
    "        w = theta[:, :-1]  # logistic weights\n",
    "        alpha = np.exp(theta[:, -1])  # the last column is logalpha\n",
    "        d = w.shape[1]\n",
    "        \n",
    "        wt = np.multiply((alpha / 2), np.sum(w ** 2, axis=1))\n",
    "        \n",
    "        coff = np.matmul(Xs, w.T)\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-1 * coff))\n",
    "        \n",
    "        dw_data = np.matmul(((nm.repmat(np.vstack(Ys), 1, theta.shape[0]) + 1) / 2.0 - y_hat).T, Xs)  # Y \\in {-1,1}\n",
    "        dw_prior = -np.multiply(nm.repmat(np.vstack(alpha), 1, d) , w)\n",
    "        dw = dw_data * 1.0 * self.X.shape[0] / Xs.shape[0] + dw_prior  # re-scale\n",
    "        \n",
    "        dalpha = d / 2.0 - wt + (self.a0 - 1) - self.b0 * alpha + 1  # the last term is the jacobian term\n",
    "        \n",
    "        return np.hstack([dw, np.vstack(dalpha)])  # % first order derivative \n",
    "    \n",
    "    def evaluation(self, theta, X_test, y_test):\n",
    "        theta = theta[:, :-1]\n",
    "        M, n_test = theta.shape[0], len(y_test)\n",
    "\n",
    "        prob = np.zeros([n_test, M])\n",
    "        for t in range(M):\n",
    "            coff = np.multiply(y_test, np.sum(-1 * np.multiply(nm.repmat(theta[t, :], n_test, 1), X_test), axis=1))\n",
    "            prob[:, t] = np.divide(np.ones(n_test), (1 + np.exp(coff)))\n",
    "        \n",
    "        prob = np.mean(prob, axis=1)\n",
    "        acc = np.mean(prob > 0.5)\n",
    "        llh = np.mean(np.log(prob))\n",
    "        return [acc, llh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = X_test.shape[0] + X_train.shape[0]\n",
    "X_input = np.hstack([X, np.ones([N, 1])])\n",
    "y_input = y\n",
    "d = X_input.shape[1]\n",
    "D = d + 1\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.2, random_state=42)\n",
    "    \n",
    "a0, b0 = 1, 0.01 #hyper-parameters\n",
    "model = BayesianLR(X_train, y_train, 100, a0, b0) # batchsize = 100\n",
    "    \n",
    "# initialization\n",
    "M = 100  # number of particles\n",
    "theta0 = np.zeros([M, D]);\n",
    "alpha0 = np.random.gamma(a0, b0, M); \n",
    "for i in range(M):\n",
    "    theta0[i, :] = np.hstack([np.random.normal(0, np.sqrt(1 / alpha0[i]), d), np.log(alpha0[i])])\n",
    "    \n",
    "theta = SVGD().update(x0=theta0, lnprob=model.dlnprob, bandwidth=-1, n_iter=6000, stepsize=0.05, alpha=0.9, debug=True)\n",
    "    \n",
    "print('[accuracy, log-likelihood]')\n",
    "print(model.evaluation(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Metropolis–Hastings algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MHAlgorithm():\n",
    "    def __init__(self, n_dims): \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                           0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "        self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "        \n",
    "#         self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "#         self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "    \n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_dims, 1],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., -1.)\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "    def generate_next(self):\n",
    "        previous = self.particles[:, -1].unsqueeze(1)\n",
    "#         self.additional_distribution = normal_density(self.n_dims, previous, 1, n_particles_second=True)\n",
    "#         candidate = self.additional_distribution.get_sample()\n",
    "        \n",
    "#         alpha = (self.target_density(candidate) * self.additional_distribution.unnormed_density(previous) / \n",
    "#                 (self.target_density(previous) * self.additional_distribution.unnormed_density(candidate)))\n",
    "        candidate = previous + torch.zeros([self.n_dims, 1], device=device, dtype=t_type).uniform_(-1., 1.)\n",
    "        alpha = self.target_density(candidate) / self.target_density(previous)\n",
    "        \n",
    "#         print(self.target_density(candidate)[0].data.numpy(), self.additional_distribution.unnormed_density(previous)[0].data.numpy(),\n",
    "#               self.target_density(previous)[0].data.numpy(), self.additional_distribution.unnormed_density(candidate)[0].data.numpy())\n",
    "        \n",
    "        if alpha > self.one:\n",
    "            self.particles = torch.cat([self.particles, candidate], dim=1)\n",
    "        else:\n",
    "            random = torch.bernoulli(alpha)\n",
    "            if random:\n",
    "                self.particles = torch.cat([self.particles, candidate], dim=1)                \n",
    "            else:\n",
    "                self.particles = torch.cat([self.particles, previous], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mh = MHAlgorithm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    mh.generate_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(mh.particles.t().data.cpu().numpy()[:,1], \n",
    "            kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(mh.particles.t().data.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import time\n",
    "\n",
    "mu,sig,N = 0,1,1000000\n",
    "pts = []\n",
    "\n",
    "def q(x):\n",
    "#     return (1/(math.sqrt(2*math.pi*sig**2)))*(math.e**(-((x-mu)**2)/(2*sig**2)))\n",
    "#     return normal_density(1, 0., 2., n_particles_second=True).unnormed_density(x)\n",
    "    return (0.3 * normal_density(1, -2., 1., n_particles_second=True).unnormed_density(x) +\n",
    "            0.7 * normal_density(1, 2., 1., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "def metropolis(N):\n",
    "    r = np.zeros(1)\n",
    "    p = q(r[0])\n",
    "    pts = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        rn = r + np.random.uniform(-1,1)\n",
    "        pn = q(rn[0])\n",
    "        if pn >= p:\n",
    "            p = pn\n",
    "            r = rn\n",
    "        else:\n",
    "            u = np.random.rand()\n",
    "            if u < pn/p:\n",
    "                p = pn\n",
    "                r = rn\n",
    "        pts.append(r)\n",
    "    \n",
    "    pts = np.array(pts)\n",
    "    return pts\n",
    "    \n",
    "def hist_plot(array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1,)\n",
    "    ax.hist(array, bins=1000)    \n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(metropolis(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gg =  lambda : (normal_density(1, -60., 20., n_particles_second=True).get_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot([gg()[0,0] for _ in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orth\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "device = None\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    use_cuda = True\n",
    "\n",
    "# Ignore cuda\n",
    "# use_cuda = False\n",
    "# device = None\n",
    "\n",
    "# Set DoubleTensor as a base type for computations\n",
    "t_type = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import functools\n",
    "\n",
    "def deprecated(func):\n",
    "    \"\"\"This is a decorator which can be used to mark functions\n",
    "    as deprecated. It will result in a warning being emitted\n",
    "    when the function is used.\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def new_func(*args, **kwargs):\n",
    "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n",
    "        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n",
    "                      category=DeprecationWarning,\n",
    "                      stacklevel=2)\n",
    "        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n",
    "        return func(*args, **kwargs)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def print_plots(data, axis, labels, file_name=None):\n",
    "    N_plots = len(data)\n",
    "    plt.figure(figsize=(30, (N_plots // 3 + 1) * 10))\n",
    "\n",
    "    for idx in range(len(data)):\n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        for jdx in range(len(data[idx])):\n",
    "            plt.plot(data[idx][jdx], label=labels[idx][jdx])\n",
    "        plt.xlabel(axis[idx][0], fontsize=16)\n",
    "        plt.ylabel(axis[idx][1], fontsize=16)\n",
    "        plt.legend(loc=0, fontsize=16)\n",
    "    if file_name is not None:\n",
    "        plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_projections(dm=None, use_real=True, kernel='tri', pdf=None, N_plots_max=10):\n",
    "    \"\"\"\n",
    "        Plot marginal kernel density estimation\n",
    "    Args:\n",
    "        dm (DistributionMover): class containing particles which define distribution\n",
    "        use_real (bool): If set to True then apply transformation dm.lt.transform before creating plot\n",
    "        kernel (str): Kernel type for kernel density estimation\n",
    "        pdf (array_like, None): Samples from target distribution\n",
    "        N_plots_max (int): Maximum number of plots\n",
    "    \"\"\"\n",
    "    N_plots = None\n",
    "    scale_factor = None\n",
    "    \n",
    "    if use_real:\n",
    "        if not dm.use_latent:\n",
    "            return\n",
    "        N_plots = dm.lt.A.shape[0]\n",
    "    else:\n",
    "        N_plots = dm.particles.shape[0]\n",
    "    if N_plots > 6:\n",
    "        scale_factor = 15\n",
    "    else:\n",
    "        scale_factor = 5\n",
    "        \n",
    "    N_plots = min(N_plots, N_plots_max)\n",
    "        \n",
    "    plt.figure(figsize=(3 * scale_factor, (N_plots // 3 + 1) * scale_factor))\n",
    "    \n",
    "    for idx in range(N_plots):\n",
    "        slice_dim = idx\n",
    "        \n",
    "        plt.subplot(N_plots // 3 + 1, 3, idx + 1)\n",
    "        \n",
    "        particles = None\n",
    "        if use_real:\n",
    "            particles = dm.lt.transform(dm.particles, n_particles_second=True).t()[:, slice_dim]\n",
    "        else:\n",
    "            particles = dm.particles.t()[:, slice_dim]\n",
    "        \n",
    "        if pdf is not None:\n",
    "            plt.plot(np.linspace(-10, 10, len(pdf), dtype=np.float64), pdf)\n",
    "        plt.plot(particles.data.cpu().numpy(), torch.zeros_like(particles).data.cpu().numpy(), 'ro')\n",
    "        sns.kdeplot(particles.data.cpu().numpy(), \n",
    "                    kernel=kernel, color='darkblue', linewidth=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_condition_distribution(dm, n_samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dm (DistributionMover): object contains unconditioned density, linear manifold and particles\n",
    "        n_samples (int): number of samples\n",
    "    Return:\n",
    "        (points, weight)\n",
    "    \"\"\"\n",
    "    if not dm.use_latent:\n",
    "        return\n",
    "    \n",
    "    points = torch.zeros([dm.n_hidden_dims, n_samples], dtype=t_type, device=device).uniform_(-10, 10)\n",
    "    weight = dm.real_target_density(dm.lt.transform(points, n_particles_second=True))\n",
    "    points = points.view(-1)\n",
    "    \n",
    "    plt.hist(points.data.cpu().numpy(), weights=weight.data.cpu().numpy(), density=True, bins=100, alpha=0.5, label='True conditional density')\n",
    "    plt.plot(dm.particles.data.cpu().numpy(), torch.zeros_like(dm.particles).data.cpu().numpy(), 'ro')\n",
    "    sns.kdeplot(dm.particles[0, :].data.cpu().numpy(), \n",
    "                kernel='tri', color='darkblue', linewidth=4, label='Approximated conditional density')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_diffs(x, y, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: x is a dxN matrix\n",
    "                   y is an optional dxM matrix\n",
    "            Output: diffs is a dxNxM matrix where diffs[i,j] is the subtraction between x[:,i] and y[:,j]\n",
    "            i.e. diffs[i,j] = x[:,i] - y[:,j]\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: x is a Nxd matrix\n",
    "                   y is an optional Mxd matrix\n",
    "            Output: diffs is a NxMxd matrix where diffs[i,j] is the subtraction between x[i,:] and y[j,:]\n",
    "            i.e. diffs[i,j] = x[i,:]-y[j,:]\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return x[:,:,np.newaxis] - y[:,np.newaxis,:]        \n",
    "    return x[:,np.newaxis,:] - y[np.newaxis,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise_dists(diffs=None, n_particles_second=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        if n_particles_second == True:\n",
    "            Input: diffs is a dxNxM matrix where diffs[i,j] = x[:,i] - y[:,j]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[:,i] - y[:,j]||\n",
    "\n",
    "        if n_particles_second == False:\n",
    "            Input: diffs is a NxMxd matrix where diffs[i,j] = x[i,:] - y[j, :]\n",
    "            Output: dist is a NxM matrix where dist[i,j] is the square norm of diffs[i,j]\n",
    "            i.e. dist[i,j] = ||x[i,:]-y[j,:]||\n",
    "    \"\"\"\n",
    "    if n_particles_second:\n",
    "        return torch.norm(diffs, dim=0)\n",
    "    return torch.norm(diffs, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class normal_density():\n",
    "    \"\"\"\n",
    "        Multinomial normal density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, mu=0., std=1., n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            mu (float, array_like): mean of distribution\n",
    "                Default: 0.\n",
    "                if mu is float - use same mean across all dimensions\n",
    "                if mu is 1D array_like - use different mean for each dimension but same for each particles dimension\n",
    "                if mu is 2D array_like - use different mean for each dimension\n",
    "            std (float, array_like): std of distribution\n",
    "                Default: 1.\n",
    "                if std is float - use same std across all dimensions\n",
    "                if std is 1D array_like - use different std for each dimension but same for each particles dimension\n",
    "                if std is 2D array_like - use different std for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis\n",
    "                Input will be reduced along all axis exclude particle axis\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.std = std\n",
    "        \n",
    "        if isinstance(self.mu, float):\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.mu.shape) == 1:\n",
    "            if self.mu.shape[0] == 1:\n",
    "                self.mu = self.mu.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.mu = torch.tensor(self.mu, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.mu.shape) == 2:\n",
    "            self.mu = torch.tensor(self.mu, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if isinstance(self.std, float):\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.std.shape) == 1:\n",
    "            if len(self.std) == 1:\n",
    "                self.std = self.std.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.std = torch.tensor(self.std, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.std = torch.tensor(self.std, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.zero = torch.tensor(0., dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.pi = torch.tensor(math.pi, dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "    def __call__(self, x, n_axis=None):        \n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.pow(self.two * self.pi, -self.n / self.two) / \n",
    "                torch.prod(self.std, dim=n_axis) * \n",
    "                torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):      \n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return torch.exp(-self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std , self.two), dim=n_axis))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):  \n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (-self.n / self.two * torch.log(self.two * self.pi) + \n",
    "                torch.sum(torch.log(self.std), dim=n_axis) -\n",
    "                self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return -self.one / self.two * torch.sum(torch.pow((x - self.mu) / self.std, self.two), dim=n_axis)\n",
    "    \n",
    "    def get_sample(self):\n",
    "        \"\"\"\n",
    "            Sample from normal distribution\n",
    "        \"\"\"\n",
    "        sample = torch.normal(self.mu, self.std)\n",
    "        if self.n_particles_second:\n",
    "            return sample.view(-1, 1)\n",
    "        else:\n",
    "            return sample.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class gamma_density():\n",
    "    \"\"\"\n",
    "        Multinomial gamma density for independent random variables. \n",
    "    \"\"\"\n",
    "    def __init__(self, n=1, alpha=1, betta=1, n_particles_second=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): number of dimensions of multinomial normal distribution\n",
    "                Default: 1\n",
    "            alpha (float, array_like): shape of distribution\n",
    "                Default: 1.\n",
    "                if alpha is float - use same shape across all dimensions\n",
    "                if alpha is 1D array_like - use different shape for each dimension but same for each particles dimension\n",
    "                if alpha is 2D array_like - use different shape for each dimension\n",
    "            betta (float, array_like): rate of distribution\n",
    "                Default: 1.\n",
    "                if betta is float - use same rate across all dimensions\n",
    "                if betta is 1D array_like - use different rate for each dimension but same for each particles dimension\n",
    "                if betta is 2D array_like - use different rate for each dimension\n",
    "            n_particles_second (bool): specify type of input\n",
    "                Default: True\n",
    "                if n_particles_second == True - input must has shape [n, n_particles]\n",
    "                if n_particles_second == False - input must has shape [n_particles, n]\n",
    "                Therefore the same mu and std are applied along particles axis  \n",
    "        \"\"\"\n",
    "        \n",
    "        self.n = torch.tensor(n, dtype=t_type, device=device)\n",
    "        self.n_particles_second = n_particles_second\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.betta = betta\n",
    "        \n",
    "        if isinstance(self.alpha, float):\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).expand(n)      \n",
    "        if len(self.alpha.shape) == 1:\n",
    "            if self.alpha.shape[0] == 1:\n",
    "                self.alpha = self.alpha.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.alpha.shape) == 2:\n",
    "            self.alpha = torch.tensor(self.alpha, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        if isinstance(self.betta, float):\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device).expand(n)        \n",
    "        if len(self.betta.shape) == 1:\n",
    "            if len(self.betta) == 1:\n",
    "                self.betta = self.betta.view(1).expand(n)\n",
    "            if self.n_particles_second:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(n, 1)\n",
    "            else:\n",
    "                self.betta = torch.tensor(self.betta, dtype=t_type, device=device).view(1, n)\n",
    "        elif len(self.std.shape) == 2:\n",
    "            self.betta = torch.tensor(self.betta, dtype=t_type, device=device)\n",
    "        else:\n",
    "            raise RuntimeError\n",
    "            \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        \n",
    "        ### specify axis to reduce\n",
    "        ### if n_particles_second == True - n_axis == 0\n",
    "        ### if n_particles_second == False - n_axis == 1\n",
    "        self.n_axis = 1 - int(self.n_particles_second)\n",
    "        \n",
    "        ### log Г(alpha)\n",
    "        self.lgamma = torch.lgamma(self.alpha)\n",
    "        ### Г(alpha)\n",
    "        self.gamma = torch.exp(self.lgamma)\n",
    "\n",
    "    def __call__(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(self.betta, self.alpha) / self.gamma * torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.prod(torch.pow(x, self.alpha - self.one), dim=n_axis) * \n",
    "                torch.exp(-torch.sum(self.betta * x, dim=n_axis)))\n",
    "    \n",
    "    def log_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density(self, x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in given point\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * torch.log(x), dim=n_axis) -\n",
    "                torch.sum(self.betta * x, dim=n_axis))\n",
    "                \n",
    "    def log_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum(self.alpha * torch.log(self.betta) - self.lgamma + (self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))\n",
    "                \n",
    "    def log_unnormed_density_log_x(self, log_x, n_axis=None):\n",
    "        \"\"\"\n",
    "            Evaluate log unnormed density in point log(x)\n",
    "        Args:\n",
    "            x (torch.tensor): tensor which defines point where log unnormed density is evaluated\n",
    "            n_axis (int): specify axis to reduce\n",
    "                Default: \n",
    "                    if n_particles_second == True - n_axis == 0\n",
    "                    if n_particles_second == False - n_axis == 1\n",
    "        \"\"\"\n",
    "        n_axis = self.n_axis if n_axis is None else n_axis\n",
    "        return (torch.sum((self.alpha - self.one) * log_x, dim=n_axis) -\n",
    "                torch.sum(self.betta * torch.exp(log_x), dim=n_axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0,
     11,
     33,
     36,
     47,
     54,
     73,
     107
    ]
   },
   "outputs": [],
   "source": [
    "class SteinLinear(nn.Module):\n",
    "    \"\"\"\n",
    "        Custom full connected layer for Stein Gradient Neural Networks\n",
    "        Transformation: y = xA + b\n",
    "        Parameters prior: \n",
    "            1 - p(w, a) = p(w|a)p(a) = П p(w_i|a_i)p(a_i)\n",
    "                p(w_i|a_i) = N(w_i|0, a_i^(-1)); p(a_i) = G(1e-4, 1e-4)\n",
    "                          \n",
    "            2 - p(w) = П p(w_i)\n",
    "                p(w_i) = N(w_i|0, alpha^(-1))\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, n_particles=1, use_bias=True, use_var_prior=True, alpha=1e-2):\n",
    "        super(SteinLinear, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features (int): size of each input sample\n",
    "            out_features (int): size of each output sample\n",
    "            n_particles (int): number of particles\n",
    "            use_bias (bool): If set to False, the layer will not learn an additive bias.\n",
    "                Default: True\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of weight variance\n",
    "                Default: True\n",
    "            alpha (float): If use_var_prior == False - defines weight variance\n",
    "                Default: 1e-2\n",
    "        \"\"\"\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_particles = n_particles\n",
    "        self.use_bias = use_bias\n",
    "        self.use_var_prior = use_var_prior\n",
    "\n",
    "        ### if alpha is None use GLOROT prior\n",
    "        if alpha is None:\n",
    "            self.alpha_weight = (self.in_features + self.out_features) / 2.\n",
    "            self.alpha_bias = (self.out_features) / 2.\n",
    "        else:\n",
    "            self.alpha_weight = alpha\n",
    "            self.alpha_bias = alpha\n",
    "    \n",
    "        \n",
    "        self.weight = torch.nn.Parameter(torch.zeros([in_features, out_features, n_particles], dtype=t_type, device=device))\n",
    "        if self.use_var_prior:\n",
    "            self.log_weight_alpha = torch.nn.Parameter(torch.zeros([in_features * out_features, n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_weight_alpha = torch.tensor([math.log(self.alpha_weight)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros([1, out_features, n_particles], dtype=t_type, device=device))\n",
    "            if self.use_var_prior:\n",
    "                self.log_bias_alpha = torch.nn.Parameter(torch.zeros([out_features, n_particles], dtype=t_type, device=device))\n",
    "            else:\n",
    "                self.log_bias_alpha = torch.tensor([math.log(self.alpha_bias)], dtype=t_type, device=device, requires_grad=False)\n",
    "        \n",
    "        if self.use_var_prior:\n",
    "            ### define prior on alpha p(a) = G(1e-4, 1e-4)\n",
    "            self.weight_alpha_log_prior = lambda x: (gamma_density(n=self.log_weight_alpha.shape[0],\n",
    "                                                                   alpha=1e-4,\n",
    "                                                                   betta=1e-4,\n",
    "                                                                   n_particles_second=True\n",
    "                                                                  ).log_unnormed_density_log_x(x))\n",
    "            if self.use_bias:\n",
    "                self.bias_alpha_log_prior = lambda x: (gamma_density(n=self.log_bias_alpha.shape[0],\n",
    "                                                                     alpha=1e-4,\n",
    "                                                                     betta=1e-4,\n",
    "                                                                     n_particles_second=True\n",
    "                                                                    ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    ### useless function - all initialization defined in DistributionMover class\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.out_features)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.log_weight_alpha.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            self.log_bias_alpha.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Apply transformation: X_out[i, :, :] = X_in[i, :, :] * W + b[i]\n",
    "        Args:\n",
    "            X (torch.tensor): tensor \n",
    "        Shape:\n",
    "            Input: [n_particles, batch_size, in_features]\n",
    "            Output: [n_particles, batch_size, out_features]\n",
    "        \"\"\"\n",
    "        ### NEED SOME OPTIMIZATION TO OMMIT .permute\n",
    "        if self.use_bias:\n",
    "            return torch.bmm(X, self.weight.permute(2, 0, 1)) + self.bias.permute(2, 0, 1)\n",
    "        return torch.bmm(X, self.weight.permute(2, 0, 1))\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())\n",
    "        \n",
    "    def calc_log_prior(self):\n",
    "        \"\"\"\n",
    "            Evaluate log prior of trainable parameters\n",
    "        log p(w,a) = log p(w|a) + log p(a)\n",
    "        \"\"\"\n",
    "        ### define prior on weight p(w|a) = N(0, 1 / alpha)\n",
    "        weight_log_prior = lambda x: (normal_density(n=self.weight.numel() // self.n_particles,\n",
    "                                                     mu=0.,\n",
    "                                                     std=self.one / torch.exp(self.log_weight_alpha),\n",
    "                                                     n_particles_second=True\n",
    "                                                    ).log_unnormed_density(x))\n",
    "\n",
    "        bias_log_prior = lambda x: (normal_density(self.bias.numel() // self.n_particles,\n",
    "                                                   mu=0.,\n",
    "                                                   std=self.one / torch.exp(self.log_bias_alpha),\n",
    "                                                   n_particles_second=True\n",
    "                                                  ).log_unnormed_density(x))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            if self.use_var_prior:\n",
    "                return (weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha) +\n",
    "                        bias_log_prior(self.bias.view(-1, self.n_particles)) + self.bias_alpha_log_prior(self.log_bias_alpha))   \n",
    "            return (weight_log_prior(self.weight.view(-1, self.n_particles)) +\n",
    "                    bias_log_prior(self.bias.view(-1, self.n_particles)))     \n",
    "        \n",
    "        if self.use_var_prior:\n",
    "            return weight_log_prior(self.weight.view(-1, self.n_particles)) + self.weight_alpha_log_prior(self.log_weight_alpha)\n",
    "        return weight_log_prior(self.weight.view(-1, self.n_particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0,
     4,
     47,
     58,
     69,
     84,
     109
    ]
   },
   "outputs": [],
   "source": [
    "class LinearTransform():\n",
    "    \"\"\"\n",
    "        Class for various linear transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, n_dims, n_hidden_dims, use_identity=False, normalize=False, A=None, theta_0=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_dims (int): dimension of the space\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_identity (bool): If set to True, use 'eye' matrix for transformations\n",
    "            normalize (bool): If set to True, columns of the transformation matrix is an orthonormal basis \n",
    "            A (2D array_like, None): Initial value for transformation matrix\n",
    "                If None then matrix will be sampled from uniform distribution and then orthonormate\n",
    "                Default: None\n",
    "            theta_0 (1D array_like, None): Initial value for bias\n",
    "                If None then matrix will be sampled from uniform distribution\n",
    "                Default: None\n",
    "        \"\"\"\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_identity = use_identity\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if self.use_identity:\n",
    "            return\n",
    "        \n",
    "        self.A = A\n",
    "        self.theta_0 = theta_0\n",
    "        \n",
    "        if self.A is None:\n",
    "            self.A = torch.zeros([self.n_dims, self.n_hidden_dims], dtype=t_type, device=device)\n",
    "            self.A.uniform_(-1., 1.)\n",
    "            if self.normalize:\n",
    "                ### normalize columns of matrix A\n",
    "                self.A = torch.tensor(orth(self.A.data.cpu().numpy()), dtype=t_type, device=device)\n",
    "                    \n",
    "        if self.theta_0 is None:\n",
    "            self.theta_0 = torch.zeros([self.n_dims, 1], dtype=t_type, device=device)\n",
    "            self.theta_0.uniform_(-1.,1.)\n",
    "        \n",
    "        ### A^(t)A\n",
    "        self.AtA = torch.matmul(self.A.t(), self.A)\n",
    "        ### (A^(t)A)^(-1)\n",
    "        self.AtA_1 = torch.inverse(self.AtA)\n",
    "        ### (A^(t)A)^(-1)A^(t)\n",
    "        self.inverse_base = torch.matmul(self.AtA_1, self.A.t())\n",
    "        \n",
    "    def transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Transform thetas as follows: \n",
    "                theta = Atheta` + theta_0\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.A, theta) + self.theta_0\n",
    "        return (torch.matmul(self.A, theta.t()) + self.theta_0).t()\n",
    "    \n",
    "    def inverse_transform(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Apply inverse transformation: \n",
    "                theta` = (A^(t)A)^(-1)A^(t)(theta - theta_0)\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        if n_particles_second:\n",
    "            return torch.matmul(self.inverse_base, theta - self.theta_0)\n",
    "        return torch.matmul(self.inverse_base, theta.t() - self.theta_0).t()\n",
    "    \n",
    "    def project_inverse(self, theta, n_particles_second=True):\n",
    "        \"\"\"\n",
    "            Project and then apply inverse transform to theta - theta_0:\n",
    "                theta_s_p_i = T^(-1)P(theta - theta_0)= (A^(t)A)^(-1)A^(t)theta\n",
    "        \"\"\"\n",
    "        if self.use_identity:\n",
    "            return theta\n",
    "        ### This optimization severely reduces performance!!!!\n",
    "        ### use solver trick: theta_s_p_i : A^(t)Atheta_s_p_i = A^(t)theta\n",
    "        if n_particles_second:\n",
    "            # return torch.gesv(torch.matmul(self.A.t(), theta), self.AtA)[0]\n",
    "            return torch.matmul(self.inverse_base, theta)\n",
    "        # return torch.gesv(torch.matmul(self.A.t(), theta.t()), self.AtA)[0].t()\n",
    "        return torch.matmul(theta, self.inverse_base.t())\n",
    "        \n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        Example::\n",
    "            >>> module.state_dict().keys()\n",
    "            ['bias', 'weight']\n",
    "        \"\"\"\n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=1)\n",
    "        \n",
    "        destination[prefix + 'A'] = self.A if keep_vars else self.A.data\n",
    "        destination[prefix + 'theta_0'] = self.theta_0 if keep_vars else self.theta_0.data\n",
    "        destination[prefix + 'n_dims'] = self.n_dims\n",
    "        destination[prefix + 'n_hidden_dims'] = self.n_hidden_dims\n",
    "        destination[prefix + 'use_identity'] = self.use_identity\n",
    "        destination[prefix + 'normalize'] = self.normalize\n",
    "        \n",
    "        return destination\n",
    "    \n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        self.A.copy_(state_dict[prefix + 'A'])\n",
    "        self.theta_0.copy_(state_dict[prefix + 'theta_0'])\n",
    "        self.__init__(state_dict[prefix + 'n_dims'],\n",
    "                      state_dict[prefix + 'n_hidden_dims'],\n",
    "                      state_dict[prefix + 'use_identity'],\n",
    "                      state_dict[prefix + 'normalize'],\n",
    "                      self.A, self.theta_0\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0,
     4,
     20,
     22,
     25,
     36,
     74,
     77
    ]
   },
   "outputs": [],
   "source": [
    "class RegressionDistribution(nn.Module):\n",
    "    \"\"\"\n",
    "        Distribution over data for regression task: p(D|w) = N(y_predicted|y, )\n",
    "    \"\"\"\n",
    "    def __init__(self, n_particles, use_var_prior=True, betta=1e-1):\n",
    "        super(RegressionDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "            use_var_prior (bool): If set to True, use Gamma prior distribution of prediction variance\n",
    "                Default: True\n",
    "            betta (float): If use_var_prior == False - defines variance of prediction\n",
    "                Default: 1e-1\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.use_var_prior = use_var_prior\n",
    "        self.betta = betta\n",
    "        \n",
    "        ### define betta - variance of data distribution for regression tasks (betta = 1 / std ** 2)\n",
    "        if self.use_var_prior:\n",
    "            self.log_betta = torch.nn.Parameter(torch.zeros([1, self.n_particles], dtype=t_type, device=device))\n",
    "        else:\n",
    "            self.log_betta = torch.tensor([math.log(self.betta)], dtype=t_type, device=device, requires_grad=False)\n",
    "\n",
    "        if self.use_var_prior:\n",
    "            ### define prior on betta p(betta)\n",
    "            self.betta_log_prior = lambda x: (gamma_density(n=1,\n",
    "                                                            alpha=1e-4,\n",
    "                                                            betta=1e-4,\n",
    "                                                            n_particles_second=True\n",
    "                                                           ).log_unnormed_density_log_x(x))\n",
    "        \n",
    "        ### Support tensors for computations\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "    \n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(theta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predicted values\n",
    "            train_size (int) - size of training dataset\n",
    "        Shapes:\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, 1]\n",
    "        \"\"\"\n",
    "        ### squeeze last axis because regression task is being solved\n",
    "        y_predict.squeeze_(2)\n",
    "        \n",
    "        batch_size = torch.tensor(y.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        log_data_distr = None\n",
    "        if self.use_var_prior:\n",
    "            log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                       mu=y,\n",
    "                                                       std=self.one / torch.sqrt(torch.exp(self.log_betta.expand(X.shape[0], self.n_particles).t())),\n",
    "                                                       n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "        else:\n",
    "            log_data_distr = lambda x: (normal_density(n=X.shape[0],\n",
    "                                                       mu=y,\n",
    "                                                       std=self.one / torch.sqrt(torch.exp(self.log_betta)),\n",
    "                                                       n_particles_second=False\n",
    "                                                      ).log_unnormed_density(x))\n",
    "            \n",
    "        if self.use_var_prior:\n",
    "            return train_size / batch_size * log_data_distr(y_predict) + self.betta_log_prior(self.log_betta)\n",
    "        return train_size / batch_size * log_data_distr(y_predict)\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ClassificationDistribution(nn.Module):\n",
    "    def __init__(self, n_particles):\n",
    "        super(ClassificationDistribution, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_particles (int): number of particles\n",
    "        \"\"\"\n",
    "        self.n_particles = n_particles \n",
    "\n",
    "    def calc_log_data(self, X, y, y_predict, train_size):  \n",
    "        \"\"\"\n",
    "            Evaluate log p(theta) \n",
    "        Args:\n",
    "            X (array_like): batch of data\n",
    "            y (array_like): batch of target values\n",
    "            y_predict (array_like): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shapes: \n",
    "            X.shape = [batch_size, in_features]\n",
    "            y.shape = [batch_size]\n",
    "            y_predict.shape = [n_particles, batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        batch_size = torch.tensor(X.shape[0], dtype=t_type, device=device)\n",
    "        train_size = torch.tensor(train_size, dtype=t_type, device=device)\n",
    "        \n",
    "        ### define distribution over data p(D|w)\n",
    "        ### n_particles_second == False because y_predict has shape = [n_particles, batch_size]\n",
    "        \n",
    "        probas = nn.LogSoftmax(dim=2)(y_predict)\n",
    "        probas_selected = torch.gather(input=probas, dim=2, index=y.view(1, -1, 1).expand(probas.shape[0], probas.shape[1], 1)).squeeze(2)\n",
    "        log_data = torch.sum(probas_selected, dim=1)\n",
    "        \n",
    "        return train_size / batch_size  * log_data\n",
    "    \n",
    "    def modules(self):\n",
    "        yield self\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"\n",
    "        if trainable:\n",
    "            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n",
    "        else:\n",
    "            return sum(param.numel() for param in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [
     1,
     45,
     47,
     51,
     58,
     65,
     73,
     77,
     79,
     82,
     98,
     103,
     116,
     129,
     226,
     257,
     267,
     287,
     328,
     334,
     352,
     466,
     477,
     484,
     490,
     516
    ]
   },
   "outputs": [],
   "source": [
    "class DistributionMover(nn.Module):\n",
    "    def __init__(self,\n",
    "                 task='app', \n",
    "                 n_particles=None,\n",
    "                 particles=None,\n",
    "                 target_density=None,\n",
    "                 n_dims=None,\n",
    "                 n_hidden_dims=None,\n",
    "                 use_latent=False,\n",
    "                 net=None,\n",
    "                 precomputed_params=None,\n",
    "                 data_distribution=None\n",
    "                ):\n",
    "        super(DistributionMover, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            task (str):\n",
    "                'app' | 'net_reg' | 'net_class'\n",
    "                - approximate target distribution\n",
    "                - solve regression task using net\n",
    "                - solve classification task using net\n",
    "            n_particles (int): number of particles\n",
    "            particles (2D array_like): array which contains initialized particles\n",
    "            target_density (callable): computes probability density function of target distribution (only for 'app' task)\n",
    "            n_dims (int): dimension of the space where optimization is performed\n",
    "            n_hidden_dims (int): dimension of the latent space\n",
    "            use_latent (bool): If set to True, Subspace Stein is used\n",
    "            acr (list): List contains arcitecture of object which is used to make predictions (for 'net_reg' and' net_class' tasks)\n",
    "            precomputed_params (1D array_like): Precomputed parameters, which will be used for particles initialization\n",
    "            data_distribution (callable): computes probability over data p(D|w) (for 'net_reg' and' net_class' tasks)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.task = task\n",
    "        \n",
    "        self.n_particles = n_particles\n",
    "        self.particles = particles\n",
    "        self.target_density = target_density\n",
    "        self.n_dims = n_dims\n",
    "        self.n_hidden_dims = n_hidden_dims\n",
    "        self.use_latent = use_latent\n",
    "        self.net = net\n",
    "        self.precomputed_params = precomputed_params\n",
    "        self.data_distribution = data_distribution\n",
    "\n",
    "        \n",
    "        if self.task == 'net_reg' or self.task == 'net_class':\n",
    "            self.n_dims = self.numel() // self.n_particles\n",
    "        if not self.use_latent:\n",
    "            self.n_hidden_dims = self.n_dims\n",
    "\n",
    "        ### Learnable samples from the target distribution\n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_hidden_dims, self.n_particles],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., 2.)\n",
    "\n",
    "        ### Class for performing linear transformations\n",
    "        if self.use_latent:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=False, \n",
    "                normalize=True\n",
    "            )\n",
    "        else:\n",
    "            self.lt = LinearTransform(\n",
    "                n_dims=self.n_dims,\n",
    "                n_hidden_dims=self.n_hidden_dims,\n",
    "                use_identity=True, \n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "        if self.precomputed_params is not None:\n",
    "            self.particles = self.lt.inverse_transform(self.precomputed_params.unsqueeze(1).expand(self.n_dims, self.n_particles))\n",
    "\n",
    "        ### Functions of probability density of target distribution\n",
    "        if self.net is None:\n",
    "            # use unnormed probability density to speedup computations\n",
    "            if target_density is not None:\n",
    "                self.target_density = target_density\n",
    "                self.real_target_density = target_density\n",
    "            else:\n",
    "                self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                                   0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "                self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                        0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "\n",
    "        ### Number of iterations since beginning\n",
    "        self.iter = 0\n",
    "\n",
    "        ### Adagrad parameters\n",
    "        self.fudge_factor = torch.tensor(1e-6, dtype=t_type, device=device)\n",
    "        self.step_size = torch.tensor(1e-2, dtype=t_type, device=device)\n",
    "        self.auto_corr = torch.tensor(0.9, dtype=t_type, device=device)\n",
    "\n",
    "        ### Gradient history term for adagrad optimization\n",
    "        if self.use_latent:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_hidden_dims, n_particles], dtype=t_type, device=device)\n",
    "            self.historical_grad_theta_0 = torch.zeros(\n",
    "                [self.n_dims, 1], dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.historical_grad = torch.zeros(\n",
    "                [self.n_dims, n_particles], dtype=t_type, device=device)\n",
    "\n",
    "        ### Factor from kernel\n",
    "        self.h = torch.tensor(0., dtype=t_type, device=device)\n",
    "\n",
    "        ### Support tensors for computations\n",
    "        self.N = torch.tensor(self.n_particles, dtype=t_type, device=device)\n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        self.two = torch.tensor(2., dtype=t_type, device=device)\n",
    "        self.three = torch.tensor(3., dtype=t_type, device=device)\n",
    "    \n",
    "    def numel(self, trainable=True):\n",
    "        \"\"\"\n",
    "            Count parameters in layer\n",
    "        Args:\n",
    "            trainable (bool): If set to False, the number of all trainable and not trainable parameters will be returned\n",
    "                Default: True\n",
    "        \"\"\"   \n",
    "        cnt = 0\n",
    "        for module in self.children():\n",
    "            if 'numel' in dir(module):\n",
    "                cnt += module.numel(trainable)\n",
    "        return cnt\n",
    "    \n",
    "    def calc_kernel_term_latent(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        ### power for rational kernel\n",
    "        p = torch.tensor(p, dtype=t_type, device=device) if p is not None else None\n",
    "        \n",
    "        ### theta = Atheta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### diffs[i, j] = A(theta`_i - theta`_j)\n",
    "        diffs = pairwise_diffs(real_particles, real_particles, n_particles_second=True)\n",
    "        ### dists[i, j] = ||A(theta`_i - theta`_j)||\n",
    "        dists = pairwise_dists(diffs=diffs, n_particles_second=True)\n",
    "        ### sq_dists[i, j] = ||A(theta`_i - theta`_j)||^2\n",
    "        sq_dists = torch.pow(dists, self.two)\n",
    "        \n",
    "        if type(h_type) == float:\n",
    "            self.h = h_type\n",
    "        elif h_type == 0:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "        elif h_type == 1:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 2:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1) * (2. * self.n_dims)\n",
    "        elif h_type == 3:\n",
    "            var = torch.var(self.particles) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1.) * (2. * self.n_dims)\n",
    "        elif h_type == 4:\n",
    "            var = torch.var(diffs) + self.fudge_factor\n",
    "            self.h = var / torch.log(self.N + 1) * (self.n_dims)\n",
    "        elif h_type == 5:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  med / (torch.pow(self.N, self.two) - self.one)\n",
    "        elif h_type == 6:\n",
    "            med = torch.median(sq_dists) + self.fudge_factor\n",
    "            self.h =  med / (torch.pow(self.N, -self.one / p) - self.one)\n",
    "        elif h_type == 7:\n",
    "            med = torch.median(torch.matmul(real_particles.t(), real_particles)) + self.fudge_factor\n",
    "            self.h = med / torch.log(self.N + 1)\n",
    "        \n",
    "        kernel = None\n",
    "        grad_kernel = None\n",
    "        if kernel_type == 'rbf':\n",
    "            ### RBF Kernel:\n",
    "            ### kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "            kernel = torch.exp(-self.one / self.h * sq_dists)\n",
    "            ### grad_kernel[i, j] = -2/h * A(theta`_i - theta`_j) * kernel[i, j]\n",
    "            grad_kernel = -self.two / self.h * kernel.unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'imq':\n",
    "            ### IMQ Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, -self.one / self.two)\n",
    "            ### grad_kernel[i, j] = -1/h * A(theta`_i - theta`_j) * kernel^(3)[i, j]\n",
    "            grad_kernel = -self.one / self.h * torch.pow(kernel, self.three).unsqueeze(0) * diffs\n",
    "        elif kernel_type == 'exp':\n",
    "            ### Exponential Kernel:\n",
    "            ### kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "            kernel = torch.exp(self.one / self.h * torch.matmul(real_particles.t(), real_particles))\n",
    "            ### grad_kernel[i, j] = 1/h * (Atheta`_j + theta_0) * kernel[i, j]\n",
    "            grad_kernel = 1. / self.h * kernel.unsqueeze(0) * real_particles.unsqueeze(1)\n",
    "        elif kernel_type == 'rat':\n",
    "            ### RAT Kernel:\n",
    "            ### kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "            kernel = torch.pow(self.one + self.one / self.h * sq_dists, p)\n",
    "            ### grad_kernel[i, j] = p/h * A(theta`_i - theta`_j) * kernel^((p - 1)/p)[i, j]\n",
    "            grad_kernel = p / self.h * torch.pow(kernel, (self.p - self.one) / p).unsqueeze(0) * diffs\n",
    "            \n",
    "        return kernel, grad_kernel\n",
    "    \n",
    "    def calc_kernel_term_latent_net(self, h_type, kernel_type='rbf', p=None):\n",
    "        \"\"\"\n",
    "            Calculate k(*,*), grad(k(*,*))\n",
    "        Args:\n",
    "            h_type (int, float): \n",
    "                If float then use h_type as kernel factor\n",
    "                If int: 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7:\n",
    "                    0 - med(dist(theta-theta`)^2) / logN\n",
    "                    1 - med(dist(theta-theta`)^2) / logN * n_dims\n",
    "                    2 - med(dist(theta-theta`)) / logN * 2 * n_dims\n",
    "                    3 - var(theta) / logN * 2 * n_dims\n",
    "                    4 - var(diff(theta-theta`) / logN * n_dims\n",
    "                    5 - med(dist(theta-theta`)^2) / (N^2 - 1)\n",
    "                    6 - med(dist(theta-theta`)^2) / (N^(-1/p) - 1)\n",
    "                    7 - -med(<Atheta`_i + theta_0, Atheta`_j + theta_0>) / logN\n",
    "            kernel_type (std):\n",
    "                'rbf' | 'imq' | 'exp' | 'rat'\n",
    "                    - kernel[i, j] = exp(-1/h * ||A(theta`_i - theta`_j)||^2)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(-1/2)\n",
    "                    - kernel[i, j] = exp(1/h * <Atheta`_i + theta_0, Atheta`_j + theta_0>)\n",
    "                    - kernel[i, j] = (1 + 1/h * ||A(theta`_i - theta`_j)||^2)^(p)\n",
    "                Default: 'rbf'\n",
    "            p (double, None): power in rational kernel \n",
    "                If kernel_type == 'rat' then p must be not None\n",
    "                Default: None\n",
    "        Shape:\n",
    "            Output: \n",
    "                ([n_particles, n_particles], [n_dims, n_particles, n_particles])\n",
    "        \"\"\"\n",
    "        return self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "    \n",
    "    def calc_log_prior_net(self):\n",
    "        \"\"\"\n",
    "            Traverse all modules and evaluate weights log prior\n",
    "        \"\"\"\n",
    "        log_prior = 0\n",
    "        for module in self.children():\n",
    "            if 'calc_log_prior' in dir(module):\n",
    "                log_prior += module.calc_log_prior()\n",
    "        return log_prior\n",
    "\n",
    "    def calc_log_term_latent(self):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta))\n",
    "        Shape:\n",
    "            Output: [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        \n",
    "        ### theta = A theta` + theta_0\n",
    "        real_particles = self.lt.transform(self.particles.detach(), n_particles_second=True).requires_grad_(True)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_term = torch.log(self.target_density(real_particles))\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "    \n",
    "        grad_log_term = real_particles.grad\n",
    "        \n",
    "        return grad_log_term\n",
    "    \n",
    "    def calc_log_term_latent_net(self, X, y, train_size):\n",
    "        \"\"\"\n",
    "            Calculate grad(log p(theta)) \n",
    "        Args:\n",
    "            X (torch.tensor): batch of data\n",
    "            y (torch.tensor): batch of predictions\n",
    "            train_size (int): size of train dataset\n",
    "        Shape:\n",
    "            Input: \n",
    "                x.shape = [batch_size, in_features]\n",
    "                y.shape = [batch_size, out_features]\n",
    "            Output: \n",
    "                [n_dims, n_particles]\n",
    "        \"\"\"\n",
    "        log_data = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        log_prior = torch.zeros([self.n_particles], dtype=t_type, device=device)\n",
    "        \n",
    "        ### get real net parameters: theta_i = A theta`_i + theta_0\n",
    "        real_particles = self.lt.transform(self.particles, n_particles_second=True)\n",
    "        ### init net with real parameters\n",
    "        self.vector_to_parameters(real_particles.view(-1), self.parameters_net())\n",
    "        ### compute log prior of all weight in the net\n",
    "        log_prior = self.calc_log_prior_net()\n",
    "        \n",
    "        ### get prediction for the batch of data\n",
    "        y_predict = self.predict_net(X)\n",
    "        ### compute log data term log p(D|w)\n",
    "        log_data = self.data_distribution.calc_log_data(X, y, y_predict, train_size)\n",
    "        \n",
    "        ### log_term = log p(theta) = log p_prior(theta) + log p_data(D|theta)\n",
    "        log_term = log_prior + log_data\n",
    "        \n",
    "        ### evaluate gradient with respect to trainable parameters\n",
    "        for idx in range(self.n_particles):\n",
    "            log_term[idx].backward(retain_graph=True)\n",
    "        \n",
    "        ### collect all gradients into one vector\n",
    "        grad_log_term = self.parameters_grad_to_vector(self.parameters_net()).view(-1, self.n_particles)\n",
    "            \n",
    "        return grad_log_term\n",
    "    \n",
    "    def parameters_net(self):\n",
    "        \"\"\"\n",
    "            Return all trainable parameters\n",
    "        \"\"\"\n",
    "        return chain(self.net.parameters(), self.data_distribution.parameters())\n",
    "    \n",
    "    def predict_net(self, X, inference=False):\n",
    "        \"\"\"\n",
    "            Use net to make predictions        \n",
    "            Args:\n",
    "                X (array_like): batch of data\n",
    "        \"\"\"\n",
    "        predictions = self.net(X.unsqueeze(0).expand(self.n_particles, *X.shape))\n",
    "        if self.task == 'net_reg':\n",
    "            if inference:\n",
    "                return torch.mean(predictions, dim=0)\n",
    "            else:\n",
    "                return predictions\n",
    "        elif self.task == 'net_class':\n",
    "            if inference:\n",
    "                return torch.log(torch.mean(torch.nn.Softmax(dim=2)(predictions), dim=0))\n",
    "            else:\n",
    "                return predictions\n",
    "\n",
    "    def update_latent(self, \n",
    "                      h_type, kernel_type='rbf', p=None, \n",
    "                      step_size=None, \n",
    "                      move_theta_0=False, \n",
    "                      burn_in=False, burn_in_coeff=None,\n",
    "                      epoch=None\n",
    "                     ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "            \n",
    "        self.iter += 1\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent()\n",
    "        \n",
    "        ### Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "        \n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "        \n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "        \n",
    "        ### Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            ### Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "            \n",
    "            ### Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            ### Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "               \n",
    "    def update_latent_net(self,\n",
    "                          h_type, kernel_type='rbf', p=None,\n",
    "                          X_batch=None, y_batch=None, train_size=None,\n",
    "                          step_size=None,\n",
    "                          move_theta_0=False, \n",
    "                          burn_in=False, burn_in_coeff=None, \n",
    "                          epoch=None\n",
    "                         ):\n",
    "        self.step_size = step_size if step_size is not None else self.step_size\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        if burn_in:\n",
    "            self.burn_in_coeff = torch.tensor(burn_in_coeff, dtype=t_type, device=device)\n",
    "        else:\n",
    "            self.burn_in_coeff = self.one\n",
    "            \n",
    "        self.iter += 1\n",
    "        self.net.zero_grad()\n",
    "        self.data_distribution.zero_grad()\n",
    "\n",
    "        ### Compute additional terms\n",
    "        kernel, grad_kernel = self.calc_kernel_term_latent_net(h_type, kernel_type, p)\n",
    "        grad_log_term = self.calc_log_term_latent_net(X_batch, y_batch, train_size)\n",
    "        \n",
    "        ### Increase grad_log_term in burn_in_coeff times\n",
    "        grad_log_term *= self.burn_in_coeff\n",
    "        \n",
    "        ### Compute value of step in functional space\n",
    "        phi = (torch.matmul(grad_log_term, kernel) + torch.sum(grad_kernel, dim=1)) / self.N\n",
    "\n",
    "        ### Transform phi from R^D space to R^d space: phi` = (A^(t)A)^(-1)A^(t)phi\n",
    "        phi = self.lt.project_inverse(phi, n_particles_second=True)\n",
    "\n",
    "        ### Update gradient history\n",
    "        if self.iter == 1:\n",
    "            self.historical_grad = self.historical_grad + phi * phi\n",
    "        else:\n",
    "            self.historical_grad = self.auto_corr * self.historical_grad + (self.one - self.auto_corr) * phi * phi\n",
    "\n",
    "        ### Adjust gradient and make step\n",
    "        adj_phi = phi / (self.fudge_factor + torch.sqrt(self.historical_grad))\n",
    "        self.particles = self.particles + self.step_size * adj_phi\n",
    "                \n",
    "        ### Update theta_0 in LinearTransform\n",
    "        if self.use_latent and move_theta_0:\n",
    "            ### Compute value of step in functional space\n",
    "            theta_0_update = torch.mean(grad_log_term, dim=1).view(-1, 1)\n",
    "            \n",
    "            ### Update gradient history\n",
    "            if self.iter == 1:\n",
    "                self.historical_grad_theta_0 = self.historical_grad_theta_0 + theta_0_update * theta_0_update\n",
    "            else:\n",
    "                self.historical_grad_theta_0 = self.auto_corr * self.historical_grad_theta_0 + (self.one - self.auto_corr) * theta_0_update * theta_0_update\n",
    "\n",
    "            ### Adjust gradient and make step\n",
    "            adj_theta_0_update = theta_0_update / (self.fudge_factor + torch.sqrt(self.historical_grad_theta_0))\n",
    "            self.lt.theta_0 = self.lt.theta_0 + self.step_size * adj_theta_0_update\n",
    "\n",
    "    @staticmethod\n",
    "    def vector_to_parameters(vec, parameters):\n",
    "        pointer = 0\n",
    "        for param in parameters:\n",
    "            # The length of the parameter\n",
    "            num_param = param.numel()\n",
    "            # Slice the vector, reshape it, and replace the old data of the parameter\n",
    "            param.data = vec[pointer:pointer + num_param].view_as(param).data\n",
    "            # Increment the pointer\n",
    "            pointer += num_param\n",
    "            \n",
    "    @staticmethod \n",
    "    def parameters_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    @staticmethod \n",
    "    def parameters_grad_to_vector(parameters):\n",
    "        vec = []\n",
    "        for param in parameters:\n",
    "            vec.append(param.grad.view(-1))\n",
    "        return torch.cat(vec)\n",
    "    \n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        r\"\"\"Returns a dictionary containing a whole state of the module.\n",
    "        Both parameters and persistent buffers (e.g. running averages) are\n",
    "        included. Keys are corresponding parameter and buffer names.\n",
    "        Returns:\n",
    "            dict:\n",
    "                a dictionary containing a whole state of the module\n",
    "        \"\"\"\n",
    "        destination = super(DistributionMover, self).state_dict(destination, prefix, keep_vars)\n",
    "        \n",
    "        if destination is None:\n",
    "            destination = OrderedDict()\n",
    "            destination._metadata = OrderedDict()\n",
    "        destination._metadata[prefix[:-1]] = dict(version=self._version)\n",
    "        \n",
    "        destination[prefix + 'particles'] = self.particles if keep_vars else self.particles.data\n",
    "        destination[prefix + 'historical_grad'] = self.historical_grad if keep_vars else self.historical_grad.data\n",
    "        if self.use_latent:\n",
    "            destination[prefix + 'historical_grad_theta_0'] = self.historical_grad_theta_0 if keep_vars else self.historical_grad_theta_0.data\n",
    "            self.lt.state_dict(destination, prefix + 'lt' + '.', keep_vars=keep_vars)\n",
    "        destination[prefix + 'step_size'] = self.step_size\n",
    "        destination[prefix + 'iter'] = self.iter\n",
    "        destination[prefix + 'epoch'] = self.epoch\n",
    "                \n",
    "        return destination\n",
    "    \n",
    "    def load_state_dict(self, state_dict, prefix=''):\n",
    "        super(DistributionMover, self).load_state_dict(state_dict, prefix)\n",
    "        \n",
    "        self.particles.copy_(state_dict[prefix + 'particles'])\n",
    "        self.historical_grad.copy_(state_dict[prefix + 'historical_grad'])\n",
    "        self.step_size = state_dict[prefix + 'step_size']\n",
    "        self.iter = state_dict[prefix + 'iter']\n",
    "        self.epoch = state_dict[prefix + 'epoch']\n",
    "        \n",
    "        if self.use_latent:\n",
    "            self.historical_grad_theta_0.copy_(state_dict[prefix + 'historical_grad_theta_0'])\n",
    "            self.lt.load_state_dict(state_dict, prefix + 'lt' + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LRStrategy:\n",
    "    def __init__(self, step_size, factor=0.1, n_epochs=1, patience=10):\n",
    "        \"\"\"\n",
    "            Multiply @step_size by factor each @n_epochs epochs\n",
    "            Freeze @step_size after @patience epochs\n",
    "        \"\"\"\n",
    "        self.step_size = step_size\n",
    "        self.factor = factor\n",
    "        self.n_epochs = n_epochs\n",
    "        self.patience = patience\n",
    "        self.iter = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.iter += 1\n",
    "        if self.iter < self.patience and self.iter % self.n_epochs == 0:\n",
    "            self.step_size *= self.factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add some methods to nn.Sequential to make code clear \n",
    "\n",
    "setattr(nn.Sequential, \"numel\", DistributionMover.numel)\n",
    "setattr(nn.Sequential, \"calc_log_prior\", DistributionMover.calc_log_prior_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Bostor housing dataset for regression task\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test = (torch.tensor(X_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(X_test, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_train, dtype=t_type, device=device),\n",
    "                                    torch.tensor(y_test, dtype=t_type, device=device))\n",
    "\n",
    "### Linear Regression baseline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.tensor(model.predict(X_test), dtype=t_type, device=device), y_test), \n",
    "      torch.nn.MSELoss()(torch.tensor(model.predict(X_train), dtype=t_type, device=device), y_train))\n",
    "\n",
    "print(torch.nn.MSELoss()(torch.mean(y_train).expand(y_test.shape[0]), y_test), \n",
    "      torch.nn.MSELoss()(torch.mean(y_train).expand(y_train.shape[0]), y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Check all functions\n",
    "dm = DistributionMover(task='app', n_dims=13, n_hidden_dims=5, n_particles=10, use_latent=True)\n",
    "dm.calc_log_term_latent()\n",
    "dm.calc_kernel_term_latent(0)\n",
    "dm.update_latent(0)\n",
    "\n",
    "net = nn.Sequential(SteinLinear(13, 1, 10, use_var_prior=True))\n",
    "dd = RegressionDistribution(n_particles=10, use_var_prior=False)\n",
    "dm = DistributionMover(task='net_reg', n_hidden_dims=5, n_particles=10, use_latent=True, net=net, data_distribution=dd)\n",
    "dm.calc_log_term_latent_net(X_train, y_train, X_train.shape[0])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Boston Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(SteinLinear(13, 1, 100, use_var_prior=False, alpha=1e-2, use_bias=True))\n",
    "data_distr = RegressionDistribution(100, use_var_prior=False, betta=1e-1)\n",
    "dm = DistributionMover(task='net_reg', n_particles=100, use_latent=False, net=net, data_distribution=data_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# alpha = dm.net[0].alpha\n",
    "# betta = dm.data_distribution.betta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Append column of ones to data \n",
    "# XX = X_train\n",
    "# XX_test = X_test\n",
    "XX = torch.cat([X_train, torch.ones([X_train.shape[0], 1], dtype=t_type, device=device)], dim=1)\n",
    "XX_test = torch.cat([X_test, torch.ones([X_test.shape[0], 1], dtype=t_type, device=device)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sigma = torch.inverse(betta * XX.t() @ XX + alpha * torch.eye(XX.shape[1], dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mu = betta * sigma @ XX.t() @ y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu @ XX.t(), y_train), torch.nn.MSELoss()(mu @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_test = torch.load('particles_12400.txt').cuda(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "particles_mean = torch.mean(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.nn.MSELoss()(particles_mean @ XX.t(), y_train), torch.nn.MSELoss()(particles_mean @ XX_test.t(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.var(particles_test, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.diag(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import *\n",
    "\n",
    "mod = WLS(y_train.data.cpu().numpy(), XX.data.cpu().numpy())\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stein_mu = (torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, torch.tensor(results.params, dtype=t_type, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, particles_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(sum(np.logical_and(results.conf_int()[:,0] < stein_mu.data.cpu().numpy(), stein_mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < mu.data.cpu().numpy(), mu.data.cpu().numpy() < results.conf_int()[:,1])),\n",
    "#       sum(np.logical_and(results.conf_int()[:,0] < particles_mean.data.cpu().numpy(), particles_mean.data.cpu().numpy() < results.conf_int()[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()(mu, torch.mean(dm.lt.transform(dm.particles, n_particles_second=True), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    step_size = 0.00075\n",
    "    dm.historical_grad.zero_()\n",
    "    for _ in range(100000):\n",
    "        dm.update_latent_net(h_type=1, kernel_type='rbf', p=-1, X_batch=X_train, y_batch=y_train, train_size=X_train.shape[0], step_size=step_size)\n",
    "        train_loss = torch.nn.MSELoss()(dm.predict_net(X_train, inference=True).view(-1), y_train)\n",
    "        test_loss = torch.nn.MSELoss()(dm.predict_net(X_test, inference=True).view(-1), y_test)\n",
    "        \n",
    "        if _ % 10 == 0:\n",
    "            clear_output()\n",
    "            \n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train): {1:.3f}\\t Empirical Loss(Test): {2:.3f}\\t Kernel factor: {3:.3f}'.format(\n",
    "                            _, train_loss, test_loss, dm.h))\n",
    "            \n",
    "            plot_projections(dm, use_real=True)\n",
    "            plot_projections(dm, use_real=False)\n",
    "            plt.pause(1e-300)\n",
    "            \n",
    "        if _ % 3000 == 0 and _ > 0:\n",
    "            step_size /= 2\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset with class selection\n",
    "\n",
    "sys.path.insert(0, '/home/m.nakhodnov/Samsung-Tasks/Datasets/MyMNIST')\n",
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform)\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform)\n",
    "\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=100, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     23,
     38,
     54
    ]
   },
   "outputs": [],
   "source": [
    "def train(dm,\n",
    "          dataloader_train, dataloader_test,\n",
    "          lr_str, start_epoch, end_epoch, n_epochs_save=20, \n",
    "          move_theta_0=False, plot_graphs=True, verbose=False,\n",
    "          checkpoint_file_name=None, plots_file_name=None, log_file_name=None,\n",
    "          n_warmup_epochs=16,\n",
    "          ):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    ### Mean loss/accuracy from @n_warmup_epochs to current epoch\n",
    "    test_losses_mean = []\n",
    "    test_accs_mean = []\n",
    "    predictions_test_cummulative = torch.zeros([1, 1], dtype=t_type, device=device)\n",
    "    \n",
    "    if log_file_name is not None:\n",
    "        log_file = open(log_file_name, 'a')\n",
    "        log_file.write('\\rNew run of training.\\r')\n",
    "        log_file.close()\n",
    "        \n",
    "    try:\n",
    "        for epoch in range(start_epoch, end_epoch):\n",
    "            for X, y in dataloader_train:\n",
    "                X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "                y = y.to(device=device)\n",
    "                burn_in_coeff = max(1. - (1. - 1.) / 20. * epoch, 1.)\n",
    "                dm.update_latent_net(h_type=0, kernel_type='rbf', p=None,\n",
    "                                     X_batch=X, y_batch=y,\n",
    "                                     train_size=len(dataloader_train) * dataloader_train.batch_size,\n",
    "                                     step_size=lr_str.step_size,\n",
    "                                     move_theta_0=move_theta_0,\n",
    "                                     burn_in=True, burn_in_coeff=burn_in_coeff,\n",
    "                                     epoch=epoch\n",
    "                                     )\n",
    "\n",
    "            train_loss = 0.\n",
    "            train_acc = 0.\n",
    "            for X_train, y_train in dataloader_train:\n",
    "                X_train = X_train.double().to(device=device).view(X.shape[0], -1)\n",
    "                y_train = y_train.to(device=device)\n",
    "\n",
    "                net_pred = dm.predict_net(X_train, inference=True)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                train_loss -= torch.sum(torch.gather(net_pred, 1, y_train.view(-1, 1)))\n",
    "                train_acc += torch.sum(y_pred == y_train).float()\n",
    "            train_loss /= (len(dataloader_train) + 0.)\n",
    "            train_acc /= (len(dataloader_train) * dataloader_train.batch_size + 0.)\n",
    "\n",
    "            test_loss = 0.\n",
    "            test_acc = 0.\n",
    "            predictions_test_current = torch.tensor([], dtype=t_type, device=device)\n",
    "            y_test_all = torch.tensor([], dtype=torch.int64, device=device)\n",
    "            for X_test, y_test in dataloader_test:\n",
    "                X_test = X_test.double().to(device=device).view(X.shape[0], -1)\n",
    "                y_test = y_test.to(device=device)\n",
    "\n",
    "                ### Get output of net before Softmax, mean and log, Shape = [n_particles, batch_size, output_features]\n",
    "                net_pred_pure = dm.predict_net(X_test, inference=False)\n",
    "                net_pred_pure = torch.mean(torch.nn.Softmax(dim=2)(net_pred_pure), dim=0)\n",
    "                predictions_test_current = torch.cat([predictions_test_current, net_pred_pure], dim=0)\n",
    "                y_test_all = torch.cat([y_test_all, y_test], dim=0)\n",
    "\n",
    "                net_pred = torch.log(net_pred_pure)\n",
    "                y_pred = torch.argmax(net_pred, dim=1)\n",
    "\n",
    "                test_loss -= torch.sum(torch.gather(net_pred, 1, y_test.view(-1, 1)))\n",
    "                test_acc += torch.sum(y_pred == y_test).float()\n",
    "            test_loss /= (len(dataloader_test) + 0.)\n",
    "            test_acc /= (len(dataloader_test) * dataloader_test.batch_size + 0.)\n",
    "\n",
    "            test_loss_mean = 0.\n",
    "            test_acc_mean = 0.\n",
    "            epoch_since_beg = epoch - start_epoch\n",
    "            if epoch_since_beg >= n_warmup_epochs:\n",
    "                print((epoch_since_beg - n_warmup_epochs) / (epoch_since_beg - n_warmup_epochs + 1.), 1. / (epoch_since_beg - n_warmup_epochs + 1.))\n",
    "                predictions_test_cummulative = (\n",
    "                            predictions_test_cummulative * (epoch_since_beg - n_warmup_epochs) / (epoch_since_beg - n_warmup_epochs + 1.) +\n",
    "                            predictions_test_current / (epoch_since_beg - n_warmup_epochs + 1.))\n",
    "                log_predictions_test = torch.log(predictions_test_cummulative)\n",
    "                y_pred_all = torch.argmax(log_predictions_test, dim=1)\n",
    "                test_loss_mean = -torch.sum(torch.gather(log_predictions_test, 1, y_test_all.view(-1, 1)))\n",
    "                test_acc_mean = torch.sum(y_pred_all == y_test_all).float()\n",
    "                test_loss_mean /= (len(dataloader_test) + 0.)\n",
    "                test_acc_mean /= (len(dataloader_test) * dataloader_test.batch_size + 0.)\n",
    "\n",
    "            train_losses.append(train_loss.data[0].cpu().numpy())\n",
    "            train_accs.append(train_acc.data[0].cpu().numpy())\n",
    "            test_losses.append(test_loss.data[0].cpu().numpy())\n",
    "            test_accs.append(test_acc.data[0].cpu().numpy())\n",
    "            if epoch >= n_warmup_epochs:\n",
    "                test_losses_mean.append(test_loss_mean.data[0].cpu().numpy())\n",
    "                test_accs_mean.append(test_acc_mean.data[0].cpu().numpy())\n",
    "            else:\n",
    "                test_losses_mean.append(None)\n",
    "                test_accs_mean.append(None)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                sys.stdout.write(\n",
    "                    ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                     '\\nEmpirical Loss(Train/Test/Test (Mean)): {4:.3f}/{5:.3f}/{6:.3f}\\t Accuracy(Train/Test/Test (Mean)): {7:.3f}/{8:.3f}/{9:.3f}\\t'\n",
    "                     ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                              train_loss, test_loss, test_loss_mean, train_acc, test_acc, test_acc_mean\n",
    "                              )\n",
    "                )\n",
    "                if log_file_name is not None:\n",
    "                    log_file = open(log_file_name, 'a')\n",
    "                    log_file.write(\n",
    "                        ('\\nEpoch {0}... \\t Step Size {1:.3f}\\t Kernel factor: {2:.3f}\\t Burn-in Coeff: {3:.3f}' +\n",
    "                         '\\nEmpirical Loss(Train/Test/Test (Mean)): {4:.3f}/{5:.3f}/{6:.3f}\\t Accuracy(Train/Test/Test (Mean)): {7:.3f}/{8:.3f}/{9:.3f}\\t'\n",
    "                         ).format(epoch, lr_str.step_size, dm.h, dm.burn_in_coeff,\n",
    "                                  train_loss, test_loss, test_loss_mean, train_acc, test_acc, test_acc_mean\n",
    "                                  )\n",
    "                    )\n",
    "                    log_file.close()\n",
    "\n",
    "            if epoch % n_epochs_save == 0 and epoch > start_epoch and checkpoint_file_name is not None:\n",
    "                torch.save(dm.state_dict(), checkpoint_file_name.format(epoch))\n",
    "\n",
    "            lr_str.step()\n",
    "            \n",
    "            gpu_profile(frame=sys._getframe(), event='line', arg=None)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    if plot_graphs:\n",
    "        print_plots([[train_losses, test_losses, test_losses_mean],\n",
    "                     [train_accs, test_accs, test_accs_mean]],\n",
    "                    [['Epochs', ''],\n",
    "                     ['Epochs', '% * 1e-2']],\n",
    "                    [['Cross Entropy Loss (Train)', 'Cross Entropy Loss (Test)', 'Cross Entropy Loss (Test (Mean))'],\n",
    "                     ['Accuracy (Train)', 'Accuracy (Test)', 'Accuracy (Mean)']\n",
    "                     ],\n",
    "                    plots_file_name\n",
    "                    )\n",
    "    if checkpoint_file_name is not None:\n",
    "        torch.save(dm.state_dict(), checkpoint_file_name.format(epoch))\n",
    "\n",
    "    if verbose:\n",
    "        return train_losses, test_losses, train_accs, test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. particles - 1 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAP estimate for MNIST classification task\n",
    "net_1 = nn.Sequential(SteinLinear(28 * 28, 18, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 1, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 1, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_1 = ClassificationDistribution(1)\n",
    "dm_1 = DistributionMover(task='net_class', n_particles=1, use_latent=False, net=net_1, data_distribution=data_distr_1)\n",
    "lr_str_1 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_name_1 = 'model_part_1'\n",
    "version_1 = 0\n",
    "checkpoint_file_name_1 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_1 + '.pth'\n",
    "plots_file_name_1 = './Experiments/Plots/' + own_name_1 + '.png'\n",
    "log_file_name_1 = './Experiments/Logs/' + own_name_1 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_1.format(version_1)):\n",
    "    dm_1.load_state_dict(torch.load(checkpoint_file_name_1.format(version_1)))\n",
    "    lr_str_1.step_size = dm_1.step_size\n",
    "    lr_str_1.iter = dm_1.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0... \t Step Size 0.030\t Kernel factor: 0.000\t Burn-in Coeff: 1.000\n",
      "Empirical Loss(Train/Test/Test (Mean)): 65.485/63.150/0.000\t Accuracy(Train/Test/Test (Mean)): 0.806/0.813/0.000\t"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sys.settrace(gpu_profile)\n",
    "train(dm=dm_1,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_1, start_epoch=lr_str_1.iter, end_epoch=1,\n",
    "      checkpoint_file_name=checkpoint_file_name_1, plots_file_name=plots_file_name_1, log_file_name=log_file_name_1,\n",
    "      n_warmup_epochs=5\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = next(iter(dataloader_m_train))\n",
    "# X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_1.update_latent_net(h_type=0, kernel_type='rbf', p=None, X_batch=X, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_1.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2. particles - 5 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_2 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_2 = ClassificationDistribution(5)\n",
    "dm_2 = DistributionMover(task='net_class',\n",
    "                         n_particles=5,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_2,\n",
    "                         data_distribution=data_distr_2\n",
    "                        )\n",
    "lr_str_2 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_2 = 'model_part_5_latent_700'\n",
    "version_2 = 0\n",
    "checkpoint_file_name_2 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_2 + '.pth'\n",
    "plots_file_name_2 = './Experiments/Plots/' + own_name_2 + '.png'\n",
    "log_file_name_2 = './Experiments/Logs/' + own_name_2 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_2.format(version_2)):\n",
    "    dm_2.load_state_dict(torch.load(checkpoint_file_name_2.format(version_2)))\n",
    "    lr_str_2.step_size = dm_2.step_size\n",
    "    lr_str_2.iter = dm_2.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_2,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_2, start_epoch=lr_str_2.iter, end_epoch=lr_str_2.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_2, plots_file_name=plots_file_name_2, log_file_name=log_file_name_2,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X, y = next(iter(dataloader_m_train))\n",
    "# X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_2.update_latent_net(h_type=0, kernel_type='rbf', p=None, X_batch=X, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_2.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3. particles - 5 + no latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_3 = nn.Sequential(SteinLinear(28 * 28, 18, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 5, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 5, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_3 = ClassificationDistribution(5)\n",
    "dm_3 = DistributionMover(task='net_class',\n",
    "                       n_particles=5,\n",
    "                       use_latent=False,\n",
    "                       net=net_3,\n",
    "                       data_distribution=data_distr_3\n",
    "                      )\n",
    "lr_str_3 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_3 = 'model_part_5'\n",
    "version_3 = 0\n",
    "checkpoint_file_name_3 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_3 + '.pth'\n",
    "plots_file_name_3 = './Experiments/Plots/' + own_name_3 + '.png'\n",
    "log_file_name_3 = './Experiments/Logs/' + own_name_3 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_3.format(version_3)):\n",
    "    dm_3.load_state_dict(torch.load(checkpoint_file_name_3.format(version_3)))\n",
    "    lr_str_3.step_size = dm_3.step_size\n",
    "    lr_str_3.iter = dm_3.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_3,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_3, start_epoch=lr_str_3.iter, end_epoch=lr_str_3.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_3, plots_file_name=plots_file_name_3, log_file_name=log_file_name_3,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# X, y = next(iter(dataloader_m_train))\n",
    "# X = X.double().to(device=device).view(X.shape[0], -1)\n",
    "# y = y.to(device=device)\n",
    "# burn_in_coeff = max(1. - (1. - 1.) / 20. * 1, 1.)\n",
    "# %lprun -f DistributionMover.update_latent_net dm_3.update_latent_net(h_type=0, kernel_type='rbf', p=None, X_batch=X, y_batch=y, train_size=len(dataloader_m_train) * dataloader_m_train.batch_size, step_size=lr_str_3.step_size, move_theta_0=True, burn_in=True, burn_in_coeff=burn_in_coeff, epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 4. particles - 20 + latent - 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_4 = nn.Sequential(SteinLinear(28 * 28, 18, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(18, 14, 20, use_var_prior=False),\n",
    "                      nn.Tanh(),\n",
    "                      SteinLinear(14, 10, 20, use_var_prior=False)\n",
    "                     ).to(device=device)\n",
    "data_distr_4 = ClassificationDistribution(20)\n",
    "dm_4 = DistributionMover(task='net_class',\n",
    "                         n_particles=20,\n",
    "                         n_hidden_dims=700,\n",
    "                         use_latent=True,\n",
    "                         net=net_4,\n",
    "                         data_distribution=data_distr_4\n",
    "                        )\n",
    "lr_str_4 = LRStrategy(step_size=0.03, factor=0.97, n_epochs=1, patience=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "own_name_4 = 'model_part_20_latent_700'\n",
    "version_4 = 0\n",
    "checkpoint_file_name_4 = './Experiments/Checkpoints/' + 'e{0}_' + own_name_4 + '.pth'\n",
    "plots_file_name_4 = './Experiments/Plots/' + own_name_4 + '.png'\n",
    "log_file_name_4 = './Experiments/Logs/' + own_name_4 + '.txt'\n",
    "if os.path.exists(checkpoint_file_name_4.format(version_4)):\n",
    "    dm_4.load_state_dict(torch.load(checkpoint_file_name_4.format(version_4)))\n",
    "    lr_str_4.step_size = dm_4.step_size\n",
    "    lr_str_4.iter = dm_4.epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train(dm=dm_4,\n",
    "      dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "      lr_str=lr_str_4, start_epoch=lr_str_4.iter, end_epoch=lr_str_4.iter + 100, n_epochs_save=20,\n",
    "      checkpoint_file_name=checkpoint_file_name_4, plots_file_name=plots_file_name_4, log_file_name=log_file_name_4,\n",
    "      n_warmup_epochs=16\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MNIST with FC neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net_nn = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 200), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10)\n",
    ").double()\n",
    "optim_nn = torch.optim.Adam(net_nn.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for _, (X, y) in enumerate(dataloader_m_train):\n",
    "        X = X.double().view(X.shape[0], -1)\n",
    "        \n",
    "        optim_nn.zero_grad()\n",
    "        train_loss = nn.CrossEntropyLoss()(net_nn(X), y)\n",
    "        train_loss.backward()\n",
    "        optim_nn.step()\n",
    "   \n",
    "        train_loss = 0. \n",
    "        train_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_train, y_train = next(iter(dataloader_m_train))\n",
    "            X_train = X_train.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_train)), dim=1)\n",
    "            train_loss += nn.CrossEntropyLoss()(net_nn(X_train), y_train)\n",
    "            train_acc += torch.sum(y_pred == y_train).float()\n",
    "        train_loss /= 30.\n",
    "        train_acc /= (30. * dataloader_m_train.batch_size)\n",
    "        \n",
    "        test_loss = 0.\n",
    "        test_acc = 0.\n",
    "        for __ in range(30):\n",
    "            X_test, y_test = next(iter(dataloader_m_test))\n",
    "            X_test = X_test.double().view(X.shape[0], -1)\n",
    "\n",
    "            y_pred = torch.argmax(nn.Softmax()(net_nn(X_test)), dim=1)\n",
    "            test_loss += nn.CrossEntropyLoss()(net_nn(X_test), y_test)\n",
    "            test_acc += torch.sum(y_pred == y_test).float()\n",
    "        test_loss /= 30.\n",
    "        test_acc /= (30. * dataloader_m_test.batch_size)\n",
    "        \n",
    "        if _ % 1 == 0:\n",
    "            sys.stdout.write('\\rEpoch {0}... Empirical Loss(Train/Test): {1:.3f}/{2:.3f}\\t Accuracy(Train/Test): {3:.3f}/{4:.3f}\\t Kernel factor: {5:.3f}'.format(\n",
    "                            _, train_loss, test_loss, train_acc, test_acc, dm.h))\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(dataloader_m_test))\n",
    "X_test = X_test.double().view(X.shape[0], -1)\n",
    "y_pred = net_nn(X_test)\n",
    "torch.argmax(nn.Softmax()(y_pred), dim=1)[2], y_test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Distribution approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "marginal_density = lambda x : (0.3 * normal_density(mu=-2., std=1., n=1)(x) + 0.7 * normal_density(mu=2., std=1., n=1)(x))\n",
    "#marginal_density = lambda x : (normal_density(mu=0., std=2., n=1)(x))\n",
    "\n",
    "pdf = []\n",
    "for _ in np.linspace(-10, 10, 1000): \n",
    "    _ = torch.tensor(_, dtype=t_type, device=device)\n",
    "    pdf.append(marginal_density(_))\n",
    "    \n",
    "from pynverse import inversefunc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "cum_density = interp1d(np.linspace(-20, 20, 250), [quad(marginal_density, -20, x)[0] for x in np.linspace(-20, 20, 250)])\n",
    "inv_cum_density = inversefunc(cum_density)\n",
    "real_samples = [inv_cum_density(np.random.random()) for x in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean = quad(lambda x : marginal_density(x) * x, -20, 20)[0]\n",
    "var = quad(lambda x : marginal_density(x) * (x - mean) ** 2, -20, 20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-10, 10, 1000), pdf)\n",
    "plt.plot(real_samples, np.zeros_like(real_samples))\n",
    "sns.kdeplot(real_samples, kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=20, n_hidden_dims=5, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot_projections(dm, use_real=True, N_plots_max=1, pdf=pdf)\n",
    "# plot_projections(dm, use_real=False, N_plots_max=1, pdf=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(3000): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plot_projections(dm, use_real=True, kernel='tri', pdf=pdf)\n",
    "            plot_projections(dm, use_real=False, kernel='tri', pdf=pdf)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(torch.mean(dm.lt.transform(dm.particles, n_particles_second=True)),\n",
    " torch.mean(torch.std(dm.lt.transform(dm.particles, n_particles_second=True), dim=1) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conditional Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm = DistributionMover(task='app', n_particles=100, n_dims=2, n_hidden_dims=1, use_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_condition_distribution(dm, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "for _ in range(200): \n",
    "    try:\n",
    "        dm.update_latent(h_type=0, kernel_type='rbf', p=-1)\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "            \n",
    "            #plot_projections(dm, use_real=True, pdf=pdf)\n",
    "            plot_condition_distribution(dm, 100000)\n",
    "            \n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-5, 5, 0.025)\n",
    "Y = np.arange(-5, 5, 0.025)\n",
    "XX, YY = np.meshgrid(X, Y)\n",
    "XY = torch.stack([torch.tensor(XX, dtype=t_type), torch.tensor(YY, dtype=t_type)], dim=2)\n",
    "Z = torch.zeros([XY.shape[0], XY.shape[1]])\n",
    "\n",
    "Z = dm.real_target_density(XY.permute(2, 0, 1).view(2, -1)).view(Z.shape)\n",
    "Z = Z.cpu().data.numpy()\n",
    "\n",
    "# Plot the surface.\n",
    "plt.contour(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "# Plot conditioning line\n",
    "XXX, YYY = dm.lt.transform(torch.tensor(X, dtype=t_type, device=device).view(1, -1), n_particles_second=True).data.cpu().numpy()\n",
    "plt.plot(XXX, YYY, 'r', label='Linear manifold', )\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Density contour lines\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compare implementations of Stein Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class SVGD():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def svgd_kernel(self, theta, h = -1):\n",
    "        sq_dist = pdist(theta)\n",
    "        pairwise_dists = squareform(sq_dist)**2\n",
    "        if h < 0: # if h < 0, using median trick\n",
    "            h = np.median(pairwise_dists)  \n",
    "            h = h / np.log(theta.shape[0]+1)\n",
    "\n",
    "        # compute the rbf kernel\n",
    "        Kxy = np.exp( -pairwise_dists / h)\n",
    "\n",
    "        dxkxy = -np.matmul(Kxy, theta)\n",
    "        sumkxy = np.sum(Kxy, axis=1)\n",
    "        for i in range(theta.shape[1]):\n",
    "            dxkxy[:, i] = dxkxy[:,i] + np.multiply(theta[:,i],sumkxy)\n",
    "        dxkxy = 2. * dxkxy / (h)\n",
    "        return (Kxy, dxkxy)\n",
    "    \n",
    " \n",
    "    def update(self, x0, lnprob, n_iter = 1000, stepsize = 1e-3, bandwidth = -1, alpha = 0.9, debug = False):\n",
    "        # Check input\n",
    "        if x0 is None or lnprob is None:\n",
    "            raise ValueError('x0 or lnprob cannot be None!')\n",
    "        \n",
    "        theta = np.copy(x0) \n",
    "        \n",
    "        # adagrad with momentum\n",
    "        fudge_factor = 1e-6\n",
    "        historical_grad = 0\n",
    "        for iter in range(n_iter):\n",
    "            if debug and (iter+1) % 1000 == 0:\n",
    "                print( 'iter ' + str(iter+1) )\n",
    "            \n",
    "            lnpgrad = lnprob(theta)\n",
    "            # calculating the kernel matrix\n",
    "            kxy, dxkxy = self.svgd_kernel(theta, h = bandwidth)  \n",
    "            grad_theta = (np.matmul(kxy, lnpgrad) + dxkxy) / x0.shape[0]  \n",
    "            \n",
    "            # adagrad \n",
    "            if iter == 0:\n",
    "                historical_grad = historical_grad + grad_theta ** 2\n",
    "            else:\n",
    "                historical_grad = alpha * historical_grad + (1 - alpha) * (grad_theta ** 2)\n",
    "            adj_grad = np.divide(grad_theta, fudge_factor+np.sqrt(historical_grad))\n",
    "            theta = theta + stepsize * adj_grad \n",
    "            \n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_ex = DistributionMover(n_particles=100, n_dims=20, n_hidden_dims=20, use_latent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dm_svgd = SVGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lnprob(particles):\n",
    "    grad_log_term = torch.zeros([particles.shape[0], particles.shape[1]], dtype=t_type, device=device)\n",
    "        \n",
    "    for idx in range(100):\n",
    "        particle = torch.tensor(particles[idx:idx+1], dtype=t_type, device=device, requires_grad=True)\n",
    "        log_term = torch.log(dm_ex.target_density(particle.t()))\n",
    "        grad_log_term[idx] = torch.autograd.grad(log_term, particle,\n",
    "                                                     only_inputs=True, retain_graph=False, \n",
    "                                                     create_graph=False, allow_unused=False)[0]\n",
    "    return grad_log_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = dm_ex.particles.data.clone().detach().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare kernel implementation\n",
    "\n",
    "ker, grad_ker = dm_ex.calc_kernel_term_latent(10.)\n",
    "print(dm_ex.h)\n",
    "grad_ker_sum = torch.sum(grad_ker, dim=1)\n",
    "\n",
    "ker_l, grad_ker_sum_l = dm_svgd.svgd_kernel(result, h=10.)\n",
    "\n",
    "print((ker - torch.tensor(ker_l, dtype=t_type, device=device)).norm())\n",
    "print((grad_ker_sum - torch.tensor(grad_ker_sum_l, dtype=t_type, device=device).t()).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Compare log_term implementation\n",
    "(lnprob(result) - dm_ex.calc_log_term_latent().t()).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(201): \n",
    "    try:\n",
    "        result = dm_svgd.update(result, lnprob, stepsize=1e-2, bandwidth=-1, n_iter=20)\n",
    "        result = torch.tensor(result, dtype=t_type, device=device)\n",
    "\n",
    "        if _ % 1 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm.h))\n",
    "\n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(result.data.cpu().numpy(), torch.zeros_like(result).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(result.data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(2000): \n",
    "    try:\n",
    "        dm_ex.update_latent(h_type=0)\n",
    "\n",
    "        if _ % 30 == 0:\n",
    "            clear_output()\n",
    "            sys.stdout.write('\\rEpoch {0}...\\nKernel factor {1}'.format(_, dm_ex.h))\n",
    "            \n",
    "            plt.plot(np.linspace(-10, 10, 1000, dtype=np.float64), pdf)\n",
    "            plt.plot(dm_ex.particles.t().data.cpu().numpy(), torch.zeros_like(dm_ex.particles.t()).data.cpu().numpy(), 'ro')\n",
    "            sns.kdeplot(dm_ex.particles.t().data.cpu().numpy()[:,0], \n",
    "                        kernel='tri', color='darkblue', linewidth=4)\n",
    "\n",
    "            plt.pause(1e-300)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(result - dm_ex.particles.t()).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression from original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"Stein-Variational-Gradient-Descent/python/\")\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy.matlib as nm\n",
    "from svgd import SVGD\n",
    "\n",
    "r\"\"\"\n",
    "    Example of Bayesian Logistic Regression (the same setting as Gershman et al. 2012):\n",
    "    The observed data D = {X, y} consist of N binary class labels, \n",
    "    y_t \\in {-1,+1}, and d covariates for each datapoint, X_t \\in R^d.\n",
    "    The hidden variables \\theta = {w, \\alpha} consist of d regression coefficients w_k \\in R,\n",
    "    and a precision parameter \\alpha \\in R_+. We assume the following model:\n",
    "        p(\\alpha) = Gamma(\\alpha; a, b)\n",
    "        p(w_k | a) = N(w_k; 0, \\alpha^-1)\n",
    "        p(y_t = 1| x_t, w) = 1 / (1+exp(-w^T x_t))\n",
    "\"\"\"\n",
    "class BayesianLR:\n",
    "    def __init__(self, X, Y, batchsize=100, a0=1, b0=0.01):\n",
    "        self.X, self.Y = X, Y\n",
    "        # TODO. Y in \\in{+1, -1}\n",
    "        self.batchsize = min(batchsize, X.shape[0])\n",
    "        self.a0, self.b0 = a0, b0\n",
    "        \n",
    "        self.N = X.shape[0]\n",
    "        self.permutation = np.random.permutation(self.N)\n",
    "        self.iter = 0\n",
    "    \n",
    "        \n",
    "    def dlnprob(self, theta):\n",
    "        \n",
    "        if self.batchsize > 0:\n",
    "            batch = [ i % self.N for i in range(self.iter * self.batchsize, (self.iter + 1) * self.batchsize) ]\n",
    "            ridx = self.permutation[batch]\n",
    "            self.iter += 1\n",
    "        else:\n",
    "            ridx = np.random.permutation(self.X.shape[0])\n",
    "            \n",
    "        Xs = self.X[ridx, :]\n",
    "        Ys = self.Y[ridx]\n",
    "        \n",
    "        w = theta[:, :-1]  # logistic weights\n",
    "        alpha = np.exp(theta[:, -1])  # the last column is logalpha\n",
    "        d = w.shape[1]\n",
    "        \n",
    "        wt = np.multiply((alpha / 2), np.sum(w ** 2, axis=1))\n",
    "        \n",
    "        coff = np.matmul(Xs, w.T)\n",
    "        y_hat = 1.0 / (1.0 + np.exp(-1 * coff))\n",
    "        \n",
    "        dw_data = np.matmul(((nm.repmat(np.vstack(Ys), 1, theta.shape[0]) + 1) / 2.0 - y_hat).T, Xs)  # Y \\in {-1,1}\n",
    "        dw_prior = -np.multiply(nm.repmat(np.vstack(alpha), 1, d) , w)\n",
    "        dw = dw_data * 1.0 * self.X.shape[0] / Xs.shape[0] + dw_prior  # re-scale\n",
    "        \n",
    "        dalpha = d / 2.0 - wt + (self.a0 - 1) - self.b0 * alpha + 1  # the last term is the jacobian term\n",
    "        \n",
    "        return np.hstack([dw, np.vstack(dalpha)])  # % first order derivative \n",
    "    \n",
    "    def evaluation(self, theta, X_test, y_test):\n",
    "        theta = theta[:, :-1]\n",
    "        M, n_test = theta.shape[0], len(y_test)\n",
    "\n",
    "        prob = np.zeros([n_test, M])\n",
    "        for t in range(M):\n",
    "            coff = np.multiply(y_test, np.sum(-1 * np.multiply(nm.repmat(theta[t, :], n_test, 1), X_test), axis=1))\n",
    "            prob[:, t] = np.divide(np.ones(n_test), (1 + np.exp(coff)))\n",
    "        \n",
    "        prob = np.mean(prob, axis=1)\n",
    "        acc = np.mean(prob > 0.5)\n",
    "        llh = np.mean(np.log(prob))\n",
    "        return [acc, llh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N = X_test.shape[0] + X_train.shape[0]\n",
    "X_input = np.hstack([X, np.ones([N, 1])])\n",
    "y_input = y\n",
    "d = X_input.shape[1]\n",
    "D = d + 1\n",
    "    \n",
    "# split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.2, random_state=42)\n",
    "    \n",
    "a0, b0 = 1, 0.01 #hyper-parameters\n",
    "model = BayesianLR(X_train, y_train, 100, a0, b0) # batchsize = 100\n",
    "    \n",
    "# initialization\n",
    "M = 100  # number of particles\n",
    "theta0 = np.zeros([M, D]);\n",
    "alpha0 = np.random.gamma(a0, b0, M); \n",
    "for i in range(M):\n",
    "    theta0[i, :] = np.hstack([np.random.normal(0, np.sqrt(1 / alpha0[i]), d), np.log(alpha0[i])])\n",
    "    \n",
    "theta = SVGD().update(x0=theta0, lnprob=model.dlnprob, bandwidth=-1, n_iter=6000, stepsize=0.05, alpha=0.9, debug=True)\n",
    "    \n",
    "print('[accuracy, log-likelihood]')\n",
    "print(model.evaluation(theta, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Metropolis–Hastings algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MHAlgorithm():\n",
    "    def __init__(self, n_dims): \n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        self.target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs) +\n",
    "                                                           0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True).unnormed_density(x, *args, **kwargs))\n",
    "\n",
    "        self.real_target_density = lambda x, *args, **kwargs : (0.3 * normal_density(self.n_dims, -2., 1., n_particles_second=True)(x, *args, **kwargs) +\n",
    "                                                                0.7 * normal_density(self.n_dims, 2., 1., n_particles_second=True)(x, *args, **kwargs))\n",
    "        \n",
    "#         self.target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "#         self.real_target_density = lambda x : (normal_density(self.n_dims, 0., 2., n_particles_second=True)(x))\n",
    "\n",
    "    \n",
    "        self.particles = torch.zeros(\n",
    "            [self.n_dims, 1],\n",
    "            dtype=t_type,\n",
    "            requires_grad=False,\n",
    "            device=device).uniform_(-2., -1.)\n",
    "        \n",
    "        self.one = torch.tensor(1., dtype=t_type, device=device)\n",
    "        \n",
    "    def generate_next(self):\n",
    "        previous = self.particles[:, -1].unsqueeze(1)\n",
    "#         self.additional_distribution = normal_density(self.n_dims, previous, 1, n_particles_second=True)\n",
    "#         candidate = self.additional_distribution.get_sample()\n",
    "        \n",
    "#         alpha = (self.target_density(candidate) * self.additional_distribution.unnormed_density(previous) / \n",
    "#                 (self.target_density(previous) * self.additional_distribution.unnormed_density(candidate)))\n",
    "        candidate = previous + torch.zeros([self.n_dims, 1], device=device, dtype=t_type).uniform_(-1., 1.)\n",
    "        alpha = self.target_density(candidate) / self.target_density(previous)\n",
    "        \n",
    "#         print(self.target_density(candidate)[0].data.numpy(), self.additional_distribution.unnormed_density(previous)[0].data.numpy(),\n",
    "#               self.target_density(previous)[0].data.numpy(), self.additional_distribution.unnormed_density(candidate)[0].data.numpy())\n",
    "        \n",
    "        if alpha > self.one:\n",
    "            self.particles = torch.cat([self.particles, candidate], dim=1)\n",
    "        else:\n",
    "            random = torch.bernoulli(alpha)\n",
    "            if random:\n",
    "                self.particles = torch.cat([self.particles, candidate], dim=1)                \n",
    "            else:\n",
    "                self.particles = torch.cat([self.particles, previous], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mh = MHAlgorithm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    mh.generate_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(mh.particles.t().data.cpu().numpy()[:,1], \n",
    "            kernel='tri', color='darkblue', linewidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(mh.particles.t().data.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import time\n",
    "\n",
    "mu,sig,N = 0,1,1000000\n",
    "pts = []\n",
    "\n",
    "def q(x):\n",
    "#     return (1/(math.sqrt(2*math.pi*sig**2)))*(math.e**(-((x-mu)**2)/(2*sig**2)))\n",
    "#     return normal_density(1, 0., 2., n_particles_second=True).unnormed_density(x)\n",
    "    return (0.3 * normal_density(1, -2., 1., n_particles_second=True).unnormed_density(x) +\n",
    "            0.7 * normal_density(1, 2., 1., n_particles_second=True).unnormed_density(x))\n",
    "\n",
    "def metropolis(N):\n",
    "    r = np.zeros(1)\n",
    "    p = q(r[0])\n",
    "    pts = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        rn = r + np.random.uniform(-1,1)\n",
    "        pn = q(rn[0])\n",
    "        if pn >= p:\n",
    "            p = pn\n",
    "            r = rn\n",
    "        else:\n",
    "            u = np.random.rand()\n",
    "            if u < pn/p:\n",
    "                p = pn\n",
    "                r = rn\n",
    "        pts.append(r)\n",
    "    \n",
    "    pts = np.array(pts)\n",
    "    return pts\n",
    "    \n",
    "def hist_plot(array):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1,)\n",
    "    ax.hist(array, bins=1000)    \n",
    "    plt.title('')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot(metropolis(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gg =  lambda : (normal_density(1, -60., 20., n_particles_second=True).get_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist_plot([gg()[0,0] for _ in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

%% -*- coding: utf-8 -*-
\documentclass[12pt,a4paper]{scrartcl} 
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{indentfirst}
\usepackage{misccorr}
\usepackage{graphicx}
\usepackage{amsmath}
\begin{document}
	
\begin{section}{Постановка задачи}	
	Целью работы было оценить практическую применимость статьи \cite{AddingOne}. Были выбранны следующие критерии оценки:
\begin{enumerate}
	\item Стабильность обучения
	\item Качество обучения
	\item Скорость сходимости сети
	\item Обобщающая способность
	\item Влияние гипер параметра (регуляризующий параметр $\lambda$) на пункты 1-4.
\end{enumerate}

	Для проведения тестов использовались следующие датасеты:
\begin{enumerate}
	\item 2 концентрические, 100-мерные сферы различного радиуса.
	\item 2 100-мерных набора семплов из нормальных распределений с одинаковыми дисперсиями и разными средними.
	\item Подмножество датасета CIFAR (классы 0 и 1). Размер датасета: 2000 - train, 2000 - test, 2000 - validation.
	\item Подмножество датасета MNIST (цифры 1 и 7). Размер датасета: 13000 - train, 2000 - test, 2000 - validation.
\end{enumerate}

\end{section}

\begin{section}{Стабильность}
	В ходе обучения сети с экспоненциальным нейроном проявлялся эффект нестабильности обучения - в любой момент ошибка на обучающей выборке может "взорваться" и, затем, как следствие, расходятся градиенты и веса сети. Причина этого в чрезвычайной чувствительности экспоненциального нейрона к малым изменениям весов в нём. Это подтверждает такой эксперимент: к весам нейрона добавляется нормальный шум из $N(0, \sigma)$, где $\sigma$ ~ в 10 раз меньше по величине чем среднее значение модулей весов в нейроне. Такое изменение экспоненциального нейрона сразу приводит к расхождению сети, хотя в случае нейрона с не экспоненциальной функцией активации (ReLU, Sigmoid, ...) такого не происходит. Частично эту проблему удаётся решить нормализацией входных данных, за счёт чего на ранних итерациях обучения не происходит "взрыва" градиентов. Так же, уменьшение learning rate в процессе обучения уменьшает частоту таких ситуаций. Однако полностью проблему решить не удалось.
\end{section}

\begin{section}{Качество}
	Основным результатом статьи \cite{AddingOne} было доказательство отсутствия локальных минимумов у получившейся сети. Было выдвинуто предположение, что существуют задачи, в которых обучение сети без экспоненциального приводит к "застреванию" в локальном минимуме, в то время как добавление экспоненциального нейрона приведёт к попаданию в глобальный минимум и, как следствие лучшему качеству на обучающей выборке. Однако, проведённые эксперименты показывают неоднозначную картину:
	\begin{center}
		\begin{tabular}{c|c|c|c|c}
			Датасет & Train Size & Без эксп. нейрона & С эксп. нейроном & Базовая архитектура\\
			\hline
			2 сферы, Радиусы $1.0$ и $1.01$     & $10000$  & $0.000 \backslash 0$     & $0.000 \backslash 0$     & \\
			2 сферы, Радиусы $1.0$ и $1.01$     & $15000$  & $44.3 \backslash 7$      & $3693.2 \backslash 1096$ & \\
			2 сферы, Радиусы $1.0$ и $1.01$     & $20000$  & $8311.8 \backslash 2338$ & $335.8 \backslash 35$    & \\
			2 гауссианы, Средние $0.0$ и $0.1$  & $1000$   & $0.000 \backslash 0$     & $0.000 \backslash 0$     & \\
			MNIST                               & $13000$  & $0.000 \backslash 0$     & $0.000 \backslash 0$     & \\
			CIFAR                               & $2000$   & $0.431 \backslash 1$     & $0.994 \backslash 2$     & \\
		\end{tabular}
	\end{center}
	При проведении экспериментов возникли следующие сложности:
	\begin{enumerate}
		\item На реальных данных (MNIST/CIFAR) задача бинарной классификации решается с 0 missclassification rate даже сетью с очень маленьким числом обучаемых параметров и, как следствие такие датасеты нельзя использовать для сравнения качества. Поэтому для оценки приходилось использовать датасеты со сложноразделимыми классами с большим числом объектов. Как следствие, обученные сети, по сути, переобучались, запоминая примеры обучающей выборки на весах и говорить о том, что в реальных задачах результаты будут аналогичны нельзя.
		\item Чтобы обучить сеть с эксп. нейроном приходилось подбирать стратегию уменьшения learning rate, т.к. в ином случае сеть рано или поздно расходилась.
	\end{enumerate}
\end{section}

\begin{thebibliography}{9} 
	\bibitem{AddingOne} Shiyu Liang, Ruoyu Sun, Jason D. Lee, R. Srikant, Adding One Neuron Can Eliminate All Bad Local Minima, arXiv:1805.08671
\end{thebibliography}


\end{document}

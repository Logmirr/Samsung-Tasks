\documentclass[a4paper,12pt,titlepage,finall]{article}

\usepackage{bm}
\usepackage{cmap}
\usepackage{tikz}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{pgfplots}
\usepackage{algorithm}
\usepackage{csvsimple}
\usepackage{indentfirst}         % для отступа в первом абзаце секции
\usepackage[T2A]{fontenc}
\usepackage{algpseudocode}
\usepackage[utf8x]{inputenc}
\usepackage[margin=2cm]{geometry}		 % для настройки размера полей
\usepackage[english,russian]{babel}

\newcommand{\lt}{\textless}    % знак меньше
\newcommand{\gt}{\textgreater} % знак больше

\usetikzlibrary{calc}

\pgfplotsset{compat=1.11}

\DeclareMathOperator*{\EE}{\mathbb{E}}% для обозначения матожидания
\DeclareMathOperator*{\PP}{\mathbb{P}} % для обозначения вероятности

% выбираем размер листа А4, все поля ставим по 2см
\geometry{a4paper,left=20mm,top=20mm,bottom=20mm,right=20mm}

\setcounter{secnumdepth}{0}      % отключаем нумерацию секций

\begin{document}
	
\begin{section}{Постановка задачи}	
	Целью работы было оценить практическую применимость статьи \cite{AddingOne}. Были выбранны следующие критерии оценки:
\begin{enumerate}
	\item Стабильность обучения.
	\item Качество обучения.
	\item Скорость сходимости сети.
	\item Обобщающая способность.
\end{enumerate}

	Для проведения тестов использовались следующие датасеты:
\begin{enumerate}
	\item 2 концентрические, 100-мерные сферы различного радиуса.
	\item 2 100-мерных набора сэмплов из нормальных распределений с одинаковыми дисперсиями и разными средними.
	\item Подмножество датасета CIFAR (классы 0 и 1). Размер датасета: 2000 - train, 2000 - test, 2000 - validation.
	\item Подмножество датасета MNIST (цифры 1 и 7). Размер датасета: 13000 - train, 2000 - test, 2000 - validation.
\end{enumerate}

\end{section}

\begin{section}{Стабильность}
	В ходе обучения сети с экспоненциальным нейроном (ЭН) проявлялся эффект нестабильности обучения - в любой момент ошибка на обучающей выборке может "взорваться" и затем, как следствие, расходятся градиенты и веса сети. Причина этого в чрезвычайной чувствительности ЭН к малым изменениям весов в нём. Это подтверждает такой эксперимент: к весам нейрона добавляется нормальный шум из $N(0, \sigma)$, где $\sigma$ ~ в 10 раз меньше по величине чем среднее значение модулей весов в нейроне. Такое изменение ЭН сразу приводит к расхождению сети, хотя в случае нейрона с не экспоненциальной функцией активации (ReLU, Sigmoid, ...) такого не происходит. Частично проблему "взрыва" градиента удаётся решить нормализацией входных данных, за счёт чего на ранних итерациях обучения не происходит расхождения сети. Так же, уменьшение learning rate с числом пройденых итераций делает обучение стабильнее. Однако полностью проблему решить не удалось.
\end{section}

\begin{section}{Качество}
	Основным результатом статьи \cite{AddingOne} было доказательство отсутствия локальных минимумов у получившейся сети. Было выдвинуто предположение, что существуют задачи, в которых обучение сети без ЭН приводит к "застреванию" в локальном минимуме, в то время как добавление ЭН приведёт к попаданию в глобальный минимум и, как следствие лучшему качеству на обучающей выборке. Однако, проведённые эксперименты эту гипотезу не доказывают:
\begin{table}[H]
	\begin{center}
		\addtolength{\leftskip} {-2cm}
		\addtolength{\rightskip}{-2cm}
		\begin{tabular}{c|c|c|c|c}
			Датасет & Train Size & Без ЭН & С ЭН & Архитектура сети\\
			\hline
			Сферы, $R_{1}=1.0, R_{2}=1.01$         & $10000$  & $0.000 \backslash 0$     & $0.000 \backslash 0$     & \ref{tabular:4} \\
			Сферы, $R_{1}=1.0, R_{2}=1.01$         & $15000$  & $44.3 \backslash 7$      & $3693.2 \backslash 1096$ & \ref{tabular:4} \\
			Сферы, $R_{1}=1.0, R_{2}=1.01$         & $20000$  & $8311.8 \backslash 2338$ & $335.8 \backslash 35$    & \ref{tabular:4} \\
			Гауссианы, $\EE_{1}=0.0, \EE_{2}=0.1$  & $15000$  & $0.000 \backslash 0$     & $0.000 \backslash 0$     & \ref{tabular:4} \\
			Гауссианы, $\EE_{1}=0.0, \EE_{2}=0.1$  & $20000$  & $41.0 \backslash 0$      & $53.9 \backslash 0$      & \ref{tabular:4} \\
			MNIST                                  & $13000$  & $0.000 \backslash 0$     & $0.000 \backslash 0$     & \ref{tabular:6} \\
			CIFAR                                  & $2000$   & $0.431 \backslash 1$     & $0.994 \backslash 2$     & \ref{tabular:5} \\
		\end{tabular}
		
		\caption{\label{tabular:1} Результаты обучения. Loss $\backslash$ misclassification rate.}
	\end{center}
\end{table}
	Как видно из \ref{tabular:1} во всех случаях кроме одного добавление ЭН не улучшает качество на обучающей выборке, хотя в некоторых случаях может наблюдаться значительное улучшение качества.\\
	При проведении экспериментов возникли следующие сложности:
	\begin{enumerate}
		\item На реальных данных (MNIST/CIFAR) задача бинарной классификации решается с 0 misclassification rate даже сетью с очень маленьким числом обучаемых параметров и, как следствие такие датасеты нельзя использовать для сравнения качества. Поэтому для оценки приходилось использовать датасеты со трудноразделимыми классами с большим числом объектов. Как следствие, обученные сети, по сути, переобучались, запоминая примеры обучающей выборки на весах, и говорить о том, что в реальных задачах результаты будут аналогичны - нельзя.
		\item Чтобы обучить сеть с ЭН приходилось подбирать параметр $\lambda$ и стратегию уменьшения learning rate , т.к. в ином случае сеть рано или поздно расходилась.
	\end{enumerate}
\end{section}

\begin{section}{Скорость обучения}
	Так как и сеть без ЭН, и с ЭН зависят от гипер параметров (learning rate, reg. lambda), то для сравнения скорости сходимости использовался следующий подход: гипер параметры перебираются из заданного множества и для каждой их комбинации считается число итераций до достижения 0 misclassification rate. Лучший результат (наименьшее число итераций) определяет скорость сходимости сети.
\begin{table}[H]
	\begin{center}
		\addtolength{\leftskip} {-2cm}
		\addtolength{\rightskip}{-2cm}
		\begin{tabular}{c|c|c|c|c}
			Датасет & Train Size & Без ЭН & С ЭН & Архитектура сети\\
			\hline
			Сферы, $R_{1}=1.0, R_{2}=1.01$         & $1000$   & $20$   & $23$     & \ref{tabular:4} \\
			Сферы, $R_{1}=1.0, R_{2}=1.01$         & $5000$   & $28$   & $35$     & \ref{tabular:4} \\
			Сферы, $R_{1}=1.0, R_{2}=1.01$         & $10000$  & $77$   & $126$    & \ref{tabular:4} \\
			Гауссианы, $\EE_{1}=0.0, \EE_{2}=0.1$  & $10000$  & $45$   & $67$     & \ref{tabular:4} \\
			Гауссианы, $\EE_{1}=0.0, \EE_{2}=0.1$  & $15000$  & $95$   & $131$    & \ref{tabular:4} \\
			MNIST                                  & $13000$  & $15$   & $20$     & \ref{tabular:6} \\
			CIFAR                                  & $2000$   & $20$   & $23$     & \ref{tabular:5} \\
		\end{tabular}
		\caption{\label{tabular:2} Скорость обучения. Количество итераций до сходимости.}
	\end{center}
\end{table}
	Как видно из \ref{tabular:2} добавление ЭН не улучшает скорость сходимости сети, а в некоторых случаях - ухудшает.
\end{section}

\begin{section}{Обобщающая способность}
	Для оценки обобщающей способности был проведён следующий эксперимент: аналогично предыдущей секции, перебирались всевозможные комбинации гипер параметров. Для каждой комбинации, на которой за фиксированное число итераций на обучающей выборке достигался нулевой misclassification rate, вычислялась ошибка на тестирующей выборке. Затем, для набора гипер параметров, на котором достигнут минимум ошибки на тестирующей выборке вычислялась ошибка на валидационном множестве. Эта ошибка и считалась характеристикой обобщающей способности сети.
	\begin{table}[H]
		\begin{center}
			\addtolength{\leftskip} {-2cm}
			\addtolength{\rightskip}{-2cm}
			\begin{tabular}{c|c|c|c|c}
				Датасет & Train Size & Без ЭН & С ЭН & Архитектура сети\\
				\hline
				Сферы, $R_{1}=1.0, R_{2}=1.5$          & $5000$   & $2684.8 \backslash 658$ & $6901.4 \backslash 796$  & \ref{tabular:4} \\
				Сферы, $R_{1}=1.0, R_{2}=5.0$          & $5000$   & $678.1 \backslash 140$  & $2351.7 \backslash 188$  & \ref{tabular:4} \\
				Гауссианы, $\EE_{1}=0.0, \EE_{2}=0.5$  & $5000$   & $384.2 \backslash 56$   & $788.6 \backslash 63$    & \ref{tabular:4} \\
				Гауссианы, $\EE_{1}=0.0, \EE_{2}=1.0$  & $5000$   & $1.1 \backslash 0$      & $4.6 \backslash 0$       & \ref{tabular:4} \\
				MNIST                                  & $13000$  & $0.2 \backslash 5$      & $0.6 \backslash 4$       & \ref{tabular:6} \\
				CIFAR                                  & $2000$   & $3901.5 \backslash 314$ & $4516.7 \backslash 313$  & \ref{tabular:5} \\
			\end{tabular}
			\caption{\label{tabular:3} Результаты обучения. Loss $\backslash$ misclassification rate.}
		\end{center}
	\end{table}
	Как видно из \ref{tabular:3}, добавление ЭН только ухудшило обобщающую способность.
\end{section}

\begin{section}{Выводы}
	Проведённые эксперименты показывают, что добавление ЭН не позволяет улучшить ни одну из характеристик, которую обычно оптимизируют при использовании машинного обучения (ни качество на тестовой и обучающей выборках, ни скорость обучения), а с учётом значительного ухудшения стабильности обучения и необходимостью дополнительно подбирать оптимальное значение ещё одного гипер параметра $\lambda$ делает практическое применение данной модификации невыгодным.
\end{section}
	
\begin{thebibliography}{9} 
	\bibitem{AddingOne} Shiyu Liang, Ruoyu Sun, Jason D. Lee, R. Srikant, Adding One Neuron Can Eliminate All Bad Local Minima, arXiv:1805.08671
\end{thebibliography}

\begin{section}{Приложение 1}
\begin{subsection}{Архитектура сетей}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{ccc}
			\hline
			Название модуля & Входной размер & Выходной размер \\
			\hline
			$X$ & --- & $100 \times 1$ \\
			Linear & $100 \times 1$ & $50 \times 1$ \\
			Sigmoid & & \\
			Linear & $50 \times 1$ & $25 \times 1$ \\
			Sigmoid & & \\
			Linear & $25 \times 1$ & $1 \times 1$ \\
			\hline
		\end{tabular}
		\caption{\label{tabular:4}Архитектура полносвязной сети 1}
	\end{table}	
	
	\begin{table}[H]
		\centering
		\begin{tabular}{ccc}
			\hline
			Название модуля & Входной размер & Выходной размер \\
			\hline
			$X$ & --- & $3072 \times 1$ \\
			Linear & $3072 \times 1$ & $1536 \times 1$ \\
			Sigmoid & & \\
			Linear & $1536 \times 1$ & $768 \times 1$ \\
			Sigmoid & & \\
			Linear & $768 \times 1$ & $1 \times 1$ \\
			\hline
		\end{tabular}
		\caption{\label{tabular:5}Архитектура полносвязной сети 2}
	\end{table}
	
	\begin{table}[H]
		\centering
		\begin{tabular}{cccccc}
			\hline
			Название модуля & Входной размер & Выходной размер & Kernel Size & Stride & Padding \\
			\hline
			$X$ & --- & $1\times28\times28$ & & & \\
			Conv & $1\times28\times28$ & $64\times27\times27$ & $4\times4$ & 0 & 1 \\
			PReLU & & & & & \\
			MaxPool & $64\times27\times27$ & $64\times9\times9$ & $3$ & & \\
			Conv & $64\times9\times9$ & $128\times8\times8$ & $4\times4$ & 0 & 1 \\
			PReLU & & & & & \\
			MaxPool & $128\times8\times8$ & $128\times2\times2$ & $3$ & & \\
			Conv & $128\times2\times2$ & $256\times1\times1$ & $4\times4$ & 0 & 1 \\
			PReLU & & & & & \\
			Linear & $256\times1$ & $1\times1$ & & & \\	
			\hline
		\end{tabular}
		\caption{\label{tabular:6}Архитектура свёрточной сети}
	\end{table}
	
\end{subsection}
\end{section}	

\end{document}

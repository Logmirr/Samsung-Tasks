{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "use_cuda = False\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.device(\"cuda:7\")\n",
    "    use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/m.nakhodnov/Samsung-Tasks/Adding_one_neuron/utils\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MNIST_Class_Selection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-30888a001017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utils/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mMyUtils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mMyDatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Samsung-Tasks/Adding_one_neuron/utils/MyDatasets.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# import MNIST dataset with class selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./MyMNIST/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mMyMNIST\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMNIST_Class_Selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MNIST_Class_Selection'"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, 'utils/')\n",
    "from MyUtils import *\n",
    "from MyDatasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'utils/MyMNIST/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyMNIST import MNIST_Class_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_WITH_EXP(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(Classifier_WITH_EXP, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size // 2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_size // 2, in_size // 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_size // 4, 1)\n",
    "        )\n",
    "        self.explinear = ExpLinear(in_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x).view(-1) + self.explinear(x).view(-1)\n",
    "    \n",
    "    # get parameter alpha from explinear block\n",
    "    def get_alpha(self):\n",
    "        return self.explinear.get_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size // 2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_size // 2, in_size // 4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_size // 4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.linear(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_c_train = TwoCircleDataset(1, 1.5, 5000, 100, train=True)\n",
    "dataset_c_test = TwoCircleDataset(1, 1.5, 5000, 100, train=False)\n",
    "dataset_c_validation = TwoCircleDataset(1, 1.5, 5000, 100, train=False)\n",
    "dataloader_c_train = DataLoader(dataset_c_train, batch_size=128, shuffle=True)\n",
    "dataloader_c_test = DataLoader(dataset_c_test, batch_size=128, shuffle=True)\n",
    "dataloader_c_validation = DataLoader(dataset_c_validation, batch_size=128, shuffle=True)\n",
    "\n",
    "dataset_g_train = TwoGaussiansDataset(0., 0.5, 5000, 100, train=True)\n",
    "dataset_g_test = TwoGaussiansDataset(0., 0.5, 5000, 100, train=False)\n",
    "dataset_g_validation = TwoGaussiansDataset(0., 0.5, 5000, 100, train=False)\n",
    "dataloader_g_train = DataLoader(dataset_g_train, batch_size=128, shuffle=True)\n",
    "dataloader_g_test = DataLoader(dataset_g_test, batch_size=128, shuffle=True)\n",
    "dataloader_g_validation = DataLoader(dataset_g_validation, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(lr_split, rl_split, max_missclass_error, net_class, *args, **kwargs):\n",
    "    lr_best = 0\n",
    "    rl_best = 0\n",
    "    err_best = 10e10\n",
    "    for lr in lr_split:\n",
    "        for rl in rl_split:\n",
    "            net = net_class(*args)\n",
    "            if use_cuda:\n",
    "                net = net.cuda()\n",
    "            res = train_(network=net, loss_func=kwargs['loss_func'],\n",
    "                         learning_rate=lr, reinit_optim=kwargs['reinit_optim'],\n",
    "                         train_strategy=kwargs['train_strategy'], test_strategy=kwargs['test_strategy'],\n",
    "                         reg_lambda=rl,\n",
    "                         epochs=kwargs['epochs'],\n",
    "                         dataloader_train=kwargs['dataloader_train'], dataloader_test=kwargs['dataloader_test'],\n",
    "                         plot_graphs=False, verbose=True,\n",
    "                         epoch_hook=kwargs['epoch_hook']() if 'epoch_hook' in kwargs else None\n",
    "                        )\n",
    "            \n",
    "            missclass_err_test = res[3][-1]\n",
    "            missclass_err_train = res[1][-1]\n",
    "            \n",
    "            if missclass_err_test < err_best and missclass_err_train <= max_missclass_error:\n",
    "                err_best = missclass_err_test\n",
    "                lr_best = lr\n",
    "                rl_best = rl\n",
    "    return lr_best, rl_best, err_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two N-dim concentric spheres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Exp Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(np.logspace(-4, 1, 10), [None], 0, Classifier, 100, \n",
    "            loss_func=loss_func,\n",
    "            reinit_optim=1000,\n",
    "            train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "            epochs=200,\n",
    "            dataloader_train=dataloader_c_train, dataloader_test=dataloader_c_test,\n",
    "#             epoch_hook=convergence_analysys_hook\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_c = Classifier(100)\n",
    "if use_cuda:\n",
    "    net_c = net_c.cuda()\n",
    "\n",
    "train_(network=net_c, loss_func=loss_func,\n",
    "       learning_rate=0.0021, reinit_optim=1500,\n",
    "       train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "       reg_lambda=None,\n",
    "       epochs=300,\n",
    "       dataloader_train=dataloader_c_train, dataloader_test=dataloader_c_validation,\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Exp Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(np.logspace(-3, 1, 8), np.logspace(-4, 1, 10), 0, Classifier_WITH_EXP, 100, \n",
    "            loss_func=loss_func_EXP,\n",
    "            reinit_optim=1000,\n",
    "            train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "            epochs=200,\n",
    "            dataloader_train=dataloader_c_train, dataloader_test=dataloader_c_test,\n",
    "#             epoch_hook=convergence_analysys_hook\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_c_e = Classifier_WITH_EXP(100)\n",
    "if use_cuda:\n",
    "    net_c_e = net_c_e.cuda()\n",
    "net_c_e.explinear.init_weigth(0, 0.001)\n",
    "\n",
    "train_EXP(network=net_c_e, loss_func=loss_func_EXP,\n",
    "          learning_rate=0.0517, reinit_optim=400,\n",
    "          train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "          reg_lambda=2.78,\n",
    "          epochs=100,\n",
    "          dataloader_train=dataloader_c_train, dataloader_test=dataloader_c_validation\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Exp Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(np.logspace(-4, 1, 10), [None], 0, Classifier, 100, \n",
    "            loss_func=loss_func,\n",
    "            reinit_optim=1000,\n",
    "            train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "            epochs=200,\n",
    "            dataloader_train=dataloader_g_train, dataloader_test=dataloader_g_test,\n",
    "#             epoch_hook=convergence_analysys_hook\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_g = Classifier(100)\n",
    "if use_cuda:\n",
    "    net_g = net_g.cuda()\n",
    "\n",
    "train_(network=net_g, loss_func=loss_func,\n",
    "       learning_rate=0.0001, reinit_optim=1000,\n",
    "       train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "       reg_lambda=None,\n",
    "       epochs=200,\n",
    "       dataloader_train=dataloader_g_train, dataloader_test=dataloader_g_validation,\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Exp Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(np.logspace(-3, 1, 5), np.logspace(-3, 1, 5), 0, Classifier_WITH_EXP, 100, \n",
    "            loss_func=loss_func_EXP,\n",
    "            reinit_optim=1000,\n",
    "            train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "            epochs=200,\n",
    "            dataloader_train=dataloader_g_train, dataloader_test=dataloader_g_test,\n",
    "#             epoch_hook=convergence_analysys_hook\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_g_e = Classifier_WITH_EXP(100)\n",
    "if use_cuda:\n",
    "    net_g_e = net_g_e.cuda()\n",
    "\n",
    "    \n",
    "train_EXP(network=net_g_e, loss_func=loss_func_EXP,\n",
    "          learning_rate=0.001, reinit_optim=1000,\n",
    "          train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "          reg_lambda=0.001,\n",
    "          epochs=600,\n",
    "          dataloader_train=dataloader_g_train, dataloader_test=dataloader_g_validation\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(dataset, classes = None):\n",
    "    if classes is None:\n",
    "        classes = get_pure(dataset.classes)\n",
    "    colors = np.where(classes <= 0, 'k', 'b')\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.xlim([-1, 1])\n",
    "    plt.ylim([-1, 1])\n",
    "    plt.scatter(get_pure(dataset.dots[:, 0]), get_pure(dataset.dots[:, 1]), linewidth=0, c=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(dataset_c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(dataset_c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(dataset_c_test, get_pure(net_c(dataset_c_test.dots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(dataset_c_test, get_pure(net_c_e(dataset_c_test.dots)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "dataset_m_train = MNIST_Class_Selection('.', train=True, download=True, transform=transform, class_nums=set([1, 7]))\n",
    "dataset_m_test = MNIST_Class_Selection('.', train=False, transform=transform, class_nums=set([1, 7]))\n",
    "\n",
    "\n",
    "dataloader_m_train = DataLoader(dataset_m_train, batch_size=32, shuffle=True)\n",
    "dataloader_m_test = DataLoader(dataset_m_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvClassifier, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Conv2d(64, 128, 4, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Conv2d(128, 256, 4, padding=1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear_layers(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvClassifier_WITH_EXP(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(ConvClassifier_WITH_EXP, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Conv2d(64, 128, 4, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Conv2d(128, 256, 4, padding=1),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.explinear = ExpLinear(in_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.explinear(x.view(x.shape[0], -1)).view(-1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.linear_layers(x).view(-1) + y\n",
    "    \n",
    "    # get parameter alpha from explinear block\n",
    "    def get_alpha(self):\n",
    "        return self.explinear.get_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(np.logspace(-4, 1, 10), [None], 0, ConvClassifier, \n",
    "            loss_func=loss_func,\n",
    "            reinit_optim=1000,\n",
    "            train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "            epochs=22,\n",
    "            dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "            epoch_hook=convergence_analysys_hook\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_m = ConvClassifier()\n",
    "if use_cuda:\n",
    "    net_m = net_m.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_(network=net_m, loss_func=loss_func,\n",
    "       learning_rate=0.0001, reinit_optim=100,\n",
    "       train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "       reg_lambda=None,\n",
    "       epochs=2,\n",
    "       dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "       )\n",
    "train_(network=net_m, loss_func=loss_func,\n",
    "       learning_rate=0.0001, reinit_optim=100,\n",
    "       train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "       reg_lambda=None,\n",
    "       epochs=20,\n",
    "       dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "       #epoch_hook=convergence_analysys_hook()\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(np.logspace(-3, 1, 5), np.logspace(-3, 1, 5), 0, ConvClassifier_WITH_EXP, 28 * 28, \n",
    "            loss_func=loss_func_EXP,\n",
    "            reinit_optim=1000,\n",
    "            train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "            epochs=22,\n",
    "            dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "            epoch_hook=convergence_analysys_hook\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_m_e = ConvClassifier_WITH_EXP(28 * 28)\n",
    "if use_cuda:\n",
    "    net_m_e = net_m_e.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_EXP(network=net_m_e, loss_func=loss_func_EXP,\n",
    "          learning_rate=0.001, reinit_optim=100,\n",
    "          train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "          reg_lambda=0.005,\n",
    "          epochs=35,\n",
    "          dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "          epoch_hook=convergence_analysys_hook()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierAug(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(ClassifierAug, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size // 2),\n",
    "            AugActivation(nn.Sigmoid()),\n",
    "            nn.Linear(in_size // 2, in_size // 4),\n",
    "            AugActivation(nn.Sigmoid()),\n",
    "            nn.Linear(in_size // 4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.linear(x).view(-1)\n",
    "    \n",
    "    def get_alpha(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_m_fc_a = ClassifierAug(28 * 28)\n",
    "if use_cuda:\n",
    "    net_m_fc_a = net_m_fc_a.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_EXP(network=net_m_fc_a, loss_func=loss_func_AUG,\n",
    "          learning_rate=0.001, reinit_optim=100,\n",
    "          train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "          reg_lambda=0.00005,\n",
    "          epochs=30,\n",
    "          dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_m_fc = Classifier(28 * 28)\n",
    "if use_cuda:\n",
    "    net_m_fc = net_m_fc.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_(network=net_m_fc, loss_func=loss_func,\n",
    "       learning_rate=0.001, reinit_optim=10,\n",
    "       train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "       reg_lambda=None,\n",
    "       epochs=30,\n",
    "       dataloader_train=dataloader_m_train, dataloader_test=dataloader_m_test,\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_CIF_train = CIFARDataset(train=True)\n",
    "dataset_CIF_test = CIFARDataset(train=False)\n",
    "dataset_CIF_validation = CIFARDataset(train=False)\n",
    "dataloader_CIF_train = DataLoader(dataset_CIF_train, batch_size=128, shuffle=True)\n",
    "dataloader_CIF_test = DataLoader(dataset_CIF_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(np.logspace(-4, 1, 10), [None], 0, Classifier, 32 * 32 * 3,\n",
    "            loss_func=loss_func,\n",
    "            reinit_optim=1000,\n",
    "            train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "            epochs=200,\n",
    "            dataloader_train=dataloader_CIF_train, dataloader_test=dataloader_CIF_test,\n",
    "            epoch_hook=convergence_analysys_hook\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_CIF_fc = Classifier(32 * 32 * 3)\n",
    "if use_cuda:\n",
    "    net_CIF_fc = net_CIF_fc.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_(network=net_CIF_fc, loss_func=loss_func,\n",
    "       learning_rate=0.0004, reinit_optim=1000,\n",
    "       train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "       reg_lambda=None,\n",
    "       epochs=200,\n",
    "       dataloader_train=dataloader_CIF_train, dataloader_test=dataloader_CIF_test,\n",
    "       #epoch_hook=convergence_analysys_hook()\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_CIF_fc_a = ClassifierAug(32 * 32 * 3)\n",
    "if use_cuda:\n",
    "    net_CIF_fc_a = net_CIF_fc_a.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_EXP(network=net_CIF_fc_a, loss_func=loss_func_AUG,\n",
    "          learning_rate=0.0001, reinit_optim=2000,\n",
    "          train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "          reg_lambda=0.0005,\n",
    "          epochs=300,\n",
    "          dataloader_train=dataloader_CIF_train, dataloader_test=dataloader_CIF_test,\n",
    "          epoch_hook=convergence_analysys_hook\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(np.logspace(-3, 1, 5), np.logspace(-3, 1, 5), 0, Classifier_WITH_EXP, 3072, \n",
    "            loss_func=loss_func_EXP,\n",
    "            reinit_optim=1000,\n",
    "            train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "            epochs=200,\n",
    "            dataloader_train=dataloader_CIF_train, dataloader_test=dataloader_CIF_test,\n",
    "            epoch_hook=convergence_analysys_hook\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_CIF_fc_e = Classifier_WITH_EXP(32 * 32 * 3)\n",
    "if use_cuda:\n",
    "    net_CIF_fc_e = net_CIF_fc_e.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_EXP(network=net_CIF_fc_e, loss_func=loss_func_EXP,\n",
    "          learning_rate=0.0001, reinit_optim=2000,\n",
    "          train_strategy=train_strategy_NO_STOCH, test_strategy=test_strategy_NO_STOCH,\n",
    "          reg_lambda=0.0005,\n",
    "          epochs=300,\n",
    "          dataloader_train=dataloader_CIF_train, dataloader_test=dataloader_CIF_test,\n",
    "          #epoch_hook=convergence_analysys_hook()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_SMALL(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(Classifier_SMALL, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size // 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_size // 512, in_size // 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_size // 1024, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_WITH_EXP_SMALL(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(Classifier_WITH_EXP_SMALL, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size // 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_size // 512, in_size // 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_size // 1024, 1)\n",
    "        )\n",
    "        self.explinear = ExpLinear(in_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear(x).view(-1) + self.explinear(x).view(-1)\n",
    "    \n",
    "    # get parameter alpha from explinear block\n",
    "    def get_alpha(self):\n",
    "        return self.explinear.get_alpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierAug_SMALL(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(ClassifierAug_SMALL, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size // 512),\n",
    "            AugActivation(nn.Sigmoid()),\n",
    "            nn.Linear(in_size // 512, in_size // 1024),\n",
    "            AugActivation(nn.Sigmoid()),\n",
    "            nn.Linear(in_size // 1024, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_CIF_fc_s = Classifier_SMALL(32 * 32 * 3)\n",
    "if use_cuda:\n",
    "    net_CIF_fc_s = net_CIF_fc_s.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_(network=net_CIF_fc_s, loss_func=loss_func,\n",
    "       learning_rate=0.00001, reinit_optim=10000,\n",
    "       train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "       reg_lambda=None,\n",
    "       epochs=2000,\n",
    "       dataloader_train=dataloader_CIF_train, dataloader_test=dataloader_CIF_test,\n",
    "       epoch_hook=convergence_analysys_hook()\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_CIF_fc_s_e = Classifier_WITH_EXP_SMALL(32 * 32 * 3)\n",
    "if use_cuda:\n",
    "    net_CIF_fc_s_e = net_CIF_fc_s_e.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_EXP(network=net_CIF_fc_s_e, loss_func=loss_func_EXP,\n",
    "          learning_rate=0.00005, reinit_optim=8000,\n",
    "          train_strategy=train_strategy_STOCH, test_strategy=test_strategy_STOCH,\n",
    "          reg_lambda=0.5,\n",
    "          epochs=1000,\n",
    "          dataloader_train=dataloader_CIF_train, dataloader_test=dataloader_CIF_test,\n",
    "          epoch_hook=convergence_analysys_hook()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explinear_analysys_hook(**kwargs):\n",
    "    if 'cnt' not in explinear_analysys_hook.__dict__:\n",
    "        explinear_analysys_hook.cnt = 0\n",
    "        explinear_analysys_hook.prev_explinear_params = deepcopy(dict(kwargs['network'].explinear.named_parameters()))\n",
    "        dict(kwargs['network'].explinear.named_parameters())['linear.0.weight'].register_hook(print)\n",
    "    else:\n",
    "        print('\\nexplinear_params: ')\n",
    "        for name, param in kwargs['network'].explinear.named_parameters():\n",
    "            print(get_pure(deepcopy(param)))\n",
    "            \n",
    "        print('explinear_params_delta: ')\n",
    "        for name, param in kwargs['network'].explinear.named_parameters():\n",
    "            print(get_pure((deepcopy(param) - explinear_analysys_hook.prev_explinear_params[name]).norm(2)))\n",
    "        explinear_analysys_hook.prev_explinear_params = deepcopy(dict(kwargs['network'].explinear.named_parameters()))\n",
    "\n",
    "        bch = next(iter(kwargs['dataloader_test']))[0]\n",
    "        print('\\nbatch_max (linear / explinear): ')\n",
    "        print(max(get_pure(kwargs['network'].linear(bch))))    \n",
    "        print(max(get_pure(kwargs['network'].explinear(bch))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(network):\n",
    "    for _, param in network.explinear.named_parameters():\n",
    "        bias = torch.zeros_like(param).cuda()\n",
    "        if len(param.shape) == 1:\n",
    "            bias = torch.distributions.normal.Normal(0, 0.06).sample_n(param.shape[0]).cuda()\n",
    "        else:\n",
    "            bias = torch.distributions.normal.Normal(0, 0.06).sample_n(param.shape[0] * param.shape[1]).cuda()\n",
    "        param.data += bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(network, dataloader):\n",
    "    bch, lbls = next(iter(dataloader))\n",
    "    bch_norm = bch[0].norm(2)\n",
    "    loss = nn.HingeEmbeddingLoss()(network(bch), lbls)**3\n",
    "    print('Batch Norm: ', bch_norm)\n",
    "    print('Loss: ', loss)\n",
    "    return bch_norm, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(im):\n",
    "    rr = np.zeros([3, 32, 32])\n",
    "    rr[0,:,:] = im[:1024].reshape(32, 32)\n",
    "    rr[1,:,:] = im[1024:2048].reshape(32, 32)\n",
    "    rr[2,:,:] = im[2048:].reshape(32, 32)\n",
    "    rr = rr.transpose(1, 2, 0)\n",
    "    plt.imshow(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convergence_analysys_hook():\n",
    "    def __init__(self):\n",
    "        self.is_conv = False\n",
    "        \n",
    "    def __call__(self, **kwargs):\n",
    "        if kwargs['misscl_rate_train'][-1] == 0 and not self.is_conv:\n",
    "            self.is_conv = True\n",
    "            print('\\nConv epoch: ', kwargs['epoch'])\n",
    "            print('Learning rate: ', kwargs['learning_rate'])\n",
    "            try:\n",
    "                print('Reg Lambda: ', kwargs['reg_lambda'])\n",
    "            except KeyError:\n",
    "                pass\n",
    "            raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
